[
  {
    "function": "LazyInit.__call__",
    "module": "autorag",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Call self as a function."
  },
  {
    "function": "LazyInit.__getattr__",
    "module": "autorag",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "name",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "LazyInit.__init__",
    "module": "autorag",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "factory",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "args",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "kwargs",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Initialize self.  See help(type(self)) for accurate signature."
  },
  {
    "function": "handle_exception",
    "module": "autorag",
    "params": [
      {
        "name": "exc_type",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "exc_value",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "exc_traceback",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "Evaluator.__embed",
    "module": "autorag.evaluator",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "node_lines",
        "param_type": "typing.Dict[str, typing.List[autorag.schema.node.Node]]",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "Evaluator.__find_conflict_point",
    "module": "autorag.evaluator",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "trial_path",
        "param_type": "<class 'str'>",
        "description": ""
      },
      {
        "name": "node_line_names",
        "param_type": "typing.List[str]",
        "description": ""
      },
      {
        "name": "node_lines",
        "param_type": "typing.Dict[str, typing.List[autorag.schema.node.Node]]",
        "description": ""
      }
    ],
    "return_type": "tuple[str, str]",
    "docstring": null
  },
  {
    "function": "Evaluator.__get_new_trial_name",
    "module": "autorag.evaluator",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "<class 'str'>",
    "docstring": null
  },
  {
    "function": "Evaluator.__make_trial_dir",
    "module": "autorag.evaluator",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "trial_name",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "Evaluator.__set_previous_result",
    "module": "autorag.evaluator",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "node_line_names",
        "param_type": "typing.List[str]",
        "description": ""
      },
      {
        "name": "node_names",
        "param_type": "typing.List[typing.List[str]]",
        "description": ""
      },
      {
        "name": "trial_path",
        "param_type": "<class 'str'>",
        "description": ""
      },
      {
        "name": "conflict_node_name",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "Evaluator.__init__",
    "module": "autorag.evaluator",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "qa_data_path",
        "param_type": "<class 'str'>",
        "description": "The path to the QA dataset.\n    Must be parquet file."
      },
      {
        "name": "corpus_data_path",
        "param_type": "<class 'str'>",
        "description": "The path to the corpus dataset.\n    Must be parquet file."
      },
      {
        "name": "project_dir",
        "param_type": "typing.Optional[str]",
        "description": "The path to the project directory.\n    Default is the current directory."
      }
    ],
    "return_type": "Any",
    "docstring": "Initialize an Evaluator object.\n\n:param qa_data_path: The path to the QA dataset.\n    Must be parquet file.\n:param corpus_data_path: The path to the corpus dataset.\n    Must be parquet file.\n:param project_dir: The path to the project directory.\n    Default is the current directory."
  },
  {
    "function": "Evaluator._append_node_line_summary",
    "module": "autorag.evaluator",
    "params": [
      {
        "name": "node_line_name",
        "param_type": "<class 'str'>",
        "description": ""
      },
      {
        "name": "node_line_dir",
        "param_type": "<class 'str'>",
        "description": ""
      },
      {
        "name": "trial_summary_df",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "Evaluator._append_node_summary",
    "module": "autorag.evaluator",
    "params": [
      {
        "name": "node_line_dir",
        "param_type": "<class 'str'>",
        "description": ""
      },
      {
        "name": "node_name",
        "param_type": "<class 'str'>",
        "description": ""
      },
      {
        "name": "summary_lst",
        "param_type": "typing.List[typing.Dict]",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "Evaluator._find_conflict_node_name",
    "module": "autorag.evaluator",
    "params": [
      {
        "name": "node_line_dir",
        "param_type": "<class 'str'>",
        "description": ""
      },
      {
        "name": "node_line",
        "param_type": "typing.List[autorag.schema.node.Node]",
        "description": ""
      }
    ],
    "return_type": "<class 'str'>",
    "docstring": null
  },
  {
    "function": "Evaluator._load_node_lines",
    "module": "autorag.evaluator",
    "params": [
      {
        "name": "yaml_path",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "typing.Dict[str, typing.List[autorag.schema.node.Node]]",
    "docstring": null
  },
  {
    "function": "Evaluator._set_remain_nodes_and_lines",
    "module": "autorag.evaluator",
    "params": [
      {
        "name": "node_line_names",
        "param_type": "typing.List[str]",
        "description": ""
      },
      {
        "name": "nodes",
        "param_type": "typing.List[typing.List[autorag.schema.node.Node]]",
        "description": ""
      },
      {
        "name": "node_names",
        "param_type": "typing.List[typing.List[str]]",
        "description": ""
      },
      {
        "name": "conflict_node_name",
        "param_type": "<class 'str'>",
        "description": ""
      },
      {
        "name": "conflict_node_line_name",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "Evaluator.restart_trial",
    "module": "autorag.evaluator",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "trial_path",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "Evaluator.start_trial",
    "module": "autorag.evaluator",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "yaml_path",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "Runner.__add_api_route",
    "module": "autorag.deploy",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "Runner.__init__",
    "module": "autorag.deploy",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "config",
        "param_type": "typing.Dict",
        "description": ""
      },
      {
        "name": "project_dir",
        "param_type": "typing.Optional[str]",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Initialize self.  See help(type(self)) for accurate signature."
  },
  {
    "function": "Runner.from_trial_folder",
    "module": "autorag.deploy",
    "params": [
      {
        "name": "trial_path",
        "param_type": "<class 'str'>",
        "description": "The path of the trial folder."
      }
    ],
    "return_type": "Any",
    "docstring": "Load Runner from evaluated trial folder.\nMust already be evaluated using Evaluator class.\nIt sets the project_dir as the parent directory of the trial folder.\n\n:param trial_path: The path of the trial folder.\n:return: Initialized Runner."
  },
  {
    "function": "Runner.from_yaml",
    "module": "autorag.deploy",
    "params": [
      {
        "name": "yaml_path",
        "param_type": "<class 'str'>",
        "description": "The path of the yaml file."
      },
      {
        "name": "project_dir",
        "param_type": "typing.Optional[str]",
        "description": "The path of the project directory.\n    Default is the current directory."
      }
    ],
    "return_type": "Any",
    "docstring": "Load Runner from yaml file.\nMust be extracted yaml file from evaluated trial using extract_best_config method.\n\n:param yaml_path: The path of the yaml file.\n:param project_dir: The path of the project directory.\n    Default is the current directory.\n:return: Initialized Runner."
  },
  {
    "function": "Runner.run",
    "module": "autorag.deploy",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "query",
        "param_type": "<class 'str'>",
        "description": "The query of the user."
      },
      {
        "name": "result_column",
        "param_type": "<class 'str'>",
        "description": "The result column name for the answer.\n    Default is `generated_texts`, which is the output of the `generation` module."
      }
    ],
    "return_type": "Any",
    "docstring": "Run the pipeline with query.\nThe loaded pipeline must start with a single query,\nso the first module of the pipeline must be `query_expansion` or `retrieval` module.\n\n:param query: The query of the user.\n:param result_column: The result column name for the answer.\n    Default is `generated_texts`, which is the output of the `generation` module.\n:return: The result of the pipeline."
  },
  {
    "function": "Runner.run_api_server",
    "module": "autorag.deploy",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "host",
        "param_type": "<class 'str'>",
        "description": "The host of the api server."
      },
      {
        "name": "port",
        "param_type": "<class 'int'>",
        "description": "The port of the api server."
      },
      {
        "name": "kwargs",
        "param_type": "Any",
        "description": "Other arguments for uvicorn.run."
      }
    ],
    "return_type": "Any",
    "docstring": "Run the pipeline as api server.\nYou can send POST request to `http://host:port/run` with json body like below:\n\n.. Code:: json\n\n    {\n        \"Query\": \"your query\",\n        \"result_column\": \"answer\"\n    }\n\nAnd it returns json response like below:\n\n.. Code:: json\n\n    {\n        \"answer\": \"your answer\"\n    }\n\n:param host: The host of the api server.\n:param port: The port of the api server.\n:param kwargs: Other arguments for uvicorn.run."
  },
  {
    "function": "extract_best_config",
    "module": "autorag.deploy",
    "params": [
      {
        "name": "trial_path",
        "param_type": "<class 'str'>",
        "description": "The path to the trial directory that you want to extract the pipeline from.\n    Must already be evaluated."
      },
      {
        "name": "output_path",
        "param_type": "typing.Optional[str]",
        "description": "Output path that pipeline yaml file will be saved.\n    Must be .yaml or .yml file.\n    If None, it does not save yaml file and just return dict values.\n    Default is None."
      }
    ],
    "return_type": "typing.Dict",
    "docstring": "Extract the optimal pipeline from evaluated trial.\n\n:param trial_path: The path to the trial directory that you want to extract the pipeline from.\n    Must already be evaluated.\n:param output_path: Output path that pipeline yaml file will be saved.\n    Must be .yaml or .yml file.\n    If None, it does not save yaml file and just return dict values.\n    Default is None.\n:return: The dictionary of the extracted pipeline."
  },
  {
    "function": "dict_to_markdown",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "d",
        "param_type": "Any",
        "description": "Dictionary to convert"
      },
      {
        "name": "level",
        "param_type": "Any",
        "description": "Current level of heading (used for nested dictionaries)"
      }
    ],
    "return_type": "Any",
    "docstring": "Convert a dictionary to a Markdown formatted string.\n\n:param d: Dictionary to convert\n:param level: Current level of heading (used for nested dictionaries)\n:return: Markdown formatted string"
  },
  {
    "function": "dict_to_markdown_table",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "data",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "key_column_name",
        "param_type": "<class 'str'>",
        "description": ""
      },
      {
        "name": "value_column_name",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "find_node_dir",
    "module": "autorag.dashboard",
    "params": [
      {
        "name": "trial_dir",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "typing.List[str]",
    "docstring": null
  },
  {
    "function": "get_metric_values",
    "module": "autorag.dashboard",
    "params": [
      {
        "name": "node_summary_df",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": ""
      }
    ],
    "return_type": "typing.Dict",
    "docstring": null
  },
  {
    "function": "make_trial_summary_md",
    "module": "autorag.dashboard",
    "params": [
      {
        "name": "trial_dir",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "node_view",
    "module": "autorag.dashboard",
    "params": [
      {
        "name": "node_dir",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "run",
    "module": "autorag.dashboard",
    "params": [
      {
        "name": "trial_dir",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "yaml_to_markdown",
    "module": "autorag.dashboard",
    "params": [
      {
        "name": "yaml_filepath",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "langchain_documents_to_parquet",
    "module": "autorag.data.corpus.langchain",
    "params": [
      {
        "name": "langchain_documents",
        "param_type": "typing.List[langchain_core.documents.base.Document]",
        "description": "List of langchain documents."
      },
      {
        "name": "output_filepath",
        "param_type": "typing.Optional[str]",
        "description": "Optional filepath to save the parquet file.\n    If None, the function will return the processed_data as pd.DataFrame, but do not save as parquet.\n    File directory must exist. File extension must be .parquet"
      },
      {
        "name": "upsert",
        "param_type": "<class 'bool'>",
        "description": "If true, the function will overwrite the existing file if it exists.\n    Default is False."
      }
    ],
    "return_type": "<class 'pandas.core.frame.DataFrame'>",
    "docstring": "Langchain documents to corpus dataframe.\nCorpus dataframe will be saved to filepath(file_dir/filename) if given.\nReturn corpus dataframe whether the filepath is given.\nYou can use this method to create corpus.parquet after load and chunk using Llama Index.\n\n:param langchain_documents: List of langchain documents.\n:param output_filepath: Optional filepath to save the parquet file.\n    If None, the function will return the processed_data as pd.DataFrame, but do not save as parquet.\n    File directory must exist. File extension must be .parquet\n:param upsert: If true, the function will overwrite the existing file if it exists.\n    Default is False.\n:return: Corpus data as pd.DataFrame"
  },
  {
    "function": "llama_documents_to_parquet",
    "module": "autorag.data.corpus.llama_index",
    "params": [
      {
        "name": "llama_documents",
        "param_type": "typing.List[llama_index.core.schema.Document]",
        "description": "List[Document]"
      },
      {
        "name": "output_filepath",
        "param_type": "typing.Optional[str]",
        "description": "Optional filepath to save the parquet file.\n    If None, the function will return the processed_data as pd.DataFrame, but do not save as parquet.\n    File directory must exist. File extension must be .parquet"
      },
      {
        "name": "upsert",
        "param_type": "<class 'bool'>",
        "description": "If true, the function will overwrite the existing file if it exists.\n    Default is False."
      }
    ],
    "return_type": "<class 'pandas.core.frame.DataFrame'>",
    "docstring": "Llama Index documents to corpus dataframe.\nCorpus dataframe will be saved to filepath(file_dir/filename) if given.\nReturn corpus dataframe whether the filepath is given.\nYou can use this method to create corpus.parquet after load and chunk using Llama Index.\n\n:param llama_documents: List[Document]\n:param output_filepath: Optional filepath to save the parquet file.\n    If None, the function will return the processed_data as pd.DataFrame, but do not save as parquet.\n    File directory must exist. File extension must be .parquet\n:param upsert: If true, the function will overwrite the existing file if it exists.\n    Default is False.\n:return: Corpus data as pd.DataFrame"
  },
  {
    "function": "llama_text_node_to_parquet",
    "module": "autorag.data.corpus.llama_index",
    "params": [
      {
        "name": "text_nodes",
        "param_type": "typing.List[llama_index.core.schema.TextNode]",
        "description": "List of llama index text nodes."
      },
      {
        "name": "output_filepath",
        "param_type": "typing.Optional[str]",
        "description": "Optional filepath to save the parquet file.\n    If None, the function will return the processed_data as pd.DataFrame, but do not save as parquet.\n    File directory must exist. File extension must be .parquet"
      },
      {
        "name": "upsert",
        "param_type": "<class 'bool'>",
        "description": "If true, the function will overwrite the existing file if it exists.\n    Default is False."
      }
    ],
    "return_type": "<class 'pandas.core.frame.DataFrame'>",
    "docstring": "Llama Index text nodes to corpus dataframe.\nCorpus dataframe will be saved to filepath(file_dir/filename) if given.\nReturn corpus dataframe whether the filepath is given.\nYou can use this method to create corpus.parquet after load and chunk using Llama Index.\n\n:param text_nodes: List of llama index text nodes.\n:param output_filepath: Optional filepath to save the parquet file.\n    If None, the function will return the processed_data as pd.DataFrame, but do not save as parquet.\n    File directory must exist. File extension must be .parquet\n:param upsert: If true, the function will overwrite the existing file if it exists.\n    Default is False.\n:return: Corpus data as pd.DataFrame"
  },
  {
    "function": "add_essential_metadata",
    "module": "autorag.data.utils.util",
    "params": [
      {
        "name": "metadata",
        "param_type": "typing.Dict",
        "description": ""
      }
    ],
    "return_type": "typing.Dict",
    "docstring": null
  },
  {
    "function": "langchain_documents_to_parquet",
    "module": "autorag.data.corpus.langchain",
    "params": [
      {
        "name": "langchain_documents",
        "param_type": "typing.List[langchain_core.documents.base.Document]",
        "description": "List of langchain documents."
      },
      {
        "name": "output_filepath",
        "param_type": "typing.Optional[str]",
        "description": "Optional filepath to save the parquet file.\n    If None, the function will return the processed_data as pd.DataFrame, but do not save as parquet.\n    File directory must exist. File extension must be .parquet"
      },
      {
        "name": "upsert",
        "param_type": "<class 'bool'>",
        "description": "If true, the function will overwrite the existing file if it exists.\n    Default is False."
      }
    ],
    "return_type": "<class 'pandas.core.frame.DataFrame'>",
    "docstring": "Langchain documents to corpus dataframe.\nCorpus dataframe will be saved to filepath(file_dir/filename) if given.\nReturn corpus dataframe whether the filepath is given.\nYou can use this method to create corpus.parquet after load and chunk using Llama Index.\n\n:param langchain_documents: List of langchain documents.\n:param output_filepath: Optional filepath to save the parquet file.\n    If None, the function will return the processed_data as pd.DataFrame, but do not save as parquet.\n    File directory must exist. File extension must be .parquet\n:param upsert: If true, the function will overwrite the existing file if it exists.\n    Default is False.\n:return: Corpus data as pd.DataFrame"
  },
  {
    "function": "save_parquet_safe",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "df",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": ""
      },
      {
        "name": "filepath",
        "param_type": "<class 'str'>",
        "description": ""
      },
      {
        "name": "upsert",
        "param_type": "<class 'bool'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "add_essential_metadata",
    "module": "autorag.data.utils.util",
    "params": [
      {
        "name": "metadata",
        "param_type": "typing.Dict",
        "description": ""
      }
    ],
    "return_type": "typing.Dict",
    "docstring": null
  },
  {
    "function": "add_essential_metadata_llama_text_node",
    "module": "autorag.data.corpus.llama_index",
    "params": [
      {
        "name": "metadata",
        "param_type": "typing.Dict",
        "description": ""
      },
      {
        "name": "relationships",
        "param_type": "typing.Dict",
        "description": ""
      }
    ],
    "return_type": "typing.Dict",
    "docstring": null
  },
  {
    "function": "llama_documents_to_parquet",
    "module": "autorag.data.corpus.llama_index",
    "params": [
      {
        "name": "llama_documents",
        "param_type": "typing.List[llama_index.core.schema.Document]",
        "description": "List[Document]"
      },
      {
        "name": "output_filepath",
        "param_type": "typing.Optional[str]",
        "description": "Optional filepath to save the parquet file.\n    If None, the function will return the processed_data as pd.DataFrame, but do not save as parquet.\n    File directory must exist. File extension must be .parquet"
      },
      {
        "name": "upsert",
        "param_type": "<class 'bool'>",
        "description": "If true, the function will overwrite the existing file if it exists.\n    Default is False."
      }
    ],
    "return_type": "<class 'pandas.core.frame.DataFrame'>",
    "docstring": "Llama Index documents to corpus dataframe.\nCorpus dataframe will be saved to filepath(file_dir/filename) if given.\nReturn corpus dataframe whether the filepath is given.\nYou can use this method to create corpus.parquet after load and chunk using Llama Index.\n\n:param llama_documents: List[Document]\n:param output_filepath: Optional filepath to save the parquet file.\n    If None, the function will return the processed_data as pd.DataFrame, but do not save as parquet.\n    File directory must exist. File extension must be .parquet\n:param upsert: If true, the function will overwrite the existing file if it exists.\n    Default is False.\n:return: Corpus data as pd.DataFrame"
  },
  {
    "function": "llama_text_node_to_parquet",
    "module": "autorag.data.corpus.llama_index",
    "params": [
      {
        "name": "text_nodes",
        "param_type": "typing.List[llama_index.core.schema.TextNode]",
        "description": "List of llama index text nodes."
      },
      {
        "name": "output_filepath",
        "param_type": "typing.Optional[str]",
        "description": "Optional filepath to save the parquet file.\n    If None, the function will return the processed_data as pd.DataFrame, but do not save as parquet.\n    File directory must exist. File extension must be .parquet"
      },
      {
        "name": "upsert",
        "param_type": "<class 'bool'>",
        "description": "If true, the function will overwrite the existing file if it exists.\n    Default is False."
      }
    ],
    "return_type": "<class 'pandas.core.frame.DataFrame'>",
    "docstring": "Llama Index text nodes to corpus dataframe.\nCorpus dataframe will be saved to filepath(file_dir/filename) if given.\nReturn corpus dataframe whether the filepath is given.\nYou can use this method to create corpus.parquet after load and chunk using Llama Index.\n\n:param text_nodes: List of llama index text nodes.\n:param output_filepath: Optional filepath to save the parquet file.\n    If None, the function will return the processed_data as pd.DataFrame, but do not save as parquet.\n    File directory must exist. File extension must be .parquet\n:param upsert: If true, the function will overwrite the existing file if it exists.\n    Default is False.\n:return: Corpus data as pd.DataFrame"
  },
  {
    "function": "save_parquet_safe",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "df",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": ""
      },
      {
        "name": "filepath",
        "param_type": "<class 'str'>",
        "description": ""
      },
      {
        "name": "upsert",
        "param_type": "<class 'bool'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "generate_qa_llama_index",
    "module": "autorag.data.qacreation.llama_index",
    "params": [
      {
        "name": "llm",
        "param_type": "typing.Union[llama_index.core.service_context_elements.llm_predictor.LLMPredictor, llama_index.core.llms.llm.LLM]",
        "description": "Llama index model"
      },
      {
        "name": "contents",
        "param_type": "typing.List[str]",
        "description": "List of content strings."
      },
      {
        "name": "prompt",
        "param_type": "typing.Optional[str]",
        "description": "The prompt to use for the qa generation.\n    The prompt must include the following placeholders:\n    - {{text}}: The content string\n    - {{num_questions}}: The number of questions to generate\n    As default, the prompt is set to the default prompt for the question type."
      },
      {
        "name": "question_num_per_content",
        "param_type": "<class 'int'>",
        "description": "Number of questions to generate for each content.\n    Default is 1."
      },
      {
        "name": "max_retries",
        "param_type": "<class 'int'>",
        "description": "The maximum number of retries when generated question number is not equal to the target number.\n    Default is 3."
      },
      {
        "name": "batch",
        "param_type": "<class 'int'>",
        "description": "The batch size to process asynchronously.\n    Default is 4."
      }
    ],
    "return_type": "typing.List[typing.List[typing.Dict]]",
    "docstring": "Generate a qa set from the list of contents.\nIt uses a single prompt for all contents.\nIf you want to use more than one prompt for generating qa,\nyou can consider using generate_qa_llama_index_by_ratio.\n\n:param llm: Llama index model\n:param contents: List of content strings.\n:param prompt: The prompt to use for the qa generation.\n    The prompt must include the following placeholders:\n    - {{text}}: The content string\n    - {{num_questions}}: The number of questions to generate\n    As default, the prompt is set to the default prompt for the question type.\n:param question_num_per_content: Number of questions to generate for each content.\n    Default is 1.\n:param max_retries: The maximum number of retries when generated question number is not equal to the target number.\n    Default is 3.\n:param batch: The batch size to process asynchronously.\n    Default is 4.\n:return: 2-d list of dictionaries containing the query and generation_gt."
  },
  {
    "function": "generate_qa_llama_index_by_ratio",
    "module": "autorag.data.qacreation.llama_index",
    "params": [
      {
        "name": "llm",
        "param_type": "typing.Union[llama_index.core.service_context_elements.llm_predictor.LLMPredictor, llama_index.core.llms.llm.LLM]",
        "description": "Llama index model"
      },
      {
        "name": "contents",
        "param_type": "typing.List[str]",
        "description": "List of content strings."
      },
      {
        "name": "prompts_ratio",
        "param_type": "typing.Dict",
        "description": "Dictionary of prompt paths and their ratios.\n    Example: {\"prompt/prompt1.txt\": 0.5, \"prompt/prompt2.txt\": 0.5}\n    The value sum doesn't have to be 1.\n    The path must be the absolute path, and the file must exist.\n    Plus, it has to be a text file which contains proper prompt.\n    Each prompt must contain the following placeholders:\n    - {{text}}: The content string\n    - {{num_questions}}: The number of questions to generate"
      },
      {
        "name": "question_num_per_content",
        "param_type": "<class 'int'>",
        "description": "Number of questions to generate for each content.\n    Default is 1."
      },
      {
        "name": "max_retries",
        "param_type": "<class 'int'>",
        "description": "The maximum number of retries when generated question number is not equal to the target number.\n    Default is 3."
      },
      {
        "name": "random_state",
        "param_type": "<class 'int'>",
        "description": "Random seed\n    Default is 42."
      },
      {
        "name": "batch",
        "param_type": "<class 'int'>",
        "description": "The batch size to process asynchronously.\n    Default is 4."
      }
    ],
    "return_type": "typing.List[typing.List[typing.Dict]]",
    "docstring": "Generate a qa set from the list of contents.\nYou can set the ratio of prompts that you want to use for generating qa.\nIt distributes the number of questions to generate for each content by the ratio randomly.\n\n:param llm: Llama index model\n:param contents: List of content strings.\n:param prompts_ratio: Dictionary of prompt paths and their ratios.\n    Example: {\"prompt/prompt1.txt\": 0.5, \"prompt/prompt2.txt\": 0.5}\n    The value sum doesn't have to be 1.\n    The path must be the absolute path, and the file must exist.\n    Plus, it has to be a text file which contains proper prompt.\n    Each prompt must contain the following placeholders:\n    - {{text}}: The content string\n    - {{num_questions}}: The number of questions to generate\n:param question_num_per_content: Number of questions to generate for each content.\n    Default is 1.\n:param max_retries: The maximum number of retries when generated question number is not equal to the target number.\n    Default is 3.\n:param random_state: Random seed\n    Default is 42.\n:param batch: The batch size to process asynchronously.\n    Default is 4.\n:return: 2-d list of dictionaries containing the query and generation_gt."
  },
  {
    "function": "make_single_content_qa",
    "module": "autorag.data.qacreation.base",
    "params": [
      {
        "name": "corpus_df",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": "The corpus dataframe to make QA dataset from."
      },
      {
        "name": "content_size",
        "param_type": "<class 'int'>",
        "description": "This function will generate QA dataset for the given number of contents."
      },
      {
        "name": "qa_creation_func",
        "param_type": "typing.Callable",
        "description": "The function to create QA pairs.\n    You can use like `generate_qa_llama_index` or `generate_qa_llama_index_by_ratio`.\n    The input func must have `contents` parameter for the list of content string."
      },
      {
        "name": "output_filepath",
        "param_type": "typing.Optional[str]",
        "description": "Optional filepath to save the parquet file.\n    If None, the function will return the processed_data as pd.DataFrame, but do not save as parquet.\n    File directory must exist. File extension must be .parquet"
      },
      {
        "name": "upsert",
        "param_type": "<class 'bool'>",
        "description": "If true, the function will overwrite the existing file if it exists.\n    Default is False."
      },
      {
        "name": "random_state",
        "param_type": "<class 'int'>",
        "description": "The random state for sampling corpus from the given corpus_df."
      },
      {
        "name": "cache_batch",
        "param_type": "<class 'int'>",
        "description": "The number of batches to use for caching the generated QA dataset.\n    When the cache_batch size data is generated, the dataset will save to the designated output_filepath.\n    If the cache_batch size is too small, the process time will be longer."
      },
      {
        "name": "kwargs",
        "param_type": "Any",
        "description": "The keyword arguments for qa_creation_func."
      }
    ],
    "return_type": "<class 'pandas.core.frame.DataFrame'>",
    "docstring": "Make single content (single-hop, single-document) QA dataset using given qa_creation_func.\nIt generates a single content QA dataset, which means its retrieval ground truth will be only one.\nIt is the most basic form of QA dataset.\n\n:param corpus_df: The corpus dataframe to make QA dataset from.\n:param content_size: This function will generate QA dataset for the given number of contents.\n:param qa_creation_func: The function to create QA pairs.\n    You can use like `generate_qa_llama_index` or `generate_qa_llama_index_by_ratio`.\n    The input func must have `contents` parameter for the list of content string.\n:param output_filepath: Optional filepath to save the parquet file.\n    If None, the function will return the processed_data as pd.DataFrame, but do not save as parquet.\n    File directory must exist. File extension must be .parquet\n:param upsert: If true, the function will overwrite the existing file if it exists.\n    Default is False.\n:param random_state: The random state for sampling corpus from the given corpus_df.\n:param cache_batch: The number of batches to use for caching the generated QA dataset.\n    When the cache_batch size data is generated, the dataset will save to the designated output_filepath.\n    If the cache_batch size is too small, the process time will be longer.\n:param kwargs: The keyword arguments for qa_creation_func.\n:return: QA dataset dataframe.\n    You can save this as parquet file to use at AutoRAG."
  },
  {
    "function": "make_single_content_qa",
    "module": "autorag.data.qacreation.base",
    "params": [
      {
        "name": "corpus_df",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": "The corpus dataframe to make QA dataset from."
      },
      {
        "name": "content_size",
        "param_type": "<class 'int'>",
        "description": "This function will generate QA dataset for the given number of contents."
      },
      {
        "name": "qa_creation_func",
        "param_type": "typing.Callable",
        "description": "The function to create QA pairs.\n    You can use like `generate_qa_llama_index` or `generate_qa_llama_index_by_ratio`.\n    The input func must have `contents` parameter for the list of content string."
      },
      {
        "name": "output_filepath",
        "param_type": "typing.Optional[str]",
        "description": "Optional filepath to save the parquet file.\n    If None, the function will return the processed_data as pd.DataFrame, but do not save as parquet.\n    File directory must exist. File extension must be .parquet"
      },
      {
        "name": "upsert",
        "param_type": "<class 'bool'>",
        "description": "If true, the function will overwrite the existing file if it exists.\n    Default is False."
      },
      {
        "name": "random_state",
        "param_type": "<class 'int'>",
        "description": "The random state for sampling corpus from the given corpus_df."
      },
      {
        "name": "cache_batch",
        "param_type": "<class 'int'>",
        "description": "The number of batches to use for caching the generated QA dataset.\n    When the cache_batch size data is generated, the dataset will save to the designated output_filepath.\n    If the cache_batch size is too small, the process time will be longer."
      },
      {
        "name": "kwargs",
        "param_type": "Any",
        "description": "The keyword arguments for qa_creation_func."
      }
    ],
    "return_type": "<class 'pandas.core.frame.DataFrame'>",
    "docstring": "Make single content (single-hop, single-document) QA dataset using given qa_creation_func.\nIt generates a single content QA dataset, which means its retrieval ground truth will be only one.\nIt is the most basic form of QA dataset.\n\n:param corpus_df: The corpus dataframe to make QA dataset from.\n:param content_size: This function will generate QA dataset for the given number of contents.\n:param qa_creation_func: The function to create QA pairs.\n    You can use like `generate_qa_llama_index` or `generate_qa_llama_index_by_ratio`.\n    The input func must have `contents` parameter for the list of content string.\n:param output_filepath: Optional filepath to save the parquet file.\n    If None, the function will return the processed_data as pd.DataFrame, but do not save as parquet.\n    File directory must exist. File extension must be .parquet\n:param upsert: If true, the function will overwrite the existing file if it exists.\n    Default is False.\n:param random_state: The random state for sampling corpus from the given corpus_df.\n:param cache_batch: The number of batches to use for caching the generated QA dataset.\n    When the cache_batch size data is generated, the dataset will save to the designated output_filepath.\n    If the cache_batch size is too small, the process time will be longer.\n:param kwargs: The keyword arguments for qa_creation_func.\n:return: QA dataset dataframe.\n    You can save this as parquet file to use at AutoRAG."
  },
  {
    "function": "save_parquet_safe",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "df",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": ""
      },
      {
        "name": "filepath",
        "param_type": "<class 'str'>",
        "description": ""
      },
      {
        "name": "upsert",
        "param_type": "<class 'bool'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "async_qa_gen_llama_index",
    "module": "autorag.data.qacreation.llama_index",
    "params": [
      {
        "name": "content",
        "param_type": "<class 'str'>",
        "description": "Content string"
      },
      {
        "name": "llm",
        "param_type": "typing.Union[llama_index.core.service_context_elements.llm_predictor.LLMPredictor, llama_index.core.llms.llm.LLM]",
        "description": "Llama index model"
      },
      {
        "name": "prompt",
        "param_type": "<class 'str'>",
        "description": "The prompt to use for the qa generation.\n    The prompt must include the following placeholders:\n    - {{text}}: The content string\n    - {{num_questions}}: The number of questions to generate"
      },
      {
        "name": "question_num",
        "param_type": "<class 'int'>",
        "description": "The number of questions to generate"
      },
      {
        "name": "max_retries",
        "param_type": "<class 'int'>",
        "description": "Maximum number of retries when generated question number is not equal to the target number"
      }
    ],
    "return_type": "Any",
    "docstring": "Generate a qa set by using the given content and the llama index model.\nYou must select the question type.\n\n:param content: Content string\n:param llm: Llama index model\n:param prompt: The prompt to use for the qa generation.\n    The prompt must include the following placeholders:\n    - {{text}}: The content string\n    - {{num_questions}}: The number of questions to generate\n:param question_num: The number of questions to generate\n:param max_retries: Maximum number of retries when generated question number is not equal to the target number\n:return: List of dictionaries containing the query and generation_gt"
  },
  {
    "function": "distribute_list_by_ratio",
    "module": "autorag.data.qacreation.llama_index",
    "params": [
      {
        "name": "input_list",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "ratio",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "typing.List[typing.List[typing.Any]]",
    "docstring": null
  },
  {
    "function": "generate_qa_llama_index",
    "module": "autorag.data.qacreation.llama_index",
    "params": [
      {
        "name": "llm",
        "param_type": "typing.Union[llama_index.core.service_context_elements.llm_predictor.LLMPredictor, llama_index.core.llms.llm.LLM]",
        "description": "Llama index model"
      },
      {
        "name": "contents",
        "param_type": "typing.List[str]",
        "description": "List of content strings."
      },
      {
        "name": "prompt",
        "param_type": "typing.Optional[str]",
        "description": "The prompt to use for the qa generation.\n    The prompt must include the following placeholders:\n    - {{text}}: The content string\n    - {{num_questions}}: The number of questions to generate\n    As default, the prompt is set to the default prompt for the question type."
      },
      {
        "name": "question_num_per_content",
        "param_type": "<class 'int'>",
        "description": "Number of questions to generate for each content.\n    Default is 1."
      },
      {
        "name": "max_retries",
        "param_type": "<class 'int'>",
        "description": "The maximum number of retries when generated question number is not equal to the target number.\n    Default is 3."
      },
      {
        "name": "batch",
        "param_type": "<class 'int'>",
        "description": "The batch size to process asynchronously.\n    Default is 4."
      }
    ],
    "return_type": "typing.List[typing.List[typing.Dict]]",
    "docstring": "Generate a qa set from the list of contents.\nIt uses a single prompt for all contents.\nIf you want to use more than one prompt for generating qa,\nyou can consider using generate_qa_llama_index_by_ratio.\n\n:param llm: Llama index model\n:param contents: List of content strings.\n:param prompt: The prompt to use for the qa generation.\n    The prompt must include the following placeholders:\n    - {{text}}: The content string\n    - {{num_questions}}: The number of questions to generate\n    As default, the prompt is set to the default prompt for the question type.\n:param question_num_per_content: Number of questions to generate for each content.\n    Default is 1.\n:param max_retries: The maximum number of retries when generated question number is not equal to the target number.\n    Default is 3.\n:param batch: The batch size to process asynchronously.\n    Default is 4.\n:return: 2-d list of dictionaries containing the query and generation_gt."
  },
  {
    "function": "generate_qa_llama_index_by_ratio",
    "module": "autorag.data.qacreation.llama_index",
    "params": [
      {
        "name": "llm",
        "param_type": "typing.Union[llama_index.core.service_context_elements.llm_predictor.LLMPredictor, llama_index.core.llms.llm.LLM]",
        "description": "Llama index model"
      },
      {
        "name": "contents",
        "param_type": "typing.List[str]",
        "description": "List of content strings."
      },
      {
        "name": "prompts_ratio",
        "param_type": "typing.Dict",
        "description": "Dictionary of prompt paths and their ratios.\n    Example: {\"prompt/prompt1.txt\": 0.5, \"prompt/prompt2.txt\": 0.5}\n    The value sum doesn't have to be 1.\n    The path must be the absolute path, and the file must exist.\n    Plus, it has to be a text file which contains proper prompt.\n    Each prompt must contain the following placeholders:\n    - {{text}}: The content string\n    - {{num_questions}}: The number of questions to generate"
      },
      {
        "name": "question_num_per_content",
        "param_type": "<class 'int'>",
        "description": "Number of questions to generate for each content.\n    Default is 1."
      },
      {
        "name": "max_retries",
        "param_type": "<class 'int'>",
        "description": "The maximum number of retries when generated question number is not equal to the target number.\n    Default is 3."
      },
      {
        "name": "random_state",
        "param_type": "<class 'int'>",
        "description": "Random seed\n    Default is 42."
      },
      {
        "name": "batch",
        "param_type": "<class 'int'>",
        "description": "The batch size to process asynchronously.\n    Default is 4."
      }
    ],
    "return_type": "typing.List[typing.List[typing.Dict]]",
    "docstring": "Generate a qa set from the list of contents.\nYou can set the ratio of prompts that you want to use for generating qa.\nIt distributes the number of questions to generate for each content by the ratio randomly.\n\n:param llm: Llama index model\n:param contents: List of content strings.\n:param prompts_ratio: Dictionary of prompt paths and their ratios.\n    Example: {\"prompt/prompt1.txt\": 0.5, \"prompt/prompt2.txt\": 0.5}\n    The value sum doesn't have to be 1.\n    The path must be the absolute path, and the file must exist.\n    Plus, it has to be a text file which contains proper prompt.\n    Each prompt must contain the following placeholders:\n    - {{text}}: The content string\n    - {{num_questions}}: The number of questions to generate\n:param question_num_per_content: Number of questions to generate for each content.\n    Default is 1.\n:param max_retries: The maximum number of retries when generated question number is not equal to the target number.\n    Default is 3.\n:param random_state: Random seed\n    Default is 42.\n:param batch: The batch size to process asynchronously.\n    Default is 4.\n:return: 2-d list of dictionaries containing the query and generation_gt."
  },
  {
    "function": "parse_output",
    "module": "autorag.data.qacreation.llama_index",
    "params": [
      {
        "name": "result",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "typing.List[typing.Dict]",
    "docstring": null
  },
  {
    "function": "process_batch",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "tasks",
        "param_type": "Any",
        "description": "A list of no-argument functions or coroutines to be executed."
      },
      {
        "name": "batch_size",
        "param_type": "<class 'int'>",
        "description": "The number of tasks to process in a single batch.\n    Default is 64."
      }
    ],
    "return_type": "typing.List[typing.Any]",
    "docstring": "Processes tasks in batches asynchronously.\n\n:param tasks: A list of no-argument functions or coroutines to be executed.\n:param batch_size: The number of tasks to process in a single batch.\n    Default is 64.\n:return: A list of results from the processed tasks."
  },
  {
    "function": "validate_llama_index_prompt",
    "module": "autorag.data.qacreation.llama_index",
    "params": [
      {
        "name": "prompt",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "<class 'bool'>",
    "docstring": "Validate the prompt for the llama index model.\nThe prompt must include the following placeholders:\n- {{text}}: The content string\n- {{num_questions}}: The number of questions to generate"
  },
  {
    "function": "cast_qa_dataset",
    "module": "autorag.utils.preprocess",
    "params": [
      {
        "name": "df",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "corpus_df_to_langchain_documents",
    "module": "autorag.data.utils.util",
    "params": [
      {
        "name": "corpus_df",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": ""
      }
    ],
    "return_type": "typing.List[langchain_core.documents.base.Document]",
    "docstring": null
  },
  {
    "function": "generate_qa_ragas",
    "module": "autorag.data.qacreation.ragas",
    "params": [
      {
        "name": "corpus_df",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": "Corpus dataframe."
      },
      {
        "name": "test_size",
        "param_type": "<class 'int'>",
        "description": "Number of queries to generate."
      },
      {
        "name": "distributions",
        "param_type": "typing.Optional[dict]",
        "description": "Distributions of different types of questions.\n    Default is \"simple is 0.5, multi_context is 0.4, and reasoning is 0.1.\"\n    Each type of questions refers to Ragas evolution types."
      },
      {
        "name": "generator_llm",
        "param_type": "typing.Optional[langchain_core.language_models.chat_models.BaseChatModel]",
        "description": "Generator language model from Langchain."
      },
      {
        "name": "critic_llm",
        "param_type": "typing.Optional[langchain_core.language_models.chat_models.BaseChatModel]",
        "description": "Critic language model from Langchain."
      },
      {
        "name": "embedding_model",
        "param_type": "typing.Optional[langchain_core.embeddings.embeddings.Embeddings]",
        "description": "Embedding model from Langchain."
      },
      {
        "name": "kwargs",
        "param_type": "Any",
        "description": "The additional option to pass to the 'generate_with_langchain_docs' method.\n    You can input 'with_debugging_logs', 'is_async', 'raise_exceptions', and 'run_config'."
      }
    ],
    "return_type": "<class 'pandas.core.frame.DataFrame'>",
    "docstring": "QA dataset generation using RAGAS.\nReturns qa dataset dataframe.\n\n:param corpus_df: Corpus dataframe.\n:param test_size: Number of queries to generate.\n:param distributions: Distributions of different types of questions.\n    Default is \"simple is 0.5, multi_context is 0.4, and reasoning is 0.1.\"\n    Each type of questions refers to Ragas evolution types.\n:param generator_llm: Generator language model from Langchain.\n:param critic_llm: Critic language model from Langchain.\n:param embedding_model: Embedding model from Langchain.\n:param kwargs: The additional option to pass to the 'generate_with_langchain_docs' method.\n    You can input 'with_debugging_logs', 'is_async', 'raise_exceptions', and 'run_config'.\n:return: QA dataset dataframe."
  },
  {
    "function": "generate_qa_row",
    "module": "autorag.data.qacreation.simple",
    "params": [
      {
        "name": "llm",
        "param_type": "<class 'guidance.models._model.Model'>",
        "description": "guidance model"
      },
      {
        "name": "corpus_data_row",
        "param_type": "Any",
        "description": "need \"contents\" column"
      }
    ],
    "return_type": "Any",
    "docstring": "this sample code to generate rag dataset using OpenAI chat model\n\n:param llm: guidance model\n:param corpus_data_row: need \"contents\" column\n:return: should to be dict which has \"query\", \"generation_gt\" columns at least."
  },
  {
    "function": "generate_simple_qa_dataset",
    "module": "autorag.data.qacreation.simple",
    "params": [
      {
        "name": "llm",
        "param_type": "<class 'guidance.models._model.Model'>",
        "description": "guidance.models.Model"
      },
      {
        "name": "corpus_data",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": "pd.DataFrame. refer to the basic structure"
      },
      {
        "name": "output_filepath",
        "param_type": "<class 'str'>",
        "description": "file_dir must exist, filepath must not exist. file extension must be .parquet"
      },
      {
        "name": "generate_row_function",
        "param_type": "typing.Callable",
        "description": "input(llm, corpus_data_row, kwargs) output(dict[columns contain \"query\" and \"generation_gt\"])"
      },
      {
        "name": "kwargs",
        "param_type": "Any",
        "description": "if generate_row_function requires more args, use kwargs"
      }
    ],
    "return_type": "Any",
    "docstring": "corpus_data to qa_dataset\nqa_dataset will be saved to filepath(file_dir/filename)\n\n:param llm: guidance.models.Model\n:param corpus_data: pd.DataFrame. refer to the basic structure\n:param output_filepath: file_dir must exist, filepath must not exist. file extension must be .parquet\n:param generate_row_function: input(llm, corpus_data_row, kwargs) output(dict[columns contain \"query\" and \"generation_gt\"])\n:param kwargs: if generate_row_function requires more args, use kwargs\n:return: qa_dataset as pd.DataFrame"
  },
  {
    "function": "add_essential_metadata",
    "module": "autorag.data.utils.util",
    "params": [
      {
        "name": "metadata",
        "param_type": "typing.Dict",
        "description": ""
      }
    ],
    "return_type": "typing.Dict",
    "docstring": null
  },
  {
    "function": "corpus_df_to_langchain_documents",
    "module": "autorag.data.utils.util",
    "params": [
      {
        "name": "corpus_df",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": ""
      }
    ],
    "return_type": "typing.List[langchain_core.documents.base.Document]",
    "docstring": null
  },
  {
    "function": "get_file_metadata",
    "module": "autorag.data.utils.util",
    "params": [
      {
        "name": "file_path",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "typing.Dict",
    "docstring": "Get some handy metadate from filesystem.\n\nArgs:\n    file_path: str: file path in str"
  },
  {
    "function": "Runner.__add_api_route",
    "module": "autorag.deploy",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "Runner.__init__",
    "module": "autorag.deploy",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "config",
        "param_type": "typing.Dict",
        "description": ""
      },
      {
        "name": "project_dir",
        "param_type": "typing.Optional[str]",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Initialize self.  See help(type(self)) for accurate signature."
  },
  {
    "function": "Runner.from_trial_folder",
    "module": "autorag.deploy",
    "params": [
      {
        "name": "trial_path",
        "param_type": "<class 'str'>",
        "description": "The path of the trial folder."
      }
    ],
    "return_type": "Any",
    "docstring": "Load Runner from evaluated trial folder.\nMust already be evaluated using Evaluator class.\nIt sets the project_dir as the parent directory of the trial folder.\n\n:param trial_path: The path of the trial folder.\n:return: Initialized Runner."
  },
  {
    "function": "Runner.from_yaml",
    "module": "autorag.deploy",
    "params": [
      {
        "name": "yaml_path",
        "param_type": "<class 'str'>",
        "description": "The path of the yaml file."
      },
      {
        "name": "project_dir",
        "param_type": "typing.Optional[str]",
        "description": "The path of the project directory.\n    Default is the current directory."
      }
    ],
    "return_type": "Any",
    "docstring": "Load Runner from yaml file.\nMust be extracted yaml file from evaluated trial using extract_best_config method.\n\n:param yaml_path: The path of the yaml file.\n:param project_dir: The path of the project directory.\n    Default is the current directory.\n:return: Initialized Runner."
  },
  {
    "function": "Runner.run",
    "module": "autorag.deploy",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "query",
        "param_type": "<class 'str'>",
        "description": "The query of the user."
      },
      {
        "name": "result_column",
        "param_type": "<class 'str'>",
        "description": "The result column name for the answer.\n    Default is `generated_texts`, which is the output of the `generation` module."
      }
    ],
    "return_type": "Any",
    "docstring": "Run the pipeline with query.\nThe loaded pipeline must start with a single query,\nso the first module of the pipeline must be `query_expansion` or `retrieval` module.\n\n:param query: The query of the user.\n:param result_column: The result column name for the answer.\n    Default is `generated_texts`, which is the output of the `generation` module.\n:return: The result of the pipeline."
  },
  {
    "function": "Runner.run_api_server",
    "module": "autorag.deploy",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "host",
        "param_type": "<class 'str'>",
        "description": "The host of the api server."
      },
      {
        "name": "port",
        "param_type": "<class 'int'>",
        "description": "The port of the api server."
      },
      {
        "name": "kwargs",
        "param_type": "Any",
        "description": "Other arguments for uvicorn.run."
      }
    ],
    "return_type": "Any",
    "docstring": "Run the pipeline as api server.\nYou can send POST request to `http://host:port/run` with json body like below:\n\n.. Code:: json\n\n    {\n        \"Query\": \"your query\",\n        \"result_column\": \"answer\"\n    }\n\nAnd it returns json response like below:\n\n.. Code:: json\n\n    {\n        \"answer\": \"your answer\"\n    }\n\n:param host: The host of the api server.\n:param port: The port of the api server.\n:param kwargs: Other arguments for uvicorn.run."
  },
  {
    "function": "extract_best_config",
    "module": "autorag.deploy",
    "params": [
      {
        "name": "trial_path",
        "param_type": "<class 'str'>",
        "description": "The path to the trial directory that you want to extract the pipeline from.\n    Must already be evaluated."
      },
      {
        "name": "output_path",
        "param_type": "typing.Optional[str]",
        "description": "Output path that pipeline yaml file will be saved.\n    Must be .yaml or .yml file.\n    If None, it does not save yaml file and just return dict values.\n    Default is None."
      }
    ],
    "return_type": "typing.Dict",
    "docstring": "Extract the optimal pipeline from evaluated trial.\n\n:param trial_path: The path to the trial directory that you want to extract the pipeline from.\n    Must already be evaluated.\n:param output_path: Output path that pipeline yaml file will be saved.\n    Must be .yaml or .yml file.\n    If None, it does not save yaml file and just return dict values.\n    Default is None.\n:return: The dictionary of the extracted pipeline."
  },
  {
    "function": "extract_node_line_names",
    "module": "autorag.deploy",
    "params": [
      {
        "name": "config_dict",
        "param_type": "typing.Dict",
        "description": "The yaml configuration dict for the pipeline.\n    You can load this to access trail_folder/config.yaml."
      }
    ],
    "return_type": "typing.List[str]",
    "docstring": "Extract node line names with the given config dictionary order.\n\n:param config_dict: The yaml configuration dict for the pipeline.\n    You can load this to access trail_folder/config.yaml.\n:return: The list of node line names.\n    It is the order of the node line names in the pipeline."
  },
  {
    "function": "extract_node_strategy",
    "module": "autorag.deploy",
    "params": [
      {
        "name": "config_dict",
        "param_type": "typing.Dict",
        "description": "The yaml configuration dict for the pipeline.\n    You can load this to access trail_folder/config.yaml."
      }
    ],
    "return_type": "typing.Dict",
    "docstring": "Extract node strategies with the given config dictionary.\nThe return value is a dictionary of node type and its strategy.\n\n:param config_dict: The yaml configuration dict for the pipeline.\n    You can load this to access trail_folder/config.yaml.\n:return: Key is node_type and value is strategy dict."
  },
  {
    "function": "get_support_modules",
    "module": "autorag.support",
    "params": [
      {
        "name": "module_name",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "typing.Callable",
    "docstring": null
  },
  {
    "function": "load_summary_file",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "summary_path",
        "param_type": "<class 'str'>",
        "description": "The path of the summary file."
      },
      {
        "name": "dict_columns",
        "param_type": "typing.Optional[typing.List[str]]",
        "description": "The columns that are dictionary type.\n    You must fill this parameter if you want to load summary file properly.\n    Default is ['module_params']."
      }
    ],
    "return_type": "<class 'pandas.core.frame.DataFrame'>",
    "docstring": "Load summary file from summary_path.\n\n:param summary_path: The path of the summary file.\n:param dict_columns: The columns that are dictionary type.\n    You must fill this parameter if you want to load summary file properly.\n    Default is ['module_params'].\n:return: The summary dataframe."
  },
  {
    "function": "summary_df_to_yaml",
    "module": "autorag.deploy",
    "params": [
      {
        "name": "summary_df",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": "The trial summary dataframe of the evaluated trial."
      },
      {
        "name": "config_dict",
        "param_type": "typing.Dict",
        "description": "The yaml configuration dict for the pipeline.\n    You can load this to access trail_folder/config.yaml."
      }
    ],
    "return_type": "typing.Dict",
    "docstring": "Convert trial summary dataframe to config yaml file.\n\n:param summary_df: The trial summary dataframe of the evaluated trial.\n:param config_dict: The yaml configuration dict for the pipeline.\n    You can load this to access trail_folder/config.yaml.\n:return: Dictionary of config yaml file.\n    You can save this dictionary to yaml file."
  },
  {
    "function": "evaluate_generation",
    "module": "autorag.evaluation.generation",
    "params": [
      {
        "name": "generation_gt",
        "param_type": "typing.List[typing.List[str]]",
        "description": ""
      },
      {
        "name": "metrics",
        "param_type": "typing.Union[typing.List[str], typing.List[typing.Dict]]",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "evaluate_retrieval",
    "module": "autorag.evaluation.retrieval",
    "params": [
      {
        "name": "retrieval_gt",
        "param_type": "typing.List[typing.List[typing.List[str]]]",
        "description": ""
      },
      {
        "name": "metrics",
        "param_type": "typing.Union[typing.List[str], typing.List[typing.Dict]]",
        "description": ""
      },
      {
        "name": "queries",
        "param_type": "typing.Optional[typing.List[str]]",
        "description": ""
      },
      {
        "name": "generation_gt",
        "param_type": "typing.Optional[typing.List[typing.List[str]]]",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "evaluate_retrieval_contents",
    "module": "autorag.evaluation.retrieval_contents",
    "params": [
      {
        "name": "retrieval_gt",
        "param_type": "typing.List[typing.List[str]]",
        "description": ""
      },
      {
        "name": "metrics",
        "param_type": "typing.List[str]",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "bert_score",
    "module": "autorag.evaluation.metric.generation",
    "params": [
      {
        "name": "generation_gt",
        "param_type": "typing.List[typing.List[str]]",
        "description": ""
      },
      {
        "name": "generations",
        "param_type": "typing.List[str]",
        "description": ""
      },
      {
        "name": "lang",
        "param_type": "<class 'str'>",
        "description": ""
      },
      {
        "name": "batch",
        "param_type": "<class 'int'>",
        "description": ""
      },
      {
        "name": "n_threads",
        "param_type": "<class 'int'>",
        "description": ""
      }
    ],
    "return_type": "typing.List[float]",
    "docstring": null
  },
  {
    "function": "bleu",
    "module": "autorag.evaluation.metric.generation",
    "params": [
      {
        "name": "generation_gt",
        "param_type": "typing.List[typing.List[str]]",
        "description": "A list of ground truth.\n    Must be 2-d list of string.\n    Because it can be a multiple ground truth."
      },
      {
        "name": "generations",
        "param_type": "[<class 'str'>]",
        "description": "A list of generations that LLM generated."
      },
      {
        "name": "tokenize",
        "param_type": "str | None",
        "description": "The tokenizer to use. If None, defaults to language-specific tokenizers with '13a' as the fallback default. check #https://github.com/mjpost/sacrebleu/blob/master/sacrebleu/metrics/bleu.py"
      },
      {
        "name": "smooth_method",
        "param_type": "<class 'str'>",
        "description": "The smoothing method to use ('floor', 'add-k', 'exp' or 'none')."
      },
      {
        "name": "smooth_value",
        "param_type": "typing.Optional[float]",
        "description": "The smoothing value for `floor` and `add-k` methods. `None` falls back to default value."
      },
      {
        "name": "max_ngram_order",
        "param_type": "<class 'int'>",
        "description": "If given, it overrides the maximum n-gram order (default: 4) when computing precisions."
      },
      {
        "name": "trg_lang",
        "param_type": "<class 'str'>",
        "description": "An optional language code to raise potential tokenizer warnings."
      },
      {
        "name": "kwargs",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "typing.List[float]",
    "docstring": "Computes the BLEU metric given pred and ground-truth.\n\n:param tokenize: The tokenizer to use. If None, defaults to language-specific tokenizers with '13a' as the fallback default. check #https://github.com/mjpost/sacrebleu/blob/master/sacrebleu/metrics/bleu.py\n:param smooth_method: The smoothing method to use ('floor', 'add-k', 'exp' or 'none').\n:param smooth_value: The smoothing value for `floor` and `add-k` methods. `None` falls back to default value.\n:param max_ngram_order: If given, it overrides the maximum n-gram order (default: 4) when computing precisions.\n:param trg_lang: An optional language code to raise potential tokenizer warnings.\n:param generation_gt: A list of ground truth.\n    Must be 2-d list of string.\n    Because it can be a multiple ground truth.\n:param generations: A list of generations that LLM generated."
  },
  {
    "function": "cast_metrics",
    "module": "autorag.evaluation.util",
    "params": [
      {
        "name": "metrics",
        "param_type": "typing.Union[typing.List[str], typing.List[typing.Dict]]",
        "description": "List of string or dictionary."
      }
    ],
    "return_type": "typing.Tuple[typing.List[str], typing.List[typing.Dict[str, typing.Any]]]",
    "docstring": " Turn metrics to list of metric names and parameter list.\n\n:param metrics: List of string or dictionary.\n:return: The list of metric names and dictionary list of metric parameters."
  },
  {
    "function": "evaluate_generation",
    "module": "autorag.evaluation.generation",
    "params": [
      {
        "name": "generation_gt",
        "param_type": "typing.List[typing.List[str]]",
        "description": ""
      },
      {
        "name": "metrics",
        "param_type": "typing.Union[typing.List[str], typing.List[typing.Dict]]",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "g_eval",
    "module": "autorag.evaluation.metric.generation",
    "params": [
      {
        "name": "generation_gt",
        "param_type": "typing.List[typing.List[str]]",
        "description": "A list of ground truth.\n    Must be 2-d list of string.\n    Because it can be a multiple ground truth.\n    It will get the max of g_eval score."
      },
      {
        "name": "generations",
        "param_type": "typing.List[str]",
        "description": "A list of generations that LLM generated."
      },
      {
        "name": "metrics",
        "param_type": "typing.Optional[typing.List[str]]",
        "description": "A list of metrics to use for evaluation.\n    Default is all metrics, which is ['coherence', 'consistency', 'fluency', 'relevance']."
      },
      {
        "name": "model",
        "param_type": "<class 'str'>",
        "description": "OpenAI model name.\n    Default is 'gpt-4-0125-preview'."
      },
      {
        "name": "batch_size",
        "param_type": "<class 'int'>",
        "description": "The batch size for processing.\n    Default is 8."
      }
    ],
    "return_type": "typing.List[float]",
    "docstring": "Calculate G-Eval score.\nG-eval is a metric that uses high-performance LLM model to evaluate generation performance.\nIt evaluates the generation result by coherence, consistency, fluency, and relevance.\nIt uses only 'openai' model, and we recommend to use gpt-4 for evaluation accuracy.\n\n:param generation_gt: A list of ground truth.\n    Must be 2-d list of string.\n    Because it can be a multiple ground truth.\n    It will get the max of g_eval score.\n:param generations: A list of generations that LLM generated.\n:param metrics: A list of metrics to use for evaluation.\n    Default is all metrics, which is ['coherence', 'consistency', 'fluency', 'relevance'].\n:param model: OpenAI model name.\n    Default is 'gpt-4-0125-preview'.\n:param batch_size: The batch size for processing.\n    Default is 8.\n:return: G-Eval score."
  },
  {
    "function": "meteor",
    "module": "autorag.evaluation.metric.generation",
    "params": [
      {
        "name": "generation_gt",
        "param_type": "typing.List[typing.List[str]]",
        "description": "A list of ground truth.\n        Must be 2-d list of string.\n        Because it can be a multiple ground truth."
      },
      {
        "name": "generations",
        "param_type": "typing.List[str]",
        "description": "A list of generations that LLM generated."
      },
      {
        "name": "alpha",
        "param_type": "<class 'float'>",
        "description": "Parameter for controlling relative weights of precision and recall.\n    Default is 0.9."
      },
      {
        "name": "beta",
        "param_type": "<class 'float'>",
        "description": "Parameter for controlling shape of penalty as a\n    function of as a function of fragmentation.\n    Default is 3.0."
      },
      {
        "name": "gamma",
        "param_type": "<class 'float'>",
        "description": "Relative weight assigned to fragmentation penalty.\n    Default is 0.5."
      }
    ],
    "return_type": "typing.List[float]",
    "docstring": "Compute meteor score for generation.\n\n:param generation_gt: A list of ground truth.\n        Must be 2-d list of string.\n        Because it can be a multiple ground truth.\n:param generations: A list of generations that LLM generated.\n:param alpha: Parameter for controlling relative weights of precision and recall.\n    Default is 0.9.\n:param beta: Parameter for controlling shape of penalty as a\n    function of as a function of fragmentation.\n    Default is 3.0.\n:param gamma: Relative weight assigned to fragmentation penalty.\n    Default is 0.5.\n:return: A list of computed metric scores."
  },
  {
    "function": "rouge",
    "module": "autorag.evaluation.metric.generation",
    "params": [
      {
        "name": "generation_gt",
        "param_type": "typing.List[typing.List[str]]",
        "description": "A list of ground truth.\n            Must be 2-d list of string.\n            Because it can be a multiple ground truth."
      },
      {
        "name": "generations",
        "param_type": "typing.List[str]",
        "description": "A list of generations that LLM generated."
      },
      {
        "name": "rouge_type",
        "param_type": "typing.Optional[str]",
        "description": "A rouge type to use for evaluation.\n        Default is 'RougeL'.\n        Choose between rouge1, rouge2, rougeL, and rougeLSum.\n        - rouge1: unigram (1-gram) based scoring.\n        - rouge2: bigram (2-gram) based scoring.\n        - rougeL: Longest Common Subsequence based scoring.\n        - rougeLSum: splits text using \"\n\""
      },
      {
        "name": "use_stemmer",
        "param_type": "<class 'bool'>",
        "description": "Bool indicating whether Porter stemmer should be used to\n        strip word suffixes to improve matching. This arg is used in the\n        DefaultTokenizer, but other tokenizers might or might not choose to\n        use this. Default is False."
      },
      {
        "name": "split_summaries",
        "param_type": "<class 'bool'>",
        "description": "Whether to add newlines between sentences for rougeLsum.\n        Default is False."
      },
      {
        "name": "batch",
        "param_type": "<class 'int'>",
        "description": "The batch size for processing.\n        Default is your cpu count."
      }
    ],
    "return_type": "typing.List[float]",
    "docstring": "    Compute rouge score for generation.\n\n    :param generation_gt: A list of ground truth.\n            Must be 2-d list of string.\n            Because it can be a multiple ground truth.\n    :param generations: A list of generations that LLM generated.\n    :param rouge_type: A rouge type to use for evaluation.\n        Default is 'RougeL'.\n        Choose between rouge1, rouge2, rougeL, and rougeLSum.\n        - rouge1: unigram (1-gram) based scoring.\n        - rouge2: bigram (2-gram) based scoring.\n        - rougeL: Longest Common Subsequence based scoring.\n        - rougeLSum: splits text using \"\n\"\n    :param use_stemmer: Bool indicating whether Porter stemmer should be used to\n        strip word suffixes to improve matching. This arg is used in the\n        DefaultTokenizer, but other tokenizers might or might not choose to\n        use this. Default is False.\n    :param split_summaries: Whether to add newlines between sentences for rougeLsum.\n        Default is False.\n    :param batch: The batch size for processing.\n        Default is your cpu count.\n    :return: A list of computed metric scores.\n    "
  },
  {
    "function": "sem_score",
    "module": "autorag.evaluation.metric.generation",
    "params": [
      {
        "name": "generation_gt",
        "param_type": "typing.List[typing.List[str]]",
        "description": "A list of ground truth.\n        Must be 2-d list of string.\n        Because it can be a multiple ground truth.\n        It will get the max of cosine similarity between generation_gt and pred."
      },
      {
        "name": "generations",
        "param_type": "typing.List[str]",
        "description": "A list of generations that LLM generated."
      },
      {
        "name": "embedding_model",
        "param_type": "typing.Optional[llama_index.core.base.embeddings.base.BaseEmbedding]",
        "description": "Embedding model to use for compute cosine similarity.\n    Default is all-mpnet-base-v2 embedding model.\n    The paper used this embedding model."
      },
      {
        "name": "batch",
        "param_type": "<class 'int'>",
        "description": "The batch size for processing.\n    Default is 128"
      }
    ],
    "return_type": "typing.List[float]",
    "docstring": "Compute sem score between generation gt and pred with cosine similarity.\n\n:param generation_gt: A list of ground truth.\n        Must be 2-d list of string.\n        Because it can be a multiple ground truth.\n        It will get the max of cosine similarity between generation_gt and pred.\n:param generations: A list of generations that LLM generated.\n:param embedding_model: Embedding model to use for compute cosine similarity.\n    Default is all-mpnet-base-v2 embedding model.\n    The paper used this embedding model.\n:param batch: The batch size for processing.\n    Default is 128\n:return: A list of computed metric scores."
  },
  {
    "function": "bert_score",
    "module": "autorag.evaluation.metric.generation",
    "params": [
      {
        "name": "generation_gt",
        "param_type": "typing.List[typing.List[str]]",
        "description": ""
      },
      {
        "name": "generations",
        "param_type": "typing.List[str]",
        "description": ""
      },
      {
        "name": "lang",
        "param_type": "<class 'str'>",
        "description": ""
      },
      {
        "name": "batch",
        "param_type": "<class 'int'>",
        "description": ""
      },
      {
        "name": "n_threads",
        "param_type": "<class 'int'>",
        "description": ""
      }
    ],
    "return_type": "typing.List[float]",
    "docstring": null
  },
  {
    "function": "bleu",
    "module": "autorag.evaluation.metric.generation",
    "params": [
      {
        "name": "generation_gt",
        "param_type": "typing.List[typing.List[str]]",
        "description": "A list of ground truth.\n    Must be 2-d list of string.\n    Because it can be a multiple ground truth."
      },
      {
        "name": "generations",
        "param_type": "[<class 'str'>]",
        "description": "A list of generations that LLM generated."
      },
      {
        "name": "tokenize",
        "param_type": "str | None",
        "description": "The tokenizer to use. If None, defaults to language-specific tokenizers with '13a' as the fallback default. check #https://github.com/mjpost/sacrebleu/blob/master/sacrebleu/metrics/bleu.py"
      },
      {
        "name": "smooth_method",
        "param_type": "<class 'str'>",
        "description": "The smoothing method to use ('floor', 'add-k', 'exp' or 'none')."
      },
      {
        "name": "smooth_value",
        "param_type": "typing.Optional[float]",
        "description": "The smoothing value for `floor` and `add-k` methods. `None` falls back to default value."
      },
      {
        "name": "max_ngram_order",
        "param_type": "<class 'int'>",
        "description": "If given, it overrides the maximum n-gram order (default: 4) when computing precisions."
      },
      {
        "name": "trg_lang",
        "param_type": "<class 'str'>",
        "description": "An optional language code to raise potential tokenizer warnings."
      },
      {
        "name": "kwargs",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "typing.List[float]",
    "docstring": "Computes the BLEU metric given pred and ground-truth.\n\n:param tokenize: The tokenizer to use. If None, defaults to language-specific tokenizers with '13a' as the fallback default. check #https://github.com/mjpost/sacrebleu/blob/master/sacrebleu/metrics/bleu.py\n:param smooth_method: The smoothing method to use ('floor', 'add-k', 'exp' or 'none').\n:param smooth_value: The smoothing value for `floor` and `add-k` methods. `None` falls back to default value.\n:param max_ngram_order: If given, it overrides the maximum n-gram order (default: 4) when computing precisions.\n:param trg_lang: An optional language code to raise potential tokenizer warnings.\n:param generation_gt: A list of ground truth.\n    Must be 2-d list of string.\n    Because it can be a multiple ground truth.\n:param generations: A list of generations that LLM generated."
  },
  {
    "function": "g_eval",
    "module": "autorag.evaluation.metric.generation",
    "params": [
      {
        "name": "generation_gt",
        "param_type": "typing.List[typing.List[str]]",
        "description": "A list of ground truth.\n    Must be 2-d list of string.\n    Because it can be a multiple ground truth.\n    It will get the max of g_eval score."
      },
      {
        "name": "generations",
        "param_type": "typing.List[str]",
        "description": "A list of generations that LLM generated."
      },
      {
        "name": "metrics",
        "param_type": "typing.Optional[typing.List[str]]",
        "description": "A list of metrics to use for evaluation.\n    Default is all metrics, which is ['coherence', 'consistency', 'fluency', 'relevance']."
      },
      {
        "name": "model",
        "param_type": "<class 'str'>",
        "description": "OpenAI model name.\n    Default is 'gpt-4-0125-preview'."
      },
      {
        "name": "batch_size",
        "param_type": "<class 'int'>",
        "description": "The batch size for processing.\n    Default is 8."
      }
    ],
    "return_type": "typing.List[float]",
    "docstring": "Calculate G-Eval score.\nG-eval is a metric that uses high-performance LLM model to evaluate generation performance.\nIt evaluates the generation result by coherence, consistency, fluency, and relevance.\nIt uses only 'openai' model, and we recommend to use gpt-4 for evaluation accuracy.\n\n:param generation_gt: A list of ground truth.\n    Must be 2-d list of string.\n    Because it can be a multiple ground truth.\n    It will get the max of g_eval score.\n:param generations: A list of generations that LLM generated.\n:param metrics: A list of metrics to use for evaluation.\n    Default is all metrics, which is ['coherence', 'consistency', 'fluency', 'relevance'].\n:param model: OpenAI model name.\n    Default is 'gpt-4-0125-preview'.\n:param batch_size: The batch size for processing.\n    Default is 8.\n:return: G-Eval score."
  },
  {
    "function": "meteor",
    "module": "autorag.evaluation.metric.generation",
    "params": [
      {
        "name": "generation_gt",
        "param_type": "typing.List[typing.List[str]]",
        "description": "A list of ground truth.\n        Must be 2-d list of string.\n        Because it can be a multiple ground truth."
      },
      {
        "name": "generations",
        "param_type": "typing.List[str]",
        "description": "A list of generations that LLM generated."
      },
      {
        "name": "alpha",
        "param_type": "<class 'float'>",
        "description": "Parameter for controlling relative weights of precision and recall.\n    Default is 0.9."
      },
      {
        "name": "beta",
        "param_type": "<class 'float'>",
        "description": "Parameter for controlling shape of penalty as a\n    function of as a function of fragmentation.\n    Default is 3.0."
      },
      {
        "name": "gamma",
        "param_type": "<class 'float'>",
        "description": "Relative weight assigned to fragmentation penalty.\n    Default is 0.5."
      }
    ],
    "return_type": "typing.List[float]",
    "docstring": "Compute meteor score for generation.\n\n:param generation_gt: A list of ground truth.\n        Must be 2-d list of string.\n        Because it can be a multiple ground truth.\n:param generations: A list of generations that LLM generated.\n:param alpha: Parameter for controlling relative weights of precision and recall.\n    Default is 0.9.\n:param beta: Parameter for controlling shape of penalty as a\n    function of as a function of fragmentation.\n    Default is 3.0.\n:param gamma: Relative weight assigned to fragmentation penalty.\n    Default is 0.5.\n:return: A list of computed metric scores."
  },
  {
    "function": "retrieval_f1",
    "module": "autorag.evaluation.metric.retrieval",
    "params": [
      {
        "name": "gt",
        "param_type": "typing.List[typing.List[str]]",
        "description": "2-d list of ground truth ids.\n    It contains and/or connections between ids."
      },
      {
        "name": "pred",
        "param_type": "typing.List[str]",
        "description": "Prediction ids."
      }
    ],
    "return_type": "Any",
    "docstring": "Compute f1 score for retrieval.\n\n:param gt: 2-d list of ground truth ids.\n    It contains and/or connections between ids.\n:param pred: Prediction ids.\n:return: The f1 score."
  },
  {
    "function": "retrieval_map",
    "module": "autorag.evaluation.metric.retrieval",
    "params": [
      {
        "name": "gt",
        "param_type": "typing.List[typing.List[str]]",
        "description": ""
      },
      {
        "name": "pred",
        "param_type": "typing.List[str]",
        "description": ""
      }
    ],
    "return_type": "<class 'float'>",
    "docstring": "Mean Average Precision (MAP) is the mean of Average Precision (AP) for all queries."
  },
  {
    "function": "retrieval_mrr",
    "module": "autorag.evaluation.metric.retrieval",
    "params": [
      {
        "name": "gt",
        "param_type": "typing.List[typing.List[str]]",
        "description": ""
      },
      {
        "name": "pred",
        "param_type": "typing.List[str]",
        "description": ""
      }
    ],
    "return_type": "<class 'float'>",
    "docstring": "Reciprocal Rank (RR) is the reciprocal of the rank of the first relevant item.\nMean of RR in whole queries is MRR."
  },
  {
    "function": "retrieval_ndcg",
    "module": "autorag.evaluation.metric.retrieval",
    "params": [
      {
        "name": "gt",
        "param_type": "typing.List[typing.List[str]]",
        "description": ""
      },
      {
        "name": "pred",
        "param_type": "typing.List[str]",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "retrieval_precision",
    "module": "autorag.evaluation.metric.retrieval",
    "params": [
      {
        "name": "gt",
        "param_type": "typing.List[typing.List[str]]",
        "description": ""
      },
      {
        "name": "pred",
        "param_type": "typing.List[str]",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "retrieval_recall",
    "module": "autorag.evaluation.metric.retrieval",
    "params": [
      {
        "name": "gt",
        "param_type": "typing.List[typing.List[str]]",
        "description": ""
      },
      {
        "name": "pred",
        "param_type": "typing.List[str]",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "retrieval_token_f1",
    "module": "autorag.evaluation.metric.retrieval_contents",
    "params": [
      {
        "name": "gt",
        "param_type": "typing.List[str]",
        "description": ""
      },
      {
        "name": "pred",
        "param_type": "typing.List[str]",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "retrieval_token_precision",
    "module": "autorag.evaluation.metric.retrieval_contents",
    "params": [
      {
        "name": "gt",
        "param_type": "typing.List[str]",
        "description": ""
      },
      {
        "name": "pred",
        "param_type": "typing.List[str]",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "retrieval_token_recall",
    "module": "autorag.evaluation.metric.retrieval_contents",
    "params": [
      {
        "name": "gt",
        "param_type": "typing.List[str]",
        "description": ""
      },
      {
        "name": "pred",
        "param_type": "typing.List[str]",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "rouge",
    "module": "autorag.evaluation.metric.generation",
    "params": [
      {
        "name": "generation_gt",
        "param_type": "typing.List[typing.List[str]]",
        "description": "A list of ground truth.\n            Must be 2-d list of string.\n            Because it can be a multiple ground truth."
      },
      {
        "name": "generations",
        "param_type": "typing.List[str]",
        "description": "A list of generations that LLM generated."
      },
      {
        "name": "rouge_type",
        "param_type": "typing.Optional[str]",
        "description": "A rouge type to use for evaluation.\n        Default is 'RougeL'.\n        Choose between rouge1, rouge2, rougeL, and rougeLSum.\n        - rouge1: unigram (1-gram) based scoring.\n        - rouge2: bigram (2-gram) based scoring.\n        - rougeL: Longest Common Subsequence based scoring.\n        - rougeLSum: splits text using \"\n\""
      },
      {
        "name": "use_stemmer",
        "param_type": "<class 'bool'>",
        "description": "Bool indicating whether Porter stemmer should be used to\n        strip word suffixes to improve matching. This arg is used in the\n        DefaultTokenizer, but other tokenizers might or might not choose to\n        use this. Default is False."
      },
      {
        "name": "split_summaries",
        "param_type": "<class 'bool'>",
        "description": "Whether to add newlines between sentences for rougeLsum.\n        Default is False."
      },
      {
        "name": "batch",
        "param_type": "<class 'int'>",
        "description": "The batch size for processing.\n        Default is your cpu count."
      }
    ],
    "return_type": "typing.List[float]",
    "docstring": "    Compute rouge score for generation.\n\n    :param generation_gt: A list of ground truth.\n            Must be 2-d list of string.\n            Because it can be a multiple ground truth.\n    :param generations: A list of generations that LLM generated.\n    :param rouge_type: A rouge type to use for evaluation.\n        Default is 'RougeL'.\n        Choose between rouge1, rouge2, rougeL, and rougeLSum.\n        - rouge1: unigram (1-gram) based scoring.\n        - rouge2: bigram (2-gram) based scoring.\n        - rougeL: Longest Common Subsequence based scoring.\n        - rougeLSum: splits text using \"\n\"\n    :param use_stemmer: Bool indicating whether Porter stemmer should be used to\n        strip word suffixes to improve matching. This arg is used in the\n        DefaultTokenizer, but other tokenizers might or might not choose to\n        use this. Default is False.\n    :param split_summaries: Whether to add newlines between sentences for rougeLsum.\n        Default is False.\n    :param batch: The batch size for processing.\n        Default is your cpu count.\n    :return: A list of computed metric scores.\n    "
  },
  {
    "function": "sem_score",
    "module": "autorag.evaluation.metric.generation",
    "params": [
      {
        "name": "generation_gt",
        "param_type": "typing.List[typing.List[str]]",
        "description": "A list of ground truth.\n        Must be 2-d list of string.\n        Because it can be a multiple ground truth.\n        It will get the max of cosine similarity between generation_gt and pred."
      },
      {
        "name": "generations",
        "param_type": "typing.List[str]",
        "description": "A list of generations that LLM generated."
      },
      {
        "name": "embedding_model",
        "param_type": "typing.Optional[llama_index.core.base.embeddings.base.BaseEmbedding]",
        "description": "Embedding model to use for compute cosine similarity.\n    Default is all-mpnet-base-v2 embedding model.\n    The paper used this embedding model."
      },
      {
        "name": "batch",
        "param_type": "<class 'int'>",
        "description": "The batch size for processing.\n    Default is 128"
      }
    ],
    "return_type": "typing.List[float]",
    "docstring": "Compute sem score between generation gt and pred with cosine similarity.\n\n:param generation_gt: A list of ground truth.\n        Must be 2-d list of string.\n        Because it can be a multiple ground truth.\n        It will get the max of cosine similarity between generation_gt and pred.\n:param generations: A list of generations that LLM generated.\n:param embedding_model: Embedding model to use for compute cosine similarity.\n    Default is all-mpnet-base-v2 embedding model.\n    The paper used this embedding model.\n:param batch: The batch size for processing.\n    Default is 128\n:return: A list of computed metric scores."
  },
  {
    "function": "async_g_eval",
    "module": "autorag.evaluation.metric.generation",
    "params": [
      {
        "name": "generation_gt",
        "param_type": "typing.List[str]",
        "description": ""
      },
      {
        "name": "pred",
        "param_type": "<class 'str'>",
        "description": ""
      },
      {
        "name": "metrics",
        "param_type": "typing.Optional[typing.List[str]]",
        "description": ""
      },
      {
        "name": "model",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "<class 'float'>",
    "docstring": null
  },
  {
    "function": "bert_score",
    "module": "autorag.evaluation.metric.generation",
    "params": [
      {
        "name": "generation_gt",
        "param_type": "typing.List[typing.List[str]]",
        "description": ""
      },
      {
        "name": "generations",
        "param_type": "typing.List[str]",
        "description": ""
      },
      {
        "name": "lang",
        "param_type": "<class 'str'>",
        "description": ""
      },
      {
        "name": "batch",
        "param_type": "<class 'int'>",
        "description": ""
      },
      {
        "name": "n_threads",
        "param_type": "<class 'int'>",
        "description": ""
      }
    ],
    "return_type": "typing.List[float]",
    "docstring": null
  },
  {
    "function": "bleu",
    "module": "autorag.evaluation.metric.generation",
    "params": [
      {
        "name": "generation_gt",
        "param_type": "typing.List[typing.List[str]]",
        "description": "A list of ground truth.\n    Must be 2-d list of string.\n    Because it can be a multiple ground truth."
      },
      {
        "name": "generations",
        "param_type": "[<class 'str'>]",
        "description": "A list of generations that LLM generated."
      },
      {
        "name": "tokenize",
        "param_type": "str | None",
        "description": "The tokenizer to use. If None, defaults to language-specific tokenizers with '13a' as the fallback default. check #https://github.com/mjpost/sacrebleu/blob/master/sacrebleu/metrics/bleu.py"
      },
      {
        "name": "smooth_method",
        "param_type": "<class 'str'>",
        "description": "The smoothing method to use ('floor', 'add-k', 'exp' or 'none')."
      },
      {
        "name": "smooth_value",
        "param_type": "typing.Optional[float]",
        "description": "The smoothing value for `floor` and `add-k` methods. `None` falls back to default value."
      },
      {
        "name": "max_ngram_order",
        "param_type": "<class 'int'>",
        "description": "If given, it overrides the maximum n-gram order (default: 4) when computing precisions."
      },
      {
        "name": "trg_lang",
        "param_type": "<class 'str'>",
        "description": "An optional language code to raise potential tokenizer warnings."
      },
      {
        "name": "kwargs",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "typing.List[float]",
    "docstring": "Computes the BLEU metric given pred and ground-truth.\n\n:param tokenize: The tokenizer to use. If None, defaults to language-specific tokenizers with '13a' as the fallback default. check #https://github.com/mjpost/sacrebleu/blob/master/sacrebleu/metrics/bleu.py\n:param smooth_method: The smoothing method to use ('floor', 'add-k', 'exp' or 'none').\n:param smooth_value: The smoothing value for `floor` and `add-k` methods. `None` falls back to default value.\n:param max_ngram_order: If given, it overrides the maximum n-gram order (default: 4) when computing precisions.\n:param trg_lang: An optional language code to raise potential tokenizer warnings.\n:param generation_gt: A list of ground truth.\n    Must be 2-d list of string.\n    Because it can be a multiple ground truth.\n:param generations: A list of generations that LLM generated."
  },
  {
    "function": "calculate_cosine_similarity",
    "module": "autorag.evaluation.metric.util",
    "params": [
      {
        "name": "a",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "b",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "convert_inputs_to_list",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "func",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Decorator to convert all function inputs to Python lists."
  },
  {
    "function": "g_eval",
    "module": "autorag.evaluation.metric.generation",
    "params": [
      {
        "name": "generation_gt",
        "param_type": "typing.List[typing.List[str]]",
        "description": "A list of ground truth.\n    Must be 2-d list of string.\n    Because it can be a multiple ground truth.\n    It will get the max of g_eval score."
      },
      {
        "name": "generations",
        "param_type": "typing.List[str]",
        "description": "A list of generations that LLM generated."
      },
      {
        "name": "metrics",
        "param_type": "typing.Optional[typing.List[str]]",
        "description": "A list of metrics to use for evaluation.\n    Default is all metrics, which is ['coherence', 'consistency', 'fluency', 'relevance']."
      },
      {
        "name": "model",
        "param_type": "<class 'str'>",
        "description": "OpenAI model name.\n    Default is 'gpt-4-0125-preview'."
      },
      {
        "name": "batch_size",
        "param_type": "<class 'int'>",
        "description": "The batch size for processing.\n    Default is 8."
      }
    ],
    "return_type": "typing.List[float]",
    "docstring": "Calculate G-Eval score.\nG-eval is a metric that uses high-performance LLM model to evaluate generation performance.\nIt evaluates the generation result by coherence, consistency, fluency, and relevance.\nIt uses only 'openai' model, and we recommend to use gpt-4 for evaluation accuracy.\n\n:param generation_gt: A list of ground truth.\n    Must be 2-d list of string.\n    Because it can be a multiple ground truth.\n    It will get the max of g_eval score.\n:param generations: A list of generations that LLM generated.\n:param metrics: A list of metrics to use for evaluation.\n    Default is all metrics, which is ['coherence', 'consistency', 'fluency', 'relevance'].\n:param model: OpenAI model name.\n    Default is 'gpt-4-0125-preview'.\n:param batch_size: The batch size for processing.\n    Default is 8.\n:return: G-Eval score."
  },
  {
    "function": "generation_metric",
    "module": "autorag.evaluation.metric.generation",
    "params": [
      {
        "name": "func",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "huggingface_evaluate",
    "module": "autorag.evaluation.metric.generation",
    "params": [
      {
        "name": "instance",
        "param_type": "Any",
        "description": "The instance of huggingface evaluates metric."
      },
      {
        "name": "key",
        "param_type": "<class 'str'>",
        "description": "The key to retrieve result score from huggingface evaluate result."
      },
      {
        "name": "generation_gt",
        "param_type": "typing.List[typing.List[str]]",
        "description": "A list of ground truth.\n    Must be 2-d list of string."
      },
      {
        "name": "generations",
        "param_type": "typing.List[str]",
        "description": "A list of generations that LLM generated."
      },
      {
        "name": "kwargs",
        "param_type": "Any",
        "description": "The additional arguments for metric function."
      }
    ],
    "return_type": "typing.List[float]",
    "docstring": "Compute huggingface evaluate metric.\n\n:param instance: The instance of huggingface evaluates metric.\n:param key: The key to retrieve result score from huggingface evaluate result.\n:param generation_gt: A list of ground truth.\n    Must be 2-d list of string.\n:param generations: A list of generations that LLM generated.\n:param kwargs: The additional arguments for metric function.\n:return: The list of scores."
  },
  {
    "function": "meteor",
    "module": "autorag.evaluation.metric.generation",
    "params": [
      {
        "name": "generation_gt",
        "param_type": "typing.List[typing.List[str]]",
        "description": "A list of ground truth.\n        Must be 2-d list of string.\n        Because it can be a multiple ground truth."
      },
      {
        "name": "generations",
        "param_type": "typing.List[str]",
        "description": "A list of generations that LLM generated."
      },
      {
        "name": "alpha",
        "param_type": "<class 'float'>",
        "description": "Parameter for controlling relative weights of precision and recall.\n    Default is 0.9."
      },
      {
        "name": "beta",
        "param_type": "<class 'float'>",
        "description": "Parameter for controlling shape of penalty as a\n    function of as a function of fragmentation.\n    Default is 3.0."
      },
      {
        "name": "gamma",
        "param_type": "<class 'float'>",
        "description": "Relative weight assigned to fragmentation penalty.\n    Default is 0.5."
      }
    ],
    "return_type": "typing.List[float]",
    "docstring": "Compute meteor score for generation.\n\n:param generation_gt: A list of ground truth.\n        Must be 2-d list of string.\n        Because it can be a multiple ground truth.\n:param generations: A list of generations that LLM generated.\n:param alpha: Parameter for controlling relative weights of precision and recall.\n    Default is 0.9.\n:param beta: Parameter for controlling shape of penalty as a\n    function of as a function of fragmentation.\n    Default is 3.0.\n:param gamma: Relative weight assigned to fragmentation penalty.\n    Default is 0.5.\n:return: A list of computed metric scores."
  },
  {
    "function": "openai_truncate_by_token",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "texts",
        "param_type": "typing.List[str]",
        "description": ""
      },
      {
        "name": "token_limit",
        "param_type": "<class 'int'>",
        "description": ""
      },
      {
        "name": "model_name",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "typing.List[str]",
    "docstring": null
  },
  {
    "function": "process_batch",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "tasks",
        "param_type": "Any",
        "description": "A list of no-argument functions or coroutines to be executed."
      },
      {
        "name": "batch_size",
        "param_type": "<class 'int'>",
        "description": "The number of tasks to process in a single batch.\n    Default is 64."
      }
    ],
    "return_type": "typing.List[typing.Any]",
    "docstring": "Processes tasks in batches asynchronously.\n\n:param tasks: A list of no-argument functions or coroutines to be executed.\n:param batch_size: The number of tasks to process in a single batch.\n    Default is 64.\n:return: A list of results from the processed tasks."
  },
  {
    "function": "rouge",
    "module": "autorag.evaluation.metric.generation",
    "params": [
      {
        "name": "generation_gt",
        "param_type": "typing.List[typing.List[str]]",
        "description": "A list of ground truth.\n            Must be 2-d list of string.\n            Because it can be a multiple ground truth."
      },
      {
        "name": "generations",
        "param_type": "typing.List[str]",
        "description": "A list of generations that LLM generated."
      },
      {
        "name": "rouge_type",
        "param_type": "typing.Optional[str]",
        "description": "A rouge type to use for evaluation.\n        Default is 'RougeL'.\n        Choose between rouge1, rouge2, rougeL, and rougeLSum.\n        - rouge1: unigram (1-gram) based scoring.\n        - rouge2: bigram (2-gram) based scoring.\n        - rougeL: Longest Common Subsequence based scoring.\n        - rougeLSum: splits text using \"\n\""
      },
      {
        "name": "use_stemmer",
        "param_type": "<class 'bool'>",
        "description": "Bool indicating whether Porter stemmer should be used to\n        strip word suffixes to improve matching. This arg is used in the\n        DefaultTokenizer, but other tokenizers might or might not choose to\n        use this. Default is False."
      },
      {
        "name": "split_summaries",
        "param_type": "<class 'bool'>",
        "description": "Whether to add newlines between sentences for rougeLsum.\n        Default is False."
      },
      {
        "name": "batch",
        "param_type": "<class 'int'>",
        "description": "The batch size for processing.\n        Default is your cpu count."
      }
    ],
    "return_type": "typing.List[float]",
    "docstring": "    Compute rouge score for generation.\n\n    :param generation_gt: A list of ground truth.\n            Must be 2-d list of string.\n            Because it can be a multiple ground truth.\n    :param generations: A list of generations that LLM generated.\n    :param rouge_type: A rouge type to use for evaluation.\n        Default is 'RougeL'.\n        Choose between rouge1, rouge2, rougeL, and rougeLSum.\n        - rouge1: unigram (1-gram) based scoring.\n        - rouge2: bigram (2-gram) based scoring.\n        - rougeL: Longest Common Subsequence based scoring.\n        - rougeLSum: splits text using \"\n\"\n    :param use_stemmer: Bool indicating whether Porter stemmer should be used to\n        strip word suffixes to improve matching. This arg is used in the\n        DefaultTokenizer, but other tokenizers might or might not choose to\n        use this. Default is False.\n    :param split_summaries: Whether to add newlines between sentences for rougeLsum.\n        Default is False.\n    :param batch: The batch size for processing.\n        Default is your cpu count.\n    :return: A list of computed metric scores.\n    "
  },
  {
    "function": "sem_score",
    "module": "autorag.evaluation.metric.generation",
    "params": [
      {
        "name": "generation_gt",
        "param_type": "typing.List[typing.List[str]]",
        "description": "A list of ground truth.\n        Must be 2-d list of string.\n        Because it can be a multiple ground truth.\n        It will get the max of cosine similarity between generation_gt and pred."
      },
      {
        "name": "generations",
        "param_type": "typing.List[str]",
        "description": "A list of generations that LLM generated."
      },
      {
        "name": "embedding_model",
        "param_type": "typing.Optional[llama_index.core.base.embeddings.base.BaseEmbedding]",
        "description": "Embedding model to use for compute cosine similarity.\n    Default is all-mpnet-base-v2 embedding model.\n    The paper used this embedding model."
      },
      {
        "name": "batch",
        "param_type": "<class 'int'>",
        "description": "The batch size for processing.\n    Default is 128"
      }
    ],
    "return_type": "typing.List[float]",
    "docstring": "Compute sem score between generation gt and pred with cosine similarity.\n\n:param generation_gt: A list of ground truth.\n        Must be 2-d list of string.\n        Because it can be a multiple ground truth.\n        It will get the max of cosine similarity between generation_gt and pred.\n:param generations: A list of generations that LLM generated.\n:param embedding_model: Embedding model to use for compute cosine similarity.\n    Default is all-mpnet-base-v2 embedding model.\n    The paper used this embedding model.\n:param batch: The batch size for processing.\n    Default is 128\n:return: A list of computed metric scores."
  },
  {
    "function": "convert_inputs_to_list",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "func",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Decorator to convert all function inputs to Python lists."
  },
  {
    "function": "retrieval_f1",
    "module": "autorag.evaluation.metric.retrieval",
    "params": [
      {
        "name": "gt",
        "param_type": "typing.List[typing.List[str]]",
        "description": "2-d list of ground truth ids.\n    It contains and/or connections between ids."
      },
      {
        "name": "pred",
        "param_type": "typing.List[str]",
        "description": "Prediction ids."
      }
    ],
    "return_type": "Any",
    "docstring": "Compute f1 score for retrieval.\n\n:param gt: 2-d list of ground truth ids.\n    It contains and/or connections between ids.\n:param pred: Prediction ids.\n:return: The f1 score."
  },
  {
    "function": "retrieval_map",
    "module": "autorag.evaluation.metric.retrieval",
    "params": [
      {
        "name": "gt",
        "param_type": "typing.List[typing.List[str]]",
        "description": ""
      },
      {
        "name": "pred",
        "param_type": "typing.List[str]",
        "description": ""
      }
    ],
    "return_type": "<class 'float'>",
    "docstring": "Mean Average Precision (MAP) is the mean of Average Precision (AP) for all queries."
  },
  {
    "function": "retrieval_metric",
    "module": "autorag.evaluation.metric.retrieval",
    "params": [
      {
        "name": "func",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "retrieval_mrr",
    "module": "autorag.evaluation.metric.retrieval",
    "params": [
      {
        "name": "gt",
        "param_type": "typing.List[typing.List[str]]",
        "description": ""
      },
      {
        "name": "pred",
        "param_type": "typing.List[str]",
        "description": ""
      }
    ],
    "return_type": "<class 'float'>",
    "docstring": "Reciprocal Rank (RR) is the reciprocal of the rank of the first relevant item.\nMean of RR in whole queries is MRR."
  },
  {
    "function": "retrieval_ndcg",
    "module": "autorag.evaluation.metric.retrieval",
    "params": [
      {
        "name": "gt",
        "param_type": "typing.List[typing.List[str]]",
        "description": ""
      },
      {
        "name": "pred",
        "param_type": "typing.List[str]",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "retrieval_precision",
    "module": "autorag.evaluation.metric.retrieval",
    "params": [
      {
        "name": "gt",
        "param_type": "typing.List[typing.List[str]]",
        "description": ""
      },
      {
        "name": "pred",
        "param_type": "typing.List[str]",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "retrieval_recall",
    "module": "autorag.evaluation.metric.retrieval",
    "params": [
      {
        "name": "gt",
        "param_type": "typing.List[typing.List[str]]",
        "description": ""
      },
      {
        "name": "pred",
        "param_type": "typing.List[str]",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "convert_inputs_to_list",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "func",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Decorator to convert all function inputs to Python lists."
  },
  {
    "function": "normalize_string",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "s",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "<class 'str'>",
    "docstring": "Taken from the official evaluation script for v1.1 of the SQuAD dataset.\nLower text and remove punctuation, articles and extra whitespace."
  },
  {
    "function": "retrieval_contents_metric",
    "module": "autorag.evaluation.metric.retrieval_contents",
    "params": [
      {
        "name": "func",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "retrieval_token_f1",
    "module": "autorag.evaluation.metric.retrieval_contents",
    "params": [
      {
        "name": "gt",
        "param_type": "typing.List[str]",
        "description": ""
      },
      {
        "name": "pred",
        "param_type": "typing.List[str]",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "retrieval_token_precision",
    "module": "autorag.evaluation.metric.retrieval_contents",
    "params": [
      {
        "name": "gt",
        "param_type": "typing.List[str]",
        "description": ""
      },
      {
        "name": "pred",
        "param_type": "typing.List[str]",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "retrieval_token_recall",
    "module": "autorag.evaluation.metric.retrieval_contents",
    "params": [
      {
        "name": "gt",
        "param_type": "typing.List[str]",
        "description": ""
      },
      {
        "name": "pred",
        "param_type": "typing.List[str]",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "single_token_f1",
    "module": "autorag.evaluation.metric.retrieval_contents",
    "params": [
      {
        "name": "ground_truth",
        "param_type": "<class 'str'>",
        "description": ""
      },
      {
        "name": "prediction",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "calculate_cosine_similarity",
    "module": "autorag.evaluation.metric.util",
    "params": [
      {
        "name": "a",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "b",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "cast_metrics",
    "module": "autorag.evaluation.util",
    "params": [
      {
        "name": "metrics",
        "param_type": "typing.Union[typing.List[str], typing.List[typing.Dict]]",
        "description": "List of string or dictionary."
      }
    ],
    "return_type": "typing.Tuple[typing.List[str], typing.List[typing.Dict[str, typing.Any]]]",
    "docstring": " Turn metrics to list of metric names and parameter list.\n\n:param metrics: List of string or dictionary.\n:return: The list of metric names and dictionary list of metric parameters."
  },
  {
    "function": "evaluate_retrieval",
    "module": "autorag.evaluation.retrieval",
    "params": [
      {
        "name": "retrieval_gt",
        "param_type": "typing.List[typing.List[typing.List[str]]]",
        "description": ""
      },
      {
        "name": "metrics",
        "param_type": "typing.Union[typing.List[str], typing.List[typing.Dict]]",
        "description": ""
      },
      {
        "name": "queries",
        "param_type": "typing.Optional[typing.List[str]]",
        "description": ""
      },
      {
        "name": "generation_gt",
        "param_type": "typing.Optional[typing.List[typing.List[str]]]",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "retrieval_f1",
    "module": "autorag.evaluation.metric.retrieval",
    "params": [
      {
        "name": "gt",
        "param_type": "typing.List[typing.List[str]]",
        "description": "2-d list of ground truth ids.\n    It contains and/or connections between ids."
      },
      {
        "name": "pred",
        "param_type": "typing.List[str]",
        "description": "Prediction ids."
      }
    ],
    "return_type": "Any",
    "docstring": "Compute f1 score for retrieval.\n\n:param gt: 2-d list of ground truth ids.\n    It contains and/or connections between ids.\n:param pred: Prediction ids.\n:return: The f1 score."
  },
  {
    "function": "retrieval_map",
    "module": "autorag.evaluation.metric.retrieval",
    "params": [
      {
        "name": "gt",
        "param_type": "typing.List[typing.List[str]]",
        "description": ""
      },
      {
        "name": "pred",
        "param_type": "typing.List[str]",
        "description": ""
      }
    ],
    "return_type": "<class 'float'>",
    "docstring": "Mean Average Precision (MAP) is the mean of Average Precision (AP) for all queries."
  },
  {
    "function": "retrieval_mrr",
    "module": "autorag.evaluation.metric.retrieval",
    "params": [
      {
        "name": "gt",
        "param_type": "typing.List[typing.List[str]]",
        "description": ""
      },
      {
        "name": "pred",
        "param_type": "typing.List[str]",
        "description": ""
      }
    ],
    "return_type": "<class 'float'>",
    "docstring": "Reciprocal Rank (RR) is the reciprocal of the rank of the first relevant item.\nMean of RR in whole queries is MRR."
  },
  {
    "function": "retrieval_ndcg",
    "module": "autorag.evaluation.metric.retrieval",
    "params": [
      {
        "name": "gt",
        "param_type": "typing.List[typing.List[str]]",
        "description": ""
      },
      {
        "name": "pred",
        "param_type": "typing.List[str]",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "retrieval_precision",
    "module": "autorag.evaluation.metric.retrieval",
    "params": [
      {
        "name": "gt",
        "param_type": "typing.List[typing.List[str]]",
        "description": ""
      },
      {
        "name": "pred",
        "param_type": "typing.List[str]",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "retrieval_recall",
    "module": "autorag.evaluation.metric.retrieval",
    "params": [
      {
        "name": "gt",
        "param_type": "typing.List[typing.List[str]]",
        "description": ""
      },
      {
        "name": "pred",
        "param_type": "typing.List[str]",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "evaluate_retrieval_contents",
    "module": "autorag.evaluation.retrieval_contents",
    "params": [
      {
        "name": "retrieval_gt",
        "param_type": "typing.List[typing.List[str]]",
        "description": ""
      },
      {
        "name": "metrics",
        "param_type": "typing.List[str]",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "retrieval_token_f1",
    "module": "autorag.evaluation.metric.retrieval_contents",
    "params": [
      {
        "name": "gt",
        "param_type": "typing.List[str]",
        "description": ""
      },
      {
        "name": "pred",
        "param_type": "typing.List[str]",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "retrieval_token_precision",
    "module": "autorag.evaluation.metric.retrieval_contents",
    "params": [
      {
        "name": "gt",
        "param_type": "typing.List[str]",
        "description": ""
      },
      {
        "name": "pred",
        "param_type": "typing.List[str]",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "retrieval_token_recall",
    "module": "autorag.evaluation.metric.retrieval_contents",
    "params": [
      {
        "name": "gt",
        "param_type": "typing.List[str]",
        "description": ""
      },
      {
        "name": "pred",
        "param_type": "typing.List[str]",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "cast_embedding_model",
    "module": "autorag.evaluation.util",
    "params": [
      {
        "name": "key",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "value",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "cast_metrics",
    "module": "autorag.evaluation.util",
    "params": [
      {
        "name": "metrics",
        "param_type": "typing.Union[typing.List[str], typing.List[typing.Dict]]",
        "description": "List of string or dictionary."
      }
    ],
    "return_type": "typing.Tuple[typing.List[str], typing.List[typing.Dict[str, typing.Any]]]",
    "docstring": " Turn metrics to list of metric names and parameter list.\n\n:param metrics: List of string or dictionary.\n:return: The list of metric names and dictionary list of metric parameters."
  },
  {
    "function": "Evaluator.__embed",
    "module": "autorag.evaluator",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "node_lines",
        "param_type": "typing.Dict[str, typing.List[autorag.schema.node.Node]]",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "Evaluator.__find_conflict_point",
    "module": "autorag.evaluator",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "trial_path",
        "param_type": "<class 'str'>",
        "description": ""
      },
      {
        "name": "node_line_names",
        "param_type": "typing.List[str]",
        "description": ""
      },
      {
        "name": "node_lines",
        "param_type": "typing.Dict[str, typing.List[autorag.schema.node.Node]]",
        "description": ""
      }
    ],
    "return_type": "tuple[str, str]",
    "docstring": null
  },
  {
    "function": "Evaluator.__get_new_trial_name",
    "module": "autorag.evaluator",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "<class 'str'>",
    "docstring": null
  },
  {
    "function": "Evaluator.__make_trial_dir",
    "module": "autorag.evaluator",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "trial_name",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "Evaluator.__set_previous_result",
    "module": "autorag.evaluator",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "node_line_names",
        "param_type": "typing.List[str]",
        "description": ""
      },
      {
        "name": "node_names",
        "param_type": "typing.List[typing.List[str]]",
        "description": ""
      },
      {
        "name": "trial_path",
        "param_type": "<class 'str'>",
        "description": ""
      },
      {
        "name": "conflict_node_name",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "Evaluator.__init__",
    "module": "autorag.evaluator",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "qa_data_path",
        "param_type": "<class 'str'>",
        "description": "The path to the QA dataset.\n    Must be parquet file."
      },
      {
        "name": "corpus_data_path",
        "param_type": "<class 'str'>",
        "description": "The path to the corpus dataset.\n    Must be parquet file."
      },
      {
        "name": "project_dir",
        "param_type": "typing.Optional[str]",
        "description": "The path to the project directory.\n    Default is the current directory."
      }
    ],
    "return_type": "Any",
    "docstring": "Initialize an Evaluator object.\n\n:param qa_data_path: The path to the QA dataset.\n    Must be parquet file.\n:param corpus_data_path: The path to the corpus dataset.\n    Must be parquet file.\n:param project_dir: The path to the project directory.\n    Default is the current directory."
  },
  {
    "function": "Evaluator._append_node_line_summary",
    "module": "autorag.evaluator",
    "params": [
      {
        "name": "node_line_name",
        "param_type": "<class 'str'>",
        "description": ""
      },
      {
        "name": "node_line_dir",
        "param_type": "<class 'str'>",
        "description": ""
      },
      {
        "name": "trial_summary_df",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "Evaluator._append_node_summary",
    "module": "autorag.evaluator",
    "params": [
      {
        "name": "node_line_dir",
        "param_type": "<class 'str'>",
        "description": ""
      },
      {
        "name": "node_name",
        "param_type": "<class 'str'>",
        "description": ""
      },
      {
        "name": "summary_lst",
        "param_type": "typing.List[typing.Dict]",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "Evaluator._find_conflict_node_name",
    "module": "autorag.evaluator",
    "params": [
      {
        "name": "node_line_dir",
        "param_type": "<class 'str'>",
        "description": ""
      },
      {
        "name": "node_line",
        "param_type": "typing.List[autorag.schema.node.Node]",
        "description": ""
      }
    ],
    "return_type": "<class 'str'>",
    "docstring": null
  },
  {
    "function": "Evaluator._load_node_lines",
    "module": "autorag.evaluator",
    "params": [
      {
        "name": "yaml_path",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "typing.Dict[str, typing.List[autorag.schema.node.Node]]",
    "docstring": null
  },
  {
    "function": "Evaluator._set_remain_nodes_and_lines",
    "module": "autorag.evaluator",
    "params": [
      {
        "name": "node_line_names",
        "param_type": "typing.List[str]",
        "description": ""
      },
      {
        "name": "nodes",
        "param_type": "typing.List[typing.List[autorag.schema.node.Node]]",
        "description": ""
      },
      {
        "name": "node_names",
        "param_type": "typing.List[typing.List[str]]",
        "description": ""
      },
      {
        "name": "conflict_node_name",
        "param_type": "<class 'str'>",
        "description": ""
      },
      {
        "name": "conflict_node_line_name",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "Evaluator.restart_trial",
    "module": "autorag.evaluator",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "trial_path",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "Evaluator.start_trial",
    "module": "autorag.evaluator",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "yaml_path",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "Node.__eq__",
    "module": "autorag.schema.node",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "other",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Return self==value."
  },
  {
    "function": "Node.__init__",
    "module": "autorag.schema.node",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "node_type",
        "param_type": "<class 'str'>",
        "description": ""
      },
      {
        "name": "strategy",
        "param_type": "typing.Dict",
        "description": ""
      },
      {
        "name": "node_params",
        "param_type": "typing.Dict",
        "description": ""
      },
      {
        "name": "modules",
        "param_type": "typing.List[autorag.schema.module.Module]",
        "description": ""
      }
    ],
    "return_type": "None",
    "docstring": "Initialize self.  See help(type(self)) for accurate signature."
  },
  {
    "function": "Node.__post_init__",
    "module": "autorag.schema.node",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "Node.__repr__",
    "module": "autorag.schema.node",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Return repr(self)."
  },
  {
    "function": "Node.from_dict",
    "module": "autorag.schema.node",
    "params": [
      {
        "name": "node_dict",
        "param_type": "typing.Dict",
        "description": ""
      }
    ],
    "return_type": "Node",
    "docstring": null
  },
  {
    "function": "Node.get_param_combinations",
    "module": "autorag.schema.node",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "typing.Tuple[typing.List[typing.Callable], typing.List[typing.Dict]]",
    "docstring": "This method returns a combination of module and node parameters, also corresponding modules.\n\n:return: Each module and its module parameters.\n:rtype: Tuple[List[Callable], List[Dict]]"
  },
  {
    "function": "Node.run",
    "module": "autorag.schema.node",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "previous_result",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": ""
      },
      {
        "name": "node_line_dir",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "<class 'pandas.core.frame.DataFrame'>",
    "docstring": null
  },
  {
    "function": "bm25_ingest",
    "module": "autorag.nodes.retrieval.bm25",
    "params": [
      {
        "name": "corpus_path",
        "param_type": "<class 'str'>",
        "description": ""
      },
      {
        "name": "corpus_data",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": ""
      },
      {
        "name": "bm25_tokenizer",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "cast_corpus_dataset",
    "module": "autorag.utils.preprocess",
    "params": [
      {
        "name": "df",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "cast_qa_dataset",
    "module": "autorag.utils.preprocess",
    "params": [
      {
        "name": "df",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "convert_env_in_dict",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "d",
        "param_type": "typing.Dict",
        "description": "The dictionary to convert."
      }
    ],
    "return_type": "Any",
    "docstring": "Recursively converts environment variable string in a dictionary to actual environment variable.\n\n:param d: The dictionary to convert.\n:return: The converted dictionary."
  },
  {
    "function": "convert_string_to_tuple_in_dict",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "d",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Recursively converts strings that start with '(' and end with ')' to tuples in a dictionary."
  },
  {
    "function": "explode",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "index_values",
        "param_type": "typing.Collection[typing.Any]",
        "description": "The index values."
      },
      {
        "name": "explode_values",
        "param_type": "typing.Collection[typing.Collection[typing.Any]]",
        "description": "The exploded values."
      }
    ],
    "return_type": "Any",
    "docstring": "Explode index_values and explode_values.\nThe index_values and explode_values must have the same length.\nIt will flatten explode_values and keep index_values as a pair.\n\n:param index_values: The index values.\n:param explode_values: The exploded values.\n:return: Tuple of exploded index_values and exploded explode_values."
  },
  {
    "function": "extract_values_from_nodes",
    "module": "autorag.schema.node",
    "params": [
      {
        "name": "nodes",
        "param_type": "typing.List[autorag.schema.node.Node]",
        "description": "The nodes you want to extract values from."
      },
      {
        "name": "key",
        "param_type": "<class 'str'>",
        "description": "The key of module_param that you want to extract."
      }
    ],
    "return_type": "typing.List[str]",
    "docstring": "This function extract values from nodes' modules' module_param.\n\n:param nodes: The nodes you want to extract values from.\n:param key: The key of module_param that you want to extract.\n:return: The list of extracted values.\n    It removes duplicated elements automatically."
  },
  {
    "function": "get_bm25_pkl_name",
    "module": "autorag.nodes.retrieval.base",
    "params": [
      {
        "name": "bm25_tokenizer",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "load_summary_file",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "summary_path",
        "param_type": "<class 'str'>",
        "description": "The path of the summary file."
      },
      {
        "name": "dict_columns",
        "param_type": "typing.Optional[typing.List[str]]",
        "description": "The columns that are dictionary type.\n    You must fill this parameter if you want to load summary file properly.\n    Default is ['module_params']."
      }
    ],
    "return_type": "<class 'pandas.core.frame.DataFrame'>",
    "docstring": "Load summary file from summary_path.\n\n:param summary_path: The path of the summary file.\n:param dict_columns: The columns that are dictionary type.\n    You must fill this parameter if you want to load summary file properly.\n    Default is ['module_params'].\n:return: The summary dataframe."
  },
  {
    "function": "module_type_exists",
    "module": "autorag.schema.node",
    "params": [
      {
        "name": "nodes",
        "param_type": "typing.List[autorag.schema.node.Node]",
        "description": "The nodes you want to check."
      },
      {
        "name": "module_type",
        "param_type": "<class 'str'>",
        "description": "The module type you want to check."
      }
    ],
    "return_type": "<class 'bool'>",
    "docstring": "This function check if the module type exists in the nodes.\n\n:param nodes: The nodes you want to check.\n:param module_type: The module type you want to check.\n:return: True if the module type exists in the nodes."
  },
  {
    "function": "run_node_line",
    "module": "autorag.node_line",
    "params": [
      {
        "name": "nodes",
        "param_type": "typing.List[autorag.schema.node.Node]",
        "description": "A list of nodes."
      },
      {
        "name": "node_line_dir",
        "param_type": "<class 'str'>",
        "description": "This node line's directory."
      },
      {
        "name": "previous_result",
        "param_type": "typing.Optional[pandas.core.frame.DataFrame]",
        "description": "A result of the previous node line.\n    If None, it loads qa data from data/qa.parquet."
      }
    ],
    "return_type": "Any",
    "docstring": "Run the whole node line by running each node.\n\n:param nodes: A list of nodes.\n:param node_line_dir: This node line's directory.\n:param previous_result: A result of the previous node line.\n    If None, it loads qa data from data/qa.parquet.\n:return: The final result of the node line."
  },
  {
    "function": "validate_qa_from_corpus_dataset",
    "module": "autorag.utils.preprocess",
    "params": [
      {
        "name": "qa_df",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": ""
      },
      {
        "name": "corpus_df",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "vectordb_ingest",
    "module": "autorag.nodes.retrieval.vectordb",
    "params": [
      {
        "name": "collection",
        "param_type": "<class 'chromadb.api.models.Collection.Collection'>",
        "description": ""
      },
      {
        "name": "corpus_data",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": ""
      },
      {
        "name": "embedding_model",
        "param_type": "<class 'llama_index.core.base.embeddings.base.BaseEmbedding'>",
        "description": ""
      },
      {
        "name": "embedding_batch",
        "param_type": "<class 'int'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "Node.__eq__",
    "module": "autorag.schema.node",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "other",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Return self==value."
  },
  {
    "function": "Node.__init__",
    "module": "autorag.schema.node",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "node_type",
        "param_type": "<class 'str'>",
        "description": ""
      },
      {
        "name": "strategy",
        "param_type": "typing.Dict",
        "description": ""
      },
      {
        "name": "node_params",
        "param_type": "typing.Dict",
        "description": ""
      },
      {
        "name": "modules",
        "param_type": "typing.List[autorag.schema.module.Module]",
        "description": ""
      }
    ],
    "return_type": "None",
    "docstring": "Initialize self.  See help(type(self)) for accurate signature."
  },
  {
    "function": "Node.__post_init__",
    "module": "autorag.schema.node",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "Node.__repr__",
    "module": "autorag.schema.node",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Return repr(self)."
  },
  {
    "function": "Node.from_dict",
    "module": "autorag.schema.node",
    "params": [
      {
        "name": "node_dict",
        "param_type": "typing.Dict",
        "description": ""
      }
    ],
    "return_type": "Node",
    "docstring": null
  },
  {
    "function": "Node.get_param_combinations",
    "module": "autorag.schema.node",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "typing.Tuple[typing.List[typing.Callable], typing.List[typing.Dict]]",
    "docstring": "This method returns a combination of module and node parameters, also corresponding modules.\n\n:return: Each module and its module parameters.\n:rtype: Tuple[List[Callable], List[Dict]]"
  },
  {
    "function": "Node.run",
    "module": "autorag.schema.node",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "previous_result",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": ""
      },
      {
        "name": "node_line_dir",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "<class 'pandas.core.frame.DataFrame'>",
    "docstring": null
  },
  {
    "function": "load_summary_file",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "summary_path",
        "param_type": "<class 'str'>",
        "description": "The path of the summary file."
      },
      {
        "name": "dict_columns",
        "param_type": "typing.Optional[typing.List[str]]",
        "description": "The columns that are dictionary type.\n    You must fill this parameter if you want to load summary file properly.\n    Default is ['module_params']."
      }
    ],
    "return_type": "<class 'pandas.core.frame.DataFrame'>",
    "docstring": "Load summary file from summary_path.\n\n:param summary_path: The path of the summary file.\n:param dict_columns: The columns that are dictionary type.\n    You must fill this parameter if you want to load summary file properly.\n    Default is ['module_params'].\n:return: The summary dataframe."
  },
  {
    "function": "make_node_lines",
    "module": "autorag.node_line",
    "params": [
      {
        "name": "node_line_dict",
        "param_type": "typing.Dict",
        "description": "Node_line_dict loaded from yaml file, or get from user input."
      }
    ],
    "return_type": "typing.List[autorag.schema.node.Node]",
    "docstring": "This method makes a list of nodes from node line dictionary.\n:param node_line_dict: Node_line_dict loaded from yaml file, or get from user input.\n:return: List of Nodes inside this node line."
  },
  {
    "function": "run_node_line",
    "module": "autorag.node_line",
    "params": [
      {
        "name": "nodes",
        "param_type": "typing.List[autorag.schema.node.Node]",
        "description": "A list of nodes."
      },
      {
        "name": "node_line_dir",
        "param_type": "<class 'str'>",
        "description": "This node line's directory."
      },
      {
        "name": "previous_result",
        "param_type": "typing.Optional[pandas.core.frame.DataFrame]",
        "description": "A result of the previous node line.\n    If None, it loads qa data from data/qa.parquet."
      }
    ],
    "return_type": "Any",
    "docstring": "Run the whole node line by running each node.\n\n:param nodes: A list of nodes.\n:param node_line_dir: This node line's directory.\n:param previous_result: A result of the previous node line.\n    If None, it loads qa data from data/qa.parquet.\n:return: The final result of the node line."
  },
  {
    "function": "llama_index_llm",
    "module": "autorag.nodes.generator.llama_index_llm",
    "params": [
      {
        "name": "prompts",
        "param_type": "typing.List[str]",
        "description": "A list of prompts."
      },
      {
        "name": "llm",
        "param_type": "typing.Union[llama_index.core.service_context_elements.llm_predictor.LLMPredictor, llama_index.core.llms.llm.LLM]",
        "description": "A llama index LLM instance."
      },
      {
        "name": "batch",
        "param_type": "<class 'int'>",
        "description": "The batch size for llm.\n    Set low if you face some errors."
      }
    ],
    "return_type": "typing.Tuple[typing.List[str], typing.List[typing.List[int]], typing.List[typing.List[float]]]",
    "docstring": "Llama Index LLM module.\nIt gets the LLM instance from llama index, and returns generated text by the input prompt.\nIt does not generate the right log probs, but it returns the pseudo log probs,\nwhich is not meant to be used for other modules.\n\n:param prompts: A list of prompts.\n:param llm: A llama index LLM instance.\n:param batch: The batch size for llm.\n    Set low if you face some errors.\n:return: A tuple of three elements.\n    The first element is a list of generated text.\n    The second element is a list of generated text's token ids, used tokenizer is GPT2Tokenizer.\n    The third element is a list of generated text's pseudo log probs."
  },
  {
    "function": "openai_llm",
    "module": "autorag.nodes.generator.openai_llm",
    "params": [
      {
        "name": "prompts",
        "param_type": "typing.List[str]",
        "description": "A list of prompts."
      },
      {
        "name": "llm",
        "param_type": "<class 'str'>",
        "description": "A model name for openai.\n    Default is gpt-3.5-turbo."
      },
      {
        "name": "batch",
        "param_type": "<class 'int'>",
        "description": "Batch size for openai api call.\n    If you get API limit errors, you should lower the batch size.\n    Default is 16."
      },
      {
        "name": "truncate",
        "param_type": "<class 'bool'>",
        "description": "Whether to truncate the input prompt.\n    Default is True."
      },
      {
        "name": "api_key",
        "param_type": "<class 'str'>",
        "description": "OpenAI API key. You can set this by passing env variable `OPENAI_API_KEY`"
      },
      {
        "name": "kwargs",
        "param_type": "Any",
        "description": "The optional parameter for openai api call `openai.chat.completion`\n    See https://platform.openai.com/docs/api-reference/chat/create for more details."
      }
    ],
    "return_type": "typing.Tuple[typing.List[str], typing.List[typing.List[int]], typing.List[typing.List[float]]]",
    "docstring": "OpenAI generator module.\nUses official openai library for generating answer from the given prompt.\nIt returns real token ids and log probs, so you must use this for using token ids and log probs.\n\n:param prompts: A list of prompts.\n:param llm: A model name for openai.\n    Default is gpt-3.5-turbo.\n:param batch: Batch size for openai api call.\n    If you get API limit errors, you should lower the batch size.\n    Default is 16.\n:param truncate: Whether to truncate the input prompt.\n    Default is True.\n:param api_key: OpenAI API key. You can set this by passing env variable `OPENAI_API_KEY`\n:param kwargs: The optional parameter for openai api call `openai.chat.completion`\n    See https://platform.openai.com/docs/api-reference/chat/create for more details.\n:return: A tuple of three elements.\n    The first element is a list of generated text.\n    The second element is a list of generated text's token ids.\n    The third element is a list of generated text's log probs."
  },
  {
    "function": "vllm",
    "module": "autorag.nodes.generator.vllm",
    "params": [
      {
        "name": "prompts",
        "param_type": "typing.List[str]",
        "description": "A list of prompts."
      },
      {
        "name": "llm",
        "param_type": "<class 'str'>",
        "description": "Model name of vLLM."
      },
      {
        "name": "kwargs",
        "param_type": "Any",
        "description": "The extra parameters for generating the text."
      }
    ],
    "return_type": "typing.Tuple[typing.List[str], typing.List[typing.List[int]], typing.List[typing.List[float]]]",
    "docstring": "Vllm module.\nIt gets the VLLM instance, and returns generated texts by the input prompt.\nYou can set logprobs to get the log probs of the generated text.\nDefault logprobs is 1.\n\n:param prompts: A list of prompts.\n:param llm: Model name of vLLM.\n:param kwargs: The extra parameters for generating the text.\n:return: A tuple of three elements.\n    The first element is a list of generated text.\n    The second element is a list of generated text's token ids.\n    The third element is a list of generated text's log probs."
  },
  {
    "function": "generator_node",
    "module": "autorag.nodes.generator.base",
    "params": [
      {
        "name": "func",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "result_to_dataframe",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "column_names",
        "param_type": "typing.List[str]",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Decorator for converting results to pd.DataFrame."
  },
  {
    "function": "generator_node",
    "module": "autorag.nodes.generator.base",
    "params": [
      {
        "name": "func",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "llama_index_llm",
    "module": "autorag.nodes.generator.llama_index_llm",
    "params": [
      {
        "name": "prompts",
        "param_type": "typing.List[str]",
        "description": "A list of prompts."
      },
      {
        "name": "llm",
        "param_type": "typing.Union[llama_index.core.service_context_elements.llm_predictor.LLMPredictor, llama_index.core.llms.llm.LLM]",
        "description": "A llama index LLM instance."
      },
      {
        "name": "batch",
        "param_type": "<class 'int'>",
        "description": "The batch size for llm.\n    Set low if you face some errors."
      }
    ],
    "return_type": "typing.Tuple[typing.List[str], typing.List[typing.List[int]], typing.List[typing.List[float]]]",
    "docstring": "Llama Index LLM module.\nIt gets the LLM instance from llama index, and returns generated text by the input prompt.\nIt does not generate the right log probs, but it returns the pseudo log probs,\nwhich is not meant to be used for other modules.\n\n:param prompts: A list of prompts.\n:param llm: A llama index LLM instance.\n:param batch: The batch size for llm.\n    Set low if you face some errors.\n:return: A tuple of three elements.\n    The first element is a list of generated text.\n    The second element is a list of generated text's token ids, used tokenizer is GPT2Tokenizer.\n    The third element is a list of generated text's pseudo log probs."
  },
  {
    "function": "process_batch",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "tasks",
        "param_type": "Any",
        "description": "A list of no-argument functions or coroutines to be executed."
      },
      {
        "name": "batch_size",
        "param_type": "<class 'int'>",
        "description": "The number of tasks to process in a single batch.\n    Default is 64."
      }
    ],
    "return_type": "typing.List[typing.Any]",
    "docstring": "Processes tasks in batches asynchronously.\n\n:param tasks: A list of no-argument functions or coroutines to be executed.\n:param batch_size: The number of tasks to process in a single batch.\n    Default is 64.\n:return: A list of results from the processed tasks."
  },
  {
    "function": "generator_node",
    "module": "autorag.nodes.generator.base",
    "params": [
      {
        "name": "func",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "get_result",
    "module": "autorag.nodes.generator.openai_llm",
    "params": [
      {
        "name": "prompt",
        "param_type": "<class 'str'>",
        "description": ""
      },
      {
        "name": "client",
        "param_type": "<class 'openai.AsyncOpenAI'>",
        "description": ""
      },
      {
        "name": "model",
        "param_type": "<class 'str'>",
        "description": ""
      },
      {
        "name": "tokenizer",
        "param_type": "<class 'tiktoken.core.Encoding'>",
        "description": ""
      },
      {
        "name": "kwargs",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "openai_llm",
    "module": "autorag.nodes.generator.openai_llm",
    "params": [
      {
        "name": "prompts",
        "param_type": "typing.List[str]",
        "description": "A list of prompts."
      },
      {
        "name": "llm",
        "param_type": "<class 'str'>",
        "description": "A model name for openai.\n    Default is gpt-3.5-turbo."
      },
      {
        "name": "batch",
        "param_type": "<class 'int'>",
        "description": "Batch size for openai api call.\n    If you get API limit errors, you should lower the batch size.\n    Default is 16."
      },
      {
        "name": "truncate",
        "param_type": "<class 'bool'>",
        "description": "Whether to truncate the input prompt.\n    Default is True."
      },
      {
        "name": "api_key",
        "param_type": "<class 'str'>",
        "description": "OpenAI API key. You can set this by passing env variable `OPENAI_API_KEY`"
      },
      {
        "name": "kwargs",
        "param_type": "Any",
        "description": "The optional parameter for openai api call `openai.chat.completion`\n    See https://platform.openai.com/docs/api-reference/chat/create for more details."
      }
    ],
    "return_type": "typing.Tuple[typing.List[str], typing.List[typing.List[int]], typing.List[typing.List[float]]]",
    "docstring": "OpenAI generator module.\nUses official openai library for generating answer from the given prompt.\nIt returns real token ids and log probs, so you must use this for using token ids and log probs.\n\n:param prompts: A list of prompts.\n:param llm: A model name for openai.\n    Default is gpt-3.5-turbo.\n:param batch: Batch size for openai api call.\n    If you get API limit errors, you should lower the batch size.\n    Default is 16.\n:param truncate: Whether to truncate the input prompt.\n    Default is True.\n:param api_key: OpenAI API key. You can set this by passing env variable `OPENAI_API_KEY`\n:param kwargs: The optional parameter for openai api call `openai.chat.completion`\n    See https://platform.openai.com/docs/api-reference/chat/create for more details.\n:return: A tuple of three elements.\n    The first element is a list of generated text.\n    The second element is a list of generated text's token ids.\n    The third element is a list of generated text's log probs."
  },
  {
    "function": "process_batch",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "tasks",
        "param_type": "Any",
        "description": "A list of no-argument functions or coroutines to be executed."
      },
      {
        "name": "batch_size",
        "param_type": "<class 'int'>",
        "description": "The number of tasks to process in a single batch.\n    Default is 64."
      }
    ],
    "return_type": "typing.List[typing.Any]",
    "docstring": "Processes tasks in batches asynchronously.\n\n:param tasks: A list of no-argument functions or coroutines to be executed.\n:param batch_size: The number of tasks to process in a single batch.\n    Default is 64.\n:return: A list of results from the processed tasks."
  },
  {
    "function": "truncate_by_token",
    "module": "autorag.nodes.generator.openai_llm",
    "params": [
      {
        "name": "prompt",
        "param_type": "<class 'str'>",
        "description": ""
      },
      {
        "name": "tokenizer",
        "param_type": "<class 'tiktoken.core.Encoding'>",
        "description": ""
      },
      {
        "name": "max_token_size",
        "param_type": "<class 'int'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "cast_metrics",
    "module": "autorag.evaluation.util",
    "params": [
      {
        "name": "metrics",
        "param_type": "typing.Union[typing.List[str], typing.List[typing.Dict]]",
        "description": "List of string or dictionary."
      }
    ],
    "return_type": "typing.Tuple[typing.List[str], typing.List[typing.Dict[str, typing.Any]]]",
    "docstring": " Turn metrics to list of metric names and parameter list.\n\n:param metrics: List of string or dictionary.\n:return: The list of metric names and dictionary list of metric parameters."
  },
  {
    "function": "evaluate_generation",
    "module": "autorag.evaluation.generation",
    "params": [
      {
        "name": "generation_gt",
        "param_type": "typing.List[typing.List[str]]",
        "description": ""
      },
      {
        "name": "metrics",
        "param_type": "typing.Union[typing.List[str], typing.List[typing.Dict]]",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "evaluate_generator_node",
    "module": "autorag.nodes.generator.run",
    "params": [
      {
        "name": "result_df",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": ""
      },
      {
        "name": "generation_gt",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "metrics",
        "param_type": "typing.Union[typing.List[str], typing.List[typing.Dict]]",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "filter_by_threshold",
    "module": "autorag.strategy",
    "params": [
      {
        "name": "results",
        "param_type": "Any",
        "description": "The result list to be filtered."
      },
      {
        "name": "value",
        "param_type": "Any",
        "description": "The value list to be filtered.\n    It must have the same length with results."
      },
      {
        "name": "threshold",
        "param_type": "Any",
        "description": "The threshold value."
      },
      {
        "name": "metadatas",
        "param_type": "Any",
        "description": "The metadata of each result."
      }
    ],
    "return_type": "typing.Tuple[typing.List, typing.List]",
    "docstring": "Filter results by value's threshold.\n\n:param results: The result list to be filtered.\n:param value: The value list to be filtered.\n    It must have the same length with results.\n:param threshold: The threshold value.\n:param metadatas: The metadata of each result.\n:return: Filtered list of results and filtered list of metadatas.\n    Metadatas will be returned even if you did not give input metadatas.\n:rtype: Tuple[List, List]"
  },
  {
    "function": "measure_speed",
    "module": "autorag.strategy",
    "params": [
      {
        "name": "func",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "args",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "kwargs",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Method for measuring execution speed of the function."
  },
  {
    "function": "run_generator_node",
    "module": "autorag.nodes.generator.run",
    "params": [
      {
        "name": "modules",
        "param_type": "typing.List[typing.Callable]",
        "description": "Generator modules to run."
      },
      {
        "name": "module_params",
        "param_type": "typing.List[typing.Dict]",
        "description": "Generator module parameters.\n    Including node parameters, which is used for every module in this node."
      },
      {
        "name": "previous_result",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": "Previous result dataframe.\n    Could be prompt maker node's result."
      },
      {
        "name": "node_line_dir",
        "param_type": "<class 'str'>",
        "description": "This node line's directory."
      },
      {
        "name": "strategies",
        "param_type": "typing.Dict",
        "description": "Strategies for generator node."
      }
    ],
    "return_type": "<class 'pandas.core.frame.DataFrame'>",
    "docstring": "Run evaluation and select the best module among generator node results.\nAnd save the results and summary to generator node directory.\n\n:param modules: Generator modules to run.\n:param module_params: Generator module parameters.\n    Including node parameters, which is used for every module in this node.\n:param previous_result: Previous result dataframe.\n    Could be prompt maker node's result.\n:param node_line_dir: This node line's directory.\n:param strategies: Strategies for generator node.\n:return: The best result dataframe.\n    It contains previous result columns and generator node's result columns."
  },
  {
    "function": "select_best",
    "module": "autorag.strategy",
    "params": [
      {
        "name": "results",
        "param_type": "typing.List[pandas.core.frame.DataFrame]",
        "description": ""
      },
      {
        "name": "columns",
        "param_type": "typing.Iterable[str]",
        "description": ""
      },
      {
        "name": "metadatas",
        "param_type": "typing.Optional[typing.List[typing.Any]]",
        "description": ""
      },
      {
        "name": "strategy_name",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "typing.Tuple[pandas.core.frame.DataFrame, typing.Any]",
    "docstring": null
  },
  {
    "function": "destroy_vllm_instance",
    "module": "autorag.nodes.generator.vllm",
    "params": [
      {
        "name": "vllm_instance",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "generator_node",
    "module": "autorag.nodes.generator.base",
    "params": [
      {
        "name": "func",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "make_vllm_instance",
    "module": "autorag.nodes.generator.vllm",
    "params": [
      {
        "name": "llm",
        "param_type": "<class 'str'>",
        "description": ""
      },
      {
        "name": "input_args",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "vllm",
    "module": "autorag.nodes.generator.vllm",
    "params": [
      {
        "name": "prompts",
        "param_type": "typing.List[str]",
        "description": "A list of prompts."
      },
      {
        "name": "llm",
        "param_type": "<class 'str'>",
        "description": "Model name of vLLM."
      },
      {
        "name": "kwargs",
        "param_type": "Any",
        "description": "The extra parameters for generating the text."
      }
    ],
    "return_type": "typing.Tuple[typing.List[str], typing.List[typing.List[int]], typing.List[typing.List[float]]]",
    "docstring": "Vllm module.\nIt gets the VLLM instance, and returns generated texts by the input prompt.\nYou can set logprobs to get the log probs of the generated text.\nDefault logprobs is 1.\n\n:param prompts: A list of prompts.\n:param llm: Model name of vLLM.\n:param kwargs: The extra parameters for generating the text.\n:return: A tuple of three elements.\n    The first element is a list of generated text.\n    The second element is a list of generated text's token ids.\n    The third element is a list of generated text's log probs."
  },
  {
    "function": "pass_passage_augmenter",
    "module": "autorag.nodes.passageaugmenter.pass_passage_augmenter",
    "params": [
      {
        "name": "ids_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": ""
      },
      {
        "name": "contents_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": ""
      },
      {
        "name": "scores_list",
        "param_type": "typing.List[typing.List[float]]",
        "description": ""
      },
      {
        "name": "kwargs",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Do not perform augmentation.\nReturn given passages, scores, and ids as is."
  },
  {
    "function": "prev_next_augmenter",
    "module": "autorag.nodes.passageaugmenter.prev_next_augmenter",
    "params": [
      {
        "name": "ids_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of ids retrieved"
      },
      {
        "name": "corpus_df",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": "The corpus dataframe"
      },
      {
        "name": "num_passages",
        "param_type": "<class 'int'>",
        "description": "The number of passages to add before and after the retrieved passage\n    Default is 1."
      },
      {
        "name": "mode",
        "param_type": "<class 'str'>",
        "description": "The mode of augmentation\n    'prev': add passages before the retrieved passage\n    'next': add passages after the retrieved passage\n    'both': add passages before and after the retrieved passage\n    Default is 'next'."
      }
    ],
    "return_type": "typing.List[typing.List[str]]",
    "docstring": "Add passages before and/or after the retrieved passage.\nFor more information, visit https://docs.llamaindex.ai/en/stable/examples/node_postprocessor/PrevNextPostprocessorDemo/.\n\n:param ids_list: The list of lists of ids retrieved\n:param corpus_df: The corpus dataframe\n:param num_passages: The number of passages to add before and after the retrieved passage\n    Default is 1.\n:param mode: The mode of augmentation\n    'prev': add passages before the retrieved passage\n    'next': add passages after the retrieved passage\n    'both': add passages before and after the retrieved passage\n    Default is 'next'.\n:return: The list of lists of augmented ids"
  },
  {
    "function": "calculate_cosine_similarity",
    "module": "autorag.evaluation.metric.util",
    "params": [
      {
        "name": "a",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "b",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "cast_corpus_dataset",
    "module": "autorag.utils.preprocess",
    "params": [
      {
        "name": "df",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "embedding_query_content",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": ""
      },
      {
        "name": "contents_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": ""
      },
      {
        "name": "embedding_model",
        "param_type": "typing.Optional[str]",
        "description": ""
      },
      {
        "name": "batch",
        "param_type": "<class 'int'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "fetch_contents",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "corpus_data",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": ""
      },
      {
        "name": "ids",
        "param_type": "typing.List[typing.List[str]]",
        "description": ""
      },
      {
        "name": "column_name",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "typing.List[typing.List[typing.Any]]",
    "docstring": null
  },
  {
    "function": "filter_dict_keys",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "dict_",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "keys",
        "param_type": "typing.List[str]",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "passage_augmenter_node",
    "module": "autorag.nodes.passageaugmenter.base",
    "params": [
      {
        "name": "func",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "result_to_dataframe",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "column_names",
        "param_type": "typing.List[str]",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Decorator for converting results to pd.DataFrame."
  },
  {
    "function": "select_top_k",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "df",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "column_names",
        "param_type": "typing.List[str]",
        "description": ""
      },
      {
        "name": "top_k",
        "param_type": "<class 'int'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "sort_by_scores",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "row",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "reverse",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Sorts each row by 'scores' column.\nThe input column names must be 'contents', 'ids', and 'scores'.\nAnd its elements must be list type."
  },
  {
    "function": "validate_corpus_dataset",
    "module": "autorag.utils.preprocess",
    "params": [
      {
        "name": "df",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "validate_qa_dataset",
    "module": "autorag.utils.preprocess",
    "params": [
      {
        "name": "df",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "pass_passage_augmenter",
    "module": "autorag.nodes.passageaugmenter.pass_passage_augmenter",
    "params": [
      {
        "name": "ids_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": ""
      },
      {
        "name": "contents_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": ""
      },
      {
        "name": "scores_list",
        "param_type": "typing.List[typing.List[float]]",
        "description": ""
      },
      {
        "name": "kwargs",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Do not perform augmentation.\nReturn given passages, scores, and ids as is."
  },
  {
    "function": "passage_augmenter_node",
    "module": "autorag.nodes.passageaugmenter.base",
    "params": [
      {
        "name": "func",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "passage_augmenter_node",
    "module": "autorag.nodes.passageaugmenter.base",
    "params": [
      {
        "name": "func",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "prev_next_augmenter",
    "module": "autorag.nodes.passageaugmenter.prev_next_augmenter",
    "params": [
      {
        "name": "ids_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of ids retrieved"
      },
      {
        "name": "corpus_df",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": "The corpus dataframe"
      },
      {
        "name": "num_passages",
        "param_type": "<class 'int'>",
        "description": "The number of passages to add before and after the retrieved passage\n    Default is 1."
      },
      {
        "name": "mode",
        "param_type": "<class 'str'>",
        "description": "The mode of augmentation\n    'prev': add passages before the retrieved passage\n    'next': add passages after the retrieved passage\n    'both': add passages before and after the retrieved passage\n    Default is 'next'."
      }
    ],
    "return_type": "typing.List[typing.List[str]]",
    "docstring": "Add passages before and/or after the retrieved passage.\nFor more information, visit https://docs.llamaindex.ai/en/stable/examples/node_postprocessor/PrevNextPostprocessorDemo/.\n\n:param ids_list: The list of lists of ids retrieved\n:param corpus_df: The corpus dataframe\n:param num_passages: The number of passages to add before and after the retrieved passage\n    Default is 1.\n:param mode: The mode of augmentation\n    'prev': add passages before the retrieved passage\n    'next': add passages after the retrieved passage\n    'both': add passages before and after the retrieved passage\n    Default is 'next'.\n:return: The list of lists of augmented ids"
  },
  {
    "function": "prev_next_augmenter_pure",
    "module": "autorag.nodes.passageaugmenter.prev_next_augmenter",
    "params": [
      {
        "name": "ids",
        "param_type": "typing.List[str]",
        "description": ""
      },
      {
        "name": "corpus_df",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": ""
      },
      {
        "name": "mode",
        "param_type": "<class 'str'>",
        "description": ""
      },
      {
        "name": "num_passages",
        "param_type": "<class 'int'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "evaluate_retrieval_node",
    "module": "autorag.nodes.retrieval.run",
    "params": [
      {
        "name": "result_df",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": "The result dataframe from a retrieval node."
      },
      {
        "name": "retrieval_gt",
        "param_type": "Any",
        "description": "Ground truth for retrieval from qa dataset."
      },
      {
        "name": "metrics",
        "param_type": "Any",
        "description": "Metric list from input strategies."
      },
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": "Query list from input strategies."
      },
      {
        "name": "generation_gt",
        "param_type": "typing.List[typing.List[str]]",
        "description": "Ground truth for generation from qa dataset."
      }
    ],
    "return_type": "<class 'pandas.core.frame.DataFrame'>",
    "docstring": "Evaluate retrieval node from retrieval node result dataframe.\n\n:param result_df: The result dataframe from a retrieval node.\n:param retrieval_gt: Ground truth for retrieval from qa dataset.\n:param metrics: Metric list from input strategies.\n:param queries: Query list from input strategies.\n:param generation_gt: Ground truth for generation from qa dataset.\n:return: Return result_df with metrics columns.\n    The columns will be 'retrieved_contents', 'retrieved_ids', 'retrieve_scores', and metric names."
  },
  {
    "function": "filter_by_threshold",
    "module": "autorag.strategy",
    "params": [
      {
        "name": "results",
        "param_type": "Any",
        "description": "The result list to be filtered."
      },
      {
        "name": "value",
        "param_type": "Any",
        "description": "The value list to be filtered.\n    It must have the same length with results."
      },
      {
        "name": "threshold",
        "param_type": "Any",
        "description": "The threshold value."
      },
      {
        "name": "metadatas",
        "param_type": "Any",
        "description": "The metadata of each result."
      }
    ],
    "return_type": "typing.Tuple[typing.List, typing.List]",
    "docstring": "Filter results by value's threshold.\n\n:param results: The result list to be filtered.\n:param value: The value list to be filtered.\n    It must have the same length with results.\n:param threshold: The threshold value.\n:param metadatas: The metadata of each result.\n:return: Filtered list of results and filtered list of metadatas.\n    Metadatas will be returned even if you did not give input metadatas.\n:rtype: Tuple[List, List]"
  },
  {
    "function": "measure_speed",
    "module": "autorag.strategy",
    "params": [
      {
        "name": "func",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "args",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "kwargs",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Method for measuring execution speed of the function."
  },
  {
    "function": "run_passage_augmenter_node",
    "module": "autorag.nodes.passageaugmenter.run",
    "params": [
      {
        "name": "modules",
        "param_type": "typing.List[typing.Callable]",
        "description": ""
      },
      {
        "name": "module_params",
        "param_type": "typing.List[typing.Dict]",
        "description": ""
      },
      {
        "name": "previous_result",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": ""
      },
      {
        "name": "node_line_dir",
        "param_type": "<class 'str'>",
        "description": ""
      },
      {
        "name": "strategies",
        "param_type": "typing.Dict",
        "description": ""
      }
    ],
    "return_type": "<class 'pandas.core.frame.DataFrame'>",
    "docstring": null
  },
  {
    "function": "select_best",
    "module": "autorag.strategy",
    "params": [
      {
        "name": "results",
        "param_type": "typing.List[pandas.core.frame.DataFrame]",
        "description": ""
      },
      {
        "name": "columns",
        "param_type": "typing.Iterable[str]",
        "description": ""
      },
      {
        "name": "metadatas",
        "param_type": "typing.Optional[typing.List[typing.Any]]",
        "description": ""
      },
      {
        "name": "strategy_name",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "typing.Tuple[pandas.core.frame.DataFrame, typing.Any]",
    "docstring": null
  },
  {
    "function": "longllmlingua",
    "module": "autorag.nodes.passagecompressor.longllmlingua",
    "params": [
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": "The queries for retrieved passages."
      },
      {
        "name": "contents",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The contents of retrieved passages."
      },
      {
        "name": "scores",
        "param_type": "Any",
        "description": "The scores of retrieved passages.\n    Do not use in this function, so you can pass an empty list."
      },
      {
        "name": "ids",
        "param_type": "Any",
        "description": "The ids of retrieved passages.\n    Do not use in this function, so you can pass an empty list."
      },
      {
        "name": "model_name",
        "param_type": "<class 'str'>",
        "description": "The model name to use for compression.\n    Default is \"NousResearch/Llama-2-7b-hf\"."
      },
      {
        "name": "instructions",
        "param_type": "typing.Optional[str]",
        "description": "The instructions for compression.\n    Default is None. When it is None, it will use default instructions."
      },
      {
        "name": "target_token",
        "param_type": "<class 'int'>",
        "description": "The target token for compression.\n    Default is 300."
      },
      {
        "name": "kwargs",
        "param_type": "Any",
        "description": "Additional keyword arguments."
      }
    ],
    "return_type": "typing.List[str]",
    "docstring": "Compresses the retrieved texts using LongLLMLingua.\nFor more information, visit https://github.com/microsoft/LLMLingua.\n\n:param queries: The queries for retrieved passages.\n:param contents: The contents of retrieved passages.\n:param scores: The scores of retrieved passages.\n    Do not use in this function, so you can pass an empty list.\n:param ids: The ids of retrieved passages.\n    Do not use in this function, so you can pass an empty list.\n:param model_name: The model name to use for compression.\n    Default is \"NousResearch/Llama-2-7b-hf\".\n:param instructions: The instructions for compression.\n    Default is None. When it is None, it will use default instructions.\n:param target_token: The target token for compression.\n    Default is 300.\n:param kwargs: Additional keyword arguments.\n:return: The list of compressed texts."
  },
  {
    "function": "pass_compressor",
    "module": "autorag.nodes.passagecompressor.pass_compressor",
    "params": [
      {
        "name": "contents",
        "param_type": "typing.List[typing.List[str]]",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Do not perform any passage compression"
  },
  {
    "function": "refine",
    "module": "autorag.nodes.passagecompressor.refine",
    "params": [
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": "The queries for retrieved passages."
      },
      {
        "name": "contents",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The contents of retrieved passages."
      },
      {
        "name": "scores",
        "param_type": "Any",
        "description": "The scores of retrieved passages.\n    Do not use in this function, so you can pass an empty list."
      },
      {
        "name": "ids",
        "param_type": "Any",
        "description": "The ids of retrieved passages.\n    Do not use in this function, so you can pass an empty list."
      },
      {
        "name": "llm",
        "param_type": "typing.Union[llama_index.core.service_context_elements.llm_predictor.LLMPredictor, llama_index.core.llms.llm.LLM]",
        "description": "The llm instance that will be used to summarize."
      },
      {
        "name": "prompt",
        "param_type": "typing.Optional[str]",
        "description": "The prompt template for refine.\n    If you want to use chat prompt, you should pass chat_prompt instead.\n    At prompt, you must specify where to put 'context_msg' and 'query_str'.\n    Default is None. When it is None, it will use llama index default prompt."
      },
      {
        "name": "chat_prompt",
        "param_type": "typing.Optional[str]",
        "description": "The chat prompt template for refine.\n    If you want to use normal prompt, you should pass prompt instead.\n    At prompt, you must specify where to put 'context_msg' and 'query_str'.\n    Default is None. When it is None, it will use llama index default chat prompt."
      },
      {
        "name": "batch",
        "param_type": "<class 'int'>",
        "description": "The batch size for llm.\n    Set low if you face some errors.\n    Default is 16."
      }
    ],
    "return_type": "typing.List[str]",
    "docstring": "Refine a response to a query across text chunks.\nThis function is a wrapper for llama_index.response_synthesizers.Refine.\nFor more information, visit https://docs.llamaindex.ai/en/stable/examples/response_synthesizers/refine/.\n\n:param queries: The queries for retrieved passages.\n:param contents: The contents of retrieved passages.\n:param scores: The scores of retrieved passages.\n    Do not use in this function, so you can pass an empty list.\n:param ids: The ids of retrieved passages.\n    Do not use in this function, so you can pass an empty list.\n:param llm: The llm instance that will be used to summarize.\n:param prompt: The prompt template for refine.\n    If you want to use chat prompt, you should pass chat_prompt instead.\n    At prompt, you must specify where to put 'context_msg' and 'query_str'.\n    Default is None. When it is None, it will use llama index default prompt.\n:param chat_prompt: The chat prompt template for refine.\n    If you want to use normal prompt, you should pass prompt instead.\n    At prompt, you must specify where to put 'context_msg' and 'query_str'.\n    Default is None. When it is None, it will use llama index default chat prompt.\n:param batch: The batch size for llm.\n    Set low if you face some errors.\n    Default is 16.\n:return: The list of compressed texts."
  },
  {
    "function": "tree_summarize",
    "module": "autorag.nodes.passagecompressor.tree_summarize",
    "params": [
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": "The queries for retrieved passages."
      },
      {
        "name": "contents",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The contents of retrieved passages."
      },
      {
        "name": "scores",
        "param_type": "Any",
        "description": "The scores of retrieved passages.\n    Do not use in this function, so you can pass an empty list."
      },
      {
        "name": "ids",
        "param_type": "Any",
        "description": "The ids of retrieved passages.\n    Do not use in this function, so you can pass an empty list."
      },
      {
        "name": "llm",
        "param_type": "typing.Union[llama_index.core.service_context_elements.llm_predictor.LLMPredictor, llama_index.core.llms.llm.LLM]",
        "description": "The llm instance that will be used to summarize."
      },
      {
        "name": "prompt",
        "param_type": "typing.Optional[str]",
        "description": "The prompt template for summarization.\n    If you want to use chat prompt, you should pass chat_prompt instead.\n    At prompt, you must specify where to put 'context_str' and 'query_str'.\n    Default is None. When it is None, it will use llama index default prompt."
      },
      {
        "name": "chat_prompt",
        "param_type": "typing.Optional[str]",
        "description": "The chat prompt template for summarization.\n    If you want to use normal prompt, you should pass prompt instead.\n    At prompt, you must specify where to put 'context_str' and 'query_str'.\n    Default is None. When it is None, it will use llama index default chat prompt."
      },
      {
        "name": "batch",
        "param_type": "<class 'int'>",
        "description": "The batch size for llm.\n    Set low if you face some errors.\n    Default is 16."
      }
    ],
    "return_type": "typing.List[str]",
    "docstring": "Recursively merge retrieved texts and summarizes them in a bottom-up fashion.\nThis function is a wrapper for llama_index.response_synthesizers.TreeSummarize.\nFor more information, visit https://docs.llamaindex.ai/en/latest/examples/response_synthesizers/tree_summarize.html.\n\n:param queries: The queries for retrieved passages.\n:param contents: The contents of retrieved passages.\n:param scores: The scores of retrieved passages.\n    Do not use in this function, so you can pass an empty list.\n:param ids: The ids of retrieved passages.\n    Do not use in this function, so you can pass an empty list.\n:param llm: The llm instance that will be used to summarize.\n:param prompt: The prompt template for summarization.\n    If you want to use chat prompt, you should pass chat_prompt instead.\n    At prompt, you must specify where to put 'context_str' and 'query_str'.\n    Default is None. When it is None, it will use llama index default prompt.\n:param chat_prompt: The chat prompt template for summarization.\n    If you want to use normal prompt, you should pass prompt instead.\n    At prompt, you must specify where to put 'context_str' and 'query_str'.\n    Default is None. When it is None, it will use llama index default chat prompt.\n:param batch: The batch size for llm.\n    Set low if you face some errors.\n    Default is 16.\n:return: The list of compressed texts."
  },
  {
    "function": "make_llm",
    "module": "autorag.nodes.passagecompressor.base",
    "params": [
      {
        "name": "llm_name",
        "param_type": "<class 'str'>",
        "description": ""
      },
      {
        "name": "kwargs",
        "param_type": "typing.Dict",
        "description": ""
      }
    ],
    "return_type": "typing.Union[llama_index.core.service_context_elements.llm_predictor.LLMPredictor, llama_index.core.llms.llm.LLM]",
    "docstring": null
  },
  {
    "function": "passage_compressor_node",
    "module": "autorag.nodes.passagecompressor.base",
    "params": [
      {
        "name": "func",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "result_to_dataframe",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "column_names",
        "param_type": "typing.List[str]",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Decorator for converting results to pd.DataFrame."
  },
  {
    "function": "llmlingua_pure",
    "module": "autorag.nodes.passagecompressor.longllmlingua",
    "params": [
      {
        "name": "query",
        "param_type": "<class 'str'>",
        "description": "The query for retrieved passages."
      },
      {
        "name": "contents",
        "param_type": "typing.List[str]",
        "description": "The contents of retrieved passages."
      },
      {
        "name": "llm_lingua",
        "param_type": "<class 'llmlingua.prompt_compressor.PromptCompressor'>",
        "description": "The llm instance that will be used to compress."
      },
      {
        "name": "instructions",
        "param_type": "<class 'str'>",
        "description": "The instructions for compression."
      },
      {
        "name": "target_token",
        "param_type": "<class 'int'>",
        "description": "The target token for compression.\n    Default is 300."
      },
      {
        "name": "kwargs",
        "param_type": "Any",
        "description": "Additional keyword arguments."
      }
    ],
    "return_type": "<class 'str'>",
    "docstring": "Return the compressed text.\n\n:param query: The query for retrieved passages.\n:param contents: The contents of retrieved passages.\n:param llm_lingua: The llm instance that will be used to compress.\n:param instructions: The instructions for compression.\n:param target_token: The target token for compression.\n    Default is 300.\n:param kwargs: Additional keyword arguments.\n:return: The compressed text."
  },
  {
    "function": "longllmlingua",
    "module": "autorag.nodes.passagecompressor.longllmlingua",
    "params": [
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": "The queries for retrieved passages."
      },
      {
        "name": "contents",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The contents of retrieved passages."
      },
      {
        "name": "scores",
        "param_type": "Any",
        "description": "The scores of retrieved passages.\n    Do not use in this function, so you can pass an empty list."
      },
      {
        "name": "ids",
        "param_type": "Any",
        "description": "The ids of retrieved passages.\n    Do not use in this function, so you can pass an empty list."
      },
      {
        "name": "model_name",
        "param_type": "<class 'str'>",
        "description": "The model name to use for compression.\n    Default is \"NousResearch/Llama-2-7b-hf\"."
      },
      {
        "name": "instructions",
        "param_type": "typing.Optional[str]",
        "description": "The instructions for compression.\n    Default is None. When it is None, it will use default instructions."
      },
      {
        "name": "target_token",
        "param_type": "<class 'int'>",
        "description": "The target token for compression.\n    Default is 300."
      },
      {
        "name": "kwargs",
        "param_type": "Any",
        "description": "Additional keyword arguments."
      }
    ],
    "return_type": "typing.List[str]",
    "docstring": "Compresses the retrieved texts using LongLLMLingua.\nFor more information, visit https://github.com/microsoft/LLMLingua.\n\n:param queries: The queries for retrieved passages.\n:param contents: The contents of retrieved passages.\n:param scores: The scores of retrieved passages.\n    Do not use in this function, so you can pass an empty list.\n:param ids: The ids of retrieved passages.\n    Do not use in this function, so you can pass an empty list.\n:param model_name: The model name to use for compression.\n    Default is \"NousResearch/Llama-2-7b-hf\".\n:param instructions: The instructions for compression.\n    Default is None. When it is None, it will use default instructions.\n:param target_token: The target token for compression.\n    Default is 300.\n:param kwargs: Additional keyword arguments.\n:return: The list of compressed texts."
  },
  {
    "function": "passage_compressor_node",
    "module": "autorag.nodes.passagecompressor.base",
    "params": [
      {
        "name": "func",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "pass_compressor",
    "module": "autorag.nodes.passagecompressor.pass_compressor",
    "params": [
      {
        "name": "contents",
        "param_type": "typing.List[typing.List[str]]",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Do not perform any passage compression"
  },
  {
    "function": "passage_compressor_node",
    "module": "autorag.nodes.passagecompressor.base",
    "params": [
      {
        "name": "func",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "passage_compressor_node",
    "module": "autorag.nodes.passagecompressor.base",
    "params": [
      {
        "name": "func",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "process_batch",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "tasks",
        "param_type": "Any",
        "description": "A list of no-argument functions or coroutines to be executed."
      },
      {
        "name": "batch_size",
        "param_type": "<class 'int'>",
        "description": "The number of tasks to process in a single batch.\n    Default is 64."
      }
    ],
    "return_type": "typing.List[typing.Any]",
    "docstring": "Processes tasks in batches asynchronously.\n\n:param tasks: A list of no-argument functions or coroutines to be executed.\n:param batch_size: The number of tasks to process in a single batch.\n    Default is 64.\n:return: A list of results from the processed tasks."
  },
  {
    "function": "refine",
    "module": "autorag.nodes.passagecompressor.refine",
    "params": [
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": "The queries for retrieved passages."
      },
      {
        "name": "contents",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The contents of retrieved passages."
      },
      {
        "name": "scores",
        "param_type": "Any",
        "description": "The scores of retrieved passages.\n    Do not use in this function, so you can pass an empty list."
      },
      {
        "name": "ids",
        "param_type": "Any",
        "description": "The ids of retrieved passages.\n    Do not use in this function, so you can pass an empty list."
      },
      {
        "name": "llm",
        "param_type": "typing.Union[llama_index.core.service_context_elements.llm_predictor.LLMPredictor, llama_index.core.llms.llm.LLM]",
        "description": "The llm instance that will be used to summarize."
      },
      {
        "name": "prompt",
        "param_type": "typing.Optional[str]",
        "description": "The prompt template for refine.\n    If you want to use chat prompt, you should pass chat_prompt instead.\n    At prompt, you must specify where to put 'context_msg' and 'query_str'.\n    Default is None. When it is None, it will use llama index default prompt."
      },
      {
        "name": "chat_prompt",
        "param_type": "typing.Optional[str]",
        "description": "The chat prompt template for refine.\n    If you want to use normal prompt, you should pass prompt instead.\n    At prompt, you must specify where to put 'context_msg' and 'query_str'.\n    Default is None. When it is None, it will use llama index default chat prompt."
      },
      {
        "name": "batch",
        "param_type": "<class 'int'>",
        "description": "The batch size for llm.\n    Set low if you face some errors.\n    Default is 16."
      }
    ],
    "return_type": "typing.List[str]",
    "docstring": "Refine a response to a query across text chunks.\nThis function is a wrapper for llama_index.response_synthesizers.Refine.\nFor more information, visit https://docs.llamaindex.ai/en/stable/examples/response_synthesizers/refine/.\n\n:param queries: The queries for retrieved passages.\n:param contents: The contents of retrieved passages.\n:param scores: The scores of retrieved passages.\n    Do not use in this function, so you can pass an empty list.\n:param ids: The ids of retrieved passages.\n    Do not use in this function, so you can pass an empty list.\n:param llm: The llm instance that will be used to summarize.\n:param prompt: The prompt template for refine.\n    If you want to use chat prompt, you should pass chat_prompt instead.\n    At prompt, you must specify where to put 'context_msg' and 'query_str'.\n    Default is None. When it is None, it will use llama index default prompt.\n:param chat_prompt: The chat prompt template for refine.\n    If you want to use normal prompt, you should pass prompt instead.\n    At prompt, you must specify where to put 'context_msg' and 'query_str'.\n    Default is None. When it is None, it will use llama index default chat prompt.\n:param batch: The batch size for llm.\n    Set low if you face some errors.\n    Default is 16.\n:return: The list of compressed texts."
  },
  {
    "function": "evaluate_passage_compressor_node",
    "module": "autorag.nodes.passagecompressor.run",
    "params": [
      {
        "name": "result_df",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": ""
      },
      {
        "name": "retrieval_contents_gt",
        "param_type": "typing.List[typing.List[str]]",
        "description": ""
      },
      {
        "name": "metrics",
        "param_type": "typing.List[str]",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "fetch_contents",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "corpus_data",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": ""
      },
      {
        "name": "ids",
        "param_type": "typing.List[typing.List[str]]",
        "description": ""
      },
      {
        "name": "column_name",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "typing.List[typing.List[typing.Any]]",
    "docstring": null
  },
  {
    "function": "filter_by_threshold",
    "module": "autorag.strategy",
    "params": [
      {
        "name": "results",
        "param_type": "Any",
        "description": "The result list to be filtered."
      },
      {
        "name": "value",
        "param_type": "Any",
        "description": "The value list to be filtered.\n    It must have the same length with results."
      },
      {
        "name": "threshold",
        "param_type": "Any",
        "description": "The threshold value."
      },
      {
        "name": "metadatas",
        "param_type": "Any",
        "description": "The metadata of each result."
      }
    ],
    "return_type": "typing.Tuple[typing.List, typing.List]",
    "docstring": "Filter results by value's threshold.\n\n:param results: The result list to be filtered.\n:param value: The value list to be filtered.\n    It must have the same length with results.\n:param threshold: The threshold value.\n:param metadatas: The metadata of each result.\n:return: Filtered list of results and filtered list of metadatas.\n    Metadatas will be returned even if you did not give input metadatas.\n:rtype: Tuple[List, List]"
  },
  {
    "function": "measure_speed",
    "module": "autorag.strategy",
    "params": [
      {
        "name": "func",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "args",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "kwargs",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Method for measuring execution speed of the function."
  },
  {
    "function": "retrieval_token_f1",
    "module": "autorag.evaluation.metric.retrieval_contents",
    "params": [
      {
        "name": "gt",
        "param_type": "typing.List[str]",
        "description": ""
      },
      {
        "name": "pred",
        "param_type": "typing.List[str]",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "retrieval_token_precision",
    "module": "autorag.evaluation.metric.retrieval_contents",
    "params": [
      {
        "name": "gt",
        "param_type": "typing.List[str]",
        "description": ""
      },
      {
        "name": "pred",
        "param_type": "typing.List[str]",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "retrieval_token_recall",
    "module": "autorag.evaluation.metric.retrieval_contents",
    "params": [
      {
        "name": "gt",
        "param_type": "typing.List[str]",
        "description": ""
      },
      {
        "name": "pred",
        "param_type": "typing.List[str]",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "run_passage_compressor_node",
    "module": "autorag.nodes.passagecompressor.run",
    "params": [
      {
        "name": "modules",
        "param_type": "typing.List[typing.Callable]",
        "description": "Passage compressor modules to run."
      },
      {
        "name": "module_params",
        "param_type": "typing.List[typing.Dict]",
        "description": "Passage compressor module parameters."
      },
      {
        "name": "previous_result",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": "Previous result dataframe.\n    Could be retrieval, reranker modules result.\n    It means it must contain 'query', 'retrieved_contents', 'retrieved_ids', 'retrieve_scores' columns."
      },
      {
        "name": "node_line_dir",
        "param_type": "<class 'str'>",
        "description": "This node line's directory."
      },
      {
        "name": "strategies",
        "param_type": "typing.Dict",
        "description": "Strategies for passage compressor node.\n    In this node, we use\n    You can skip evaluation when you use only one module and a module parameter."
      }
    ],
    "return_type": "<class 'pandas.core.frame.DataFrame'>",
    "docstring": "Run evaluation and select the best module among passage compressor modules.\n\n:param modules: Passage compressor modules to run.\n:param module_params: Passage compressor module parameters.\n:param previous_result: Previous result dataframe.\n    Could be retrieval, reranker modules result.\n    It means it must contain 'query', 'retrieved_contents', 'retrieved_ids', 'retrieve_scores' columns.\n:param node_line_dir: This node line's directory.\n:param strategies: Strategies for passage compressor node.\n    In this node, we use\n    You can skip evaluation when you use only one module and a module parameter.\n:return: The best result dataframe with previous result columns.\n    This node will replace 'retrieved_contents' to compressed passages, so its length will be one."
  },
  {
    "function": "select_best",
    "module": "autorag.strategy",
    "params": [
      {
        "name": "results",
        "param_type": "typing.List[pandas.core.frame.DataFrame]",
        "description": ""
      },
      {
        "name": "columns",
        "param_type": "typing.Iterable[str]",
        "description": ""
      },
      {
        "name": "metadatas",
        "param_type": "typing.Optional[typing.List[typing.Any]]",
        "description": ""
      },
      {
        "name": "strategy_name",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "typing.Tuple[pandas.core.frame.DataFrame, typing.Any]",
    "docstring": null
  },
  {
    "function": "passage_compressor_node",
    "module": "autorag.nodes.passagecompressor.base",
    "params": [
      {
        "name": "func",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "process_batch",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "tasks",
        "param_type": "Any",
        "description": "A list of no-argument functions or coroutines to be executed."
      },
      {
        "name": "batch_size",
        "param_type": "<class 'int'>",
        "description": "The number of tasks to process in a single batch.\n    Default is 64."
      }
    ],
    "return_type": "typing.List[typing.Any]",
    "docstring": "Processes tasks in batches asynchronously.\n\n:param tasks: A list of no-argument functions or coroutines to be executed.\n:param batch_size: The number of tasks to process in a single batch.\n    Default is 64.\n:return: A list of results from the processed tasks."
  },
  {
    "function": "tree_summarize",
    "module": "autorag.nodes.passagecompressor.tree_summarize",
    "params": [
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": "The queries for retrieved passages."
      },
      {
        "name": "contents",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The contents of retrieved passages."
      },
      {
        "name": "scores",
        "param_type": "Any",
        "description": "The scores of retrieved passages.\n    Do not use in this function, so you can pass an empty list."
      },
      {
        "name": "ids",
        "param_type": "Any",
        "description": "The ids of retrieved passages.\n    Do not use in this function, so you can pass an empty list."
      },
      {
        "name": "llm",
        "param_type": "typing.Union[llama_index.core.service_context_elements.llm_predictor.LLMPredictor, llama_index.core.llms.llm.LLM]",
        "description": "The llm instance that will be used to summarize."
      },
      {
        "name": "prompt",
        "param_type": "typing.Optional[str]",
        "description": "The prompt template for summarization.\n    If you want to use chat prompt, you should pass chat_prompt instead.\n    At prompt, you must specify where to put 'context_str' and 'query_str'.\n    Default is None. When it is None, it will use llama index default prompt."
      },
      {
        "name": "chat_prompt",
        "param_type": "typing.Optional[str]",
        "description": "The chat prompt template for summarization.\n    If you want to use normal prompt, you should pass prompt instead.\n    At prompt, you must specify where to put 'context_str' and 'query_str'.\n    Default is None. When it is None, it will use llama index default chat prompt."
      },
      {
        "name": "batch",
        "param_type": "<class 'int'>",
        "description": "The batch size for llm.\n    Set low if you face some errors.\n    Default is 16."
      }
    ],
    "return_type": "typing.List[str]",
    "docstring": "Recursively merge retrieved texts and summarizes them in a bottom-up fashion.\nThis function is a wrapper for llama_index.response_synthesizers.TreeSummarize.\nFor more information, visit https://docs.llamaindex.ai/en/latest/examples/response_synthesizers/tree_summarize.html.\n\n:param queries: The queries for retrieved passages.\n:param contents: The contents of retrieved passages.\n:param scores: The scores of retrieved passages.\n    Do not use in this function, so you can pass an empty list.\n:param ids: The ids of retrieved passages.\n    Do not use in this function, so you can pass an empty list.\n:param llm: The llm instance that will be used to summarize.\n:param prompt: The prompt template for summarization.\n    If you want to use chat prompt, you should pass chat_prompt instead.\n    At prompt, you must specify where to put 'context_str' and 'query_str'.\n    Default is None. When it is None, it will use llama index default prompt.\n:param chat_prompt: The chat prompt template for summarization.\n    If you want to use normal prompt, you should pass prompt instead.\n    At prompt, you must specify where to put 'context_str' and 'query_str'.\n    Default is None. When it is None, it will use llama index default chat prompt.\n:param batch: The batch size for llm.\n    Set low if you face some errors.\n    Default is 16.\n:return: The list of compressed texts."
  },
  {
    "function": "pass_passage_filter",
    "module": "autorag.nodes.passagefilter.pass_passage_filter",
    "params": [
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": ""
      },
      {
        "name": "contents_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": ""
      },
      {
        "name": "scores_list",
        "param_type": "typing.List[typing.List[float]]",
        "description": ""
      },
      {
        "name": "ids_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Do not perform filtering.\nReturn given passages, scores, and ids as is."
  },
  {
    "function": "percentile_cutoff",
    "module": "autorag.nodes.passagefilter.percentile_cutoff",
    "params": [
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": "The list of queries to use for filtering"
      },
      {
        "name": "contents_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of contents to filter"
      },
      {
        "name": "scores_list",
        "param_type": "typing.List[typing.List[float]]",
        "description": "The list of lists of scores retrieved"
      },
      {
        "name": "ids_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of ids retrieved"
      },
      {
        "name": "percentile",
        "param_type": "<class 'float'>",
        "description": "The percentile to cut off"
      },
      {
        "name": "reverse",
        "param_type": "<class 'bool'>",
        "description": "If True, the lower the score, the better\n    Default is False."
      }
    ],
    "return_type": "typing.Tuple[typing.List[typing.List[str]], typing.List[typing.List[str]], typing.List[typing.List[float]]]",
    "docstring": "Filter out the contents that are below the content's length times percentile.\nIf This is a filter and does not override scores.\nIf the value of content's length times percentile is less than 1, keep the only one highest similarity content.\n\n:param queries: The list of queries to use for filtering\n:param contents_list: The list of lists of contents to filter\n:param scores_list: The list of lists of scores retrieved\n:param ids_list: The list of lists of ids retrieved\n:param percentile: The percentile to cut off\n:param reverse: If True, the lower the score, the better\n    Default is False.\n:return: Tuple of lists containing the filtered contents, ids, and scores"
  },
  {
    "function": "recency_filter",
    "module": "autorag.nodes.passagefilter.recency",
    "params": [
      {
        "name": "contents_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of contents to filter"
      },
      {
        "name": "scores_list",
        "param_type": "typing.List[typing.List[float]]",
        "description": "The list of lists of scores retrieved"
      },
      {
        "name": "ids_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of ids retrieved"
      },
      {
        "name": "time_list",
        "param_type": "typing.List[typing.List[datetime.datetime]]",
        "description": "The list of lists of datetime retrieved"
      },
      {
        "name": "threshold",
        "param_type": "typing.Union[datetime.datetime, datetime.date]",
        "description": "The threshold to cut off"
      }
    ],
    "return_type": "typing.Tuple[typing.List[typing.List[str]], typing.List[typing.List[str]], typing.List[typing.List[float]]]",
    "docstring": "Filter out the contents that are below the threshold datetime.\nIf all contents are filtered, keep the only one recency content.\nIf the threshold date format is incorrect, return the original contents.\n\n:param contents_list: The list of lists of contents to filter\n:param scores_list: The list of lists of scores retrieved\n:param ids_list: The list of lists of ids retrieved\n:param time_list: The list of lists of datetime retrieved\n:param threshold: The threshold to cut off\n:return: Tuple of lists containing the filtered contents, ids, and scores"
  },
  {
    "function": "similarity_percentile_cutoff",
    "module": "autorag.nodes.passagefilter.similarity_percentile_cutoff",
    "params": [
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": "The list of queries to use for filtering"
      },
      {
        "name": "contents_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of contents to filter"
      },
      {
        "name": "scores_list",
        "param_type": "typing.List[typing.List[float]]",
        "description": "The list of lists of scores retrieved"
      },
      {
        "name": "ids_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of ids retrieved"
      },
      {
        "name": "percentile",
        "param_type": "<class 'float'>",
        "description": "The percentile to cut off"
      },
      {
        "name": "embedding_model",
        "param_type": "typing.Optional[str]",
        "description": "The embedding model to use for calculating similarity\n    Default is OpenAIEmbedding."
      },
      {
        "name": "batch",
        "param_type": "<class 'int'>",
        "description": "The number of queries to be processed in a batch\n    Default is 128."
      }
    ],
    "return_type": "typing.Tuple[typing.List[typing.List[str]], typing.List[typing.List[str]], typing.List[typing.List[float]]]",
    "docstring": "Re-calculate each content's similarity with the query and filter out the contents that are below the content's\nlength times percentile. If This is a filter and does not override scores. The output of scores is not coming from\nquery-content similarity.\nIf the value of content's length times percentile is less than 1, keep the only one highest similarity content.\n\n:param queries: The list of queries to use for filtering\n:param contents_list: The list of lists of contents to filter\n:param scores_list: The list of lists of scores retrieved\n:param ids_list: The list of lists of ids retrieved\n:param percentile: The percentile to cut off\n:param embedding_model: The embedding model to use for calculating similarity\n    Default is OpenAIEmbedding.\n:param batch: The number of queries to be processed in a batch\n    Default is 128.\n:return: Tuple of lists containing the filtered contents, ids, and scores"
  },
  {
    "function": "similarity_threshold_cutoff",
    "module": "autorag.nodes.passagefilter.similarity_threshold_cutoff",
    "params": [
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": "The list of queries to use for filtering"
      },
      {
        "name": "contents_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of contents to filter"
      },
      {
        "name": "scores_list",
        "param_type": "typing.List[typing.List[float]]",
        "description": "The list of lists of scores retrieved"
      },
      {
        "name": "ids_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of ids retrieved"
      },
      {
        "name": "threshold",
        "param_type": "<class 'float'>",
        "description": "The threshold to cut off"
      },
      {
        "name": "embedding_model",
        "param_type": "typing.Optional[str]",
        "description": "The embedding model to use for calculating similarity\n    Default is OpenAIEmbedding."
      },
      {
        "name": "batch",
        "param_type": "<class 'int'>",
        "description": "The number of queries to be processed in a batch\n    Default is 128."
      }
    ],
    "return_type": "typing.Tuple[typing.List[typing.List[str]], typing.List[typing.List[str]], typing.List[typing.List[float]]]",
    "docstring": "Re-calculate each content's similarity with the query and filter out the contents that are below the threshold.\nIf all contents are filtered, keep the only one highest similarity content.\nThis is a filter and does not override scores.\nThe output of scores is not coming from query-content similarity.\n\n:param queries: The list of queries to use for filtering\n:param contents_list: The list of lists of contents to filter\n:param scores_list: The list of lists of scores retrieved\n:param ids_list: The list of lists of ids retrieved\n:param threshold: The threshold to cut off\n:param embedding_model: The embedding model to use for calculating similarity\n    Default is OpenAIEmbedding.\n:param batch: The number of queries to be processed in a batch\n    Default is 128.\n:return: Tuple of lists containing the filtered contents, ids, and scores"
  },
  {
    "function": "threshold_cutoff",
    "module": "autorag.nodes.passagefilter.threshold_cutoff",
    "params": [
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": "List of query strings (not used in the current implementation)."
      },
      {
        "name": "contents_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "List of content strings for each query."
      },
      {
        "name": "scores_list",
        "param_type": "typing.List[typing.List[float]]",
        "description": "List of scores for each content."
      },
      {
        "name": "ids_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "List of ids for each content."
      },
      {
        "name": "threshold",
        "param_type": "<class 'float'>",
        "description": "The minimum score to keep an item."
      },
      {
        "name": "reverse",
        "param_type": "<class 'bool'>",
        "description": "If True, the lower the score, the better.\n    Default is False."
      }
    ],
    "return_type": "typing.Tuple[typing.List[typing.List[str]], typing.List[typing.List[str]], typing.List[typing.List[float]]]",
    "docstring": "Filters the contents, scores, and ids based on a previous result's score.\nKeeps at least one item per query if all scores are below the threshold.\n\n:param queries: List of query strings (not used in the current implementation).\n:param contents_list: List of content strings for each query.\n:param scores_list: List of scores for each content.\n:param ids_list: List of ids for each content.\n:param threshold: The minimum score to keep an item.\n:param reverse: If True, the lower the score, the better.\n    Default is False.\n:return: Filtered lists of contents, ids, and scores."
  },
  {
    "function": "fetch_contents",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "corpus_data",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": ""
      },
      {
        "name": "ids",
        "param_type": "typing.List[typing.List[str]]",
        "description": ""
      },
      {
        "name": "column_name",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "typing.List[typing.List[typing.Any]]",
    "docstring": null
  },
  {
    "function": "passage_filter_node",
    "module": "autorag.nodes.passagefilter.base",
    "params": [
      {
        "name": "func",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "result_to_dataframe",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "column_names",
        "param_type": "typing.List[str]",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Decorator for converting results to pd.DataFrame."
  },
  {
    "function": "validate_qa_dataset",
    "module": "autorag.utils.preprocess",
    "params": [
      {
        "name": "df",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "pass_passage_filter",
    "module": "autorag.nodes.passagefilter.pass_passage_filter",
    "params": [
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": ""
      },
      {
        "name": "contents_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": ""
      },
      {
        "name": "scores_list",
        "param_type": "typing.List[typing.List[float]]",
        "description": ""
      },
      {
        "name": "ids_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Do not perform filtering.\nReturn given passages, scores, and ids as is."
  },
  {
    "function": "passage_filter_node",
    "module": "autorag.nodes.passagefilter.base",
    "params": [
      {
        "name": "func",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "passage_filter_node",
    "module": "autorag.nodes.passagefilter.base",
    "params": [
      {
        "name": "func",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "percentile_cutoff",
    "module": "autorag.nodes.passagefilter.percentile_cutoff",
    "params": [
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": "The list of queries to use for filtering"
      },
      {
        "name": "contents_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of contents to filter"
      },
      {
        "name": "scores_list",
        "param_type": "typing.List[typing.List[float]]",
        "description": "The list of lists of scores retrieved"
      },
      {
        "name": "ids_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of ids retrieved"
      },
      {
        "name": "percentile",
        "param_type": "<class 'float'>",
        "description": "The percentile to cut off"
      },
      {
        "name": "reverse",
        "param_type": "<class 'bool'>",
        "description": "If True, the lower the score, the better\n    Default is False."
      }
    ],
    "return_type": "typing.Tuple[typing.List[typing.List[str]], typing.List[typing.List[str]], typing.List[typing.List[float]]]",
    "docstring": "Filter out the contents that are below the content's length times percentile.\nIf This is a filter and does not override scores.\nIf the value of content's length times percentile is less than 1, keep the only one highest similarity content.\n\n:param queries: The list of queries to use for filtering\n:param contents_list: The list of lists of contents to filter\n:param scores_list: The list of lists of scores retrieved\n:param ids_list: The list of lists of ids retrieved\n:param percentile: The percentile to cut off\n:param reverse: If True, the lower the score, the better\n    Default is False.\n:return: Tuple of lists containing the filtered contents, ids, and scores"
  },
  {
    "function": "select_top_k",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "df",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "column_names",
        "param_type": "typing.List[str]",
        "description": ""
      },
      {
        "name": "top_k",
        "param_type": "<class 'int'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "sort_by_scores",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "row",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "reverse",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Sorts each row by 'scores' column.\nThe input column names must be 'contents', 'ids', and 'scores'.\nAnd its elements must be list type."
  },
  {
    "function": "passage_filter_node",
    "module": "autorag.nodes.passagefilter.base",
    "params": [
      {
        "name": "func",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "recency_filter",
    "module": "autorag.nodes.passagefilter.recency",
    "params": [
      {
        "name": "contents_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of contents to filter"
      },
      {
        "name": "scores_list",
        "param_type": "typing.List[typing.List[float]]",
        "description": "The list of lists of scores retrieved"
      },
      {
        "name": "ids_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of ids retrieved"
      },
      {
        "name": "time_list",
        "param_type": "typing.List[typing.List[datetime.datetime]]",
        "description": "The list of lists of datetime retrieved"
      },
      {
        "name": "threshold",
        "param_type": "typing.Union[datetime.datetime, datetime.date]",
        "description": "The threshold to cut off"
      }
    ],
    "return_type": "typing.Tuple[typing.List[typing.List[str]], typing.List[typing.List[str]], typing.List[typing.List[float]]]",
    "docstring": "Filter out the contents that are below the threshold datetime.\nIf all contents are filtered, keep the only one recency content.\nIf the threshold date format is incorrect, return the original contents.\n\n:param contents_list: The list of lists of contents to filter\n:param scores_list: The list of lists of scores retrieved\n:param ids_list: The list of lists of ids retrieved\n:param time_list: The list of lists of datetime retrieved\n:param threshold: The threshold to cut off\n:return: Tuple of lists containing the filtered contents, ids, and scores"
  },
  {
    "function": "evaluate_retrieval_node",
    "module": "autorag.nodes.retrieval.run",
    "params": [
      {
        "name": "result_df",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": "The result dataframe from a retrieval node."
      },
      {
        "name": "retrieval_gt",
        "param_type": "Any",
        "description": "Ground truth for retrieval from qa dataset."
      },
      {
        "name": "metrics",
        "param_type": "Any",
        "description": "Metric list from input strategies."
      },
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": "Query list from input strategies."
      },
      {
        "name": "generation_gt",
        "param_type": "typing.List[typing.List[str]]",
        "description": "Ground truth for generation from qa dataset."
      }
    ],
    "return_type": "<class 'pandas.core.frame.DataFrame'>",
    "docstring": "Evaluate retrieval node from retrieval node result dataframe.\n\n:param result_df: The result dataframe from a retrieval node.\n:param retrieval_gt: Ground truth for retrieval from qa dataset.\n:param metrics: Metric list from input strategies.\n:param queries: Query list from input strategies.\n:param generation_gt: Ground truth for generation from qa dataset.\n:return: Return result_df with metrics columns.\n    The columns will be 'retrieved_contents', 'retrieved_ids', 'retrieve_scores', and metric names."
  },
  {
    "function": "filter_by_threshold",
    "module": "autorag.strategy",
    "params": [
      {
        "name": "results",
        "param_type": "Any",
        "description": "The result list to be filtered."
      },
      {
        "name": "value",
        "param_type": "Any",
        "description": "The value list to be filtered.\n    It must have the same length with results."
      },
      {
        "name": "threshold",
        "param_type": "Any",
        "description": "The threshold value."
      },
      {
        "name": "metadatas",
        "param_type": "Any",
        "description": "The metadata of each result."
      }
    ],
    "return_type": "typing.Tuple[typing.List, typing.List]",
    "docstring": "Filter results by value's threshold.\n\n:param results: The result list to be filtered.\n:param value: The value list to be filtered.\n    It must have the same length with results.\n:param threshold: The threshold value.\n:param metadatas: The metadata of each result.\n:return: Filtered list of results and filtered list of metadatas.\n    Metadatas will be returned even if you did not give input metadatas.\n:rtype: Tuple[List, List]"
  },
  {
    "function": "measure_speed",
    "module": "autorag.strategy",
    "params": [
      {
        "name": "func",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "args",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "kwargs",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Method for measuring execution speed of the function."
  },
  {
    "function": "run_passage_filter_node",
    "module": "autorag.nodes.passagefilter.run",
    "params": [
      {
        "name": "modules",
        "param_type": "typing.List[typing.Callable]",
        "description": "Passage filter modules to run."
      },
      {
        "name": "module_params",
        "param_type": "typing.List[typing.Dict]",
        "description": "Passage filter module parameters."
      },
      {
        "name": "previous_result",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": "Previous result dataframe.\n    Could be retrieval, reranker, passage filter modules result.\n    It means it must contain 'query', 'retrieved_contents', 'retrieved_ids', 'retrieve_scores' columns."
      },
      {
        "name": "node_line_dir",
        "param_type": "<class 'str'>",
        "description": "This node line's directory."
      },
      {
        "name": "strategies",
        "param_type": "typing.Dict",
        "description": "Strategies for passage filter node.\n    In this node, we use 'retrieval_f1', 'retrieval_recall' and 'retrieval_precision'.\n    You can skip evaluation when you use only one module and a module parameter."
      }
    ],
    "return_type": "<class 'pandas.core.frame.DataFrame'>",
    "docstring": "Run evaluation and select the best module among passage filter node results.\n\n:param modules: Passage filter modules to run.\n:param module_params: Passage filter module parameters.\n:param previous_result: Previous result dataframe.\n    Could be retrieval, reranker, passage filter modules result.\n    It means it must contain 'query', 'retrieved_contents', 'retrieved_ids', 'retrieve_scores' columns.\n:param node_line_dir: This node line's directory.\n:param strategies: Strategies for passage filter node.\n    In this node, we use 'retrieval_f1', 'retrieval_recall' and 'retrieval_precision'.\n    You can skip evaluation when you use only one module and a module parameter.\n:return: The best result dataframe with previous result columns."
  },
  {
    "function": "select_best",
    "module": "autorag.strategy",
    "params": [
      {
        "name": "results",
        "param_type": "typing.List[pandas.core.frame.DataFrame]",
        "description": ""
      },
      {
        "name": "columns",
        "param_type": "typing.Iterable[str]",
        "description": ""
      },
      {
        "name": "metadatas",
        "param_type": "typing.Optional[typing.List[typing.Any]]",
        "description": ""
      },
      {
        "name": "strategy_name",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "typing.Tuple[pandas.core.frame.DataFrame, typing.Any]",
    "docstring": null
  },
  {
    "function": "calculate_cosine_similarity",
    "module": "autorag.evaluation.metric.util",
    "params": [
      {
        "name": "a",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "b",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "embedding_query_content",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": ""
      },
      {
        "name": "contents_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": ""
      },
      {
        "name": "embedding_model",
        "param_type": "typing.Optional[str]",
        "description": ""
      },
      {
        "name": "batch",
        "param_type": "<class 'int'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "passage_filter_node",
    "module": "autorag.nodes.passagefilter.base",
    "params": [
      {
        "name": "func",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "similarity_percentile_cutoff",
    "module": "autorag.nodes.passagefilter.similarity_percentile_cutoff",
    "params": [
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": "The list of queries to use for filtering"
      },
      {
        "name": "contents_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of contents to filter"
      },
      {
        "name": "scores_list",
        "param_type": "typing.List[typing.List[float]]",
        "description": "The list of lists of scores retrieved"
      },
      {
        "name": "ids_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of ids retrieved"
      },
      {
        "name": "percentile",
        "param_type": "<class 'float'>",
        "description": "The percentile to cut off"
      },
      {
        "name": "embedding_model",
        "param_type": "typing.Optional[str]",
        "description": "The embedding model to use for calculating similarity\n    Default is OpenAIEmbedding."
      },
      {
        "name": "batch",
        "param_type": "<class 'int'>",
        "description": "The number of queries to be processed in a batch\n    Default is 128."
      }
    ],
    "return_type": "typing.Tuple[typing.List[typing.List[str]], typing.List[typing.List[str]], typing.List[typing.List[float]]]",
    "docstring": "Re-calculate each content's similarity with the query and filter out the contents that are below the content's\nlength times percentile. If This is a filter and does not override scores. The output of scores is not coming from\nquery-content similarity.\nIf the value of content's length times percentile is less than 1, keep the only one highest similarity content.\n\n:param queries: The list of queries to use for filtering\n:param contents_list: The list of lists of contents to filter\n:param scores_list: The list of lists of scores retrieved\n:param ids_list: The list of lists of ids retrieved\n:param percentile: The percentile to cut off\n:param embedding_model: The embedding model to use for calculating similarity\n    Default is OpenAIEmbedding.\n:param batch: The number of queries to be processed in a batch\n    Default is 128.\n:return: Tuple of lists containing the filtered contents, ids, and scores"
  },
  {
    "function": "similarity_percentile_cutoff_pure",
    "module": "autorag.nodes.passagefilter.similarity_percentile_cutoff",
    "params": [
      {
        "name": "query_embedding",
        "param_type": "<class 'str'>",
        "description": "Query embedding"
      },
      {
        "name": "content_embeddings",
        "param_type": "typing.List[typing.List[float]]",
        "description": "Each content embedding"
      },
      {
        "name": "content_list",
        "param_type": "typing.List[str]",
        "description": "Each content"
      },
      {
        "name": "ids_list",
        "param_type": "typing.List[str]",
        "description": "Each id"
      },
      {
        "name": "scores_list",
        "param_type": "typing.List[float]",
        "description": "Each score"
      },
      {
        "name": "percentile",
        "param_type": "<class 'float'>",
        "description": "The percentile to cut off"
      }
    ],
    "return_type": "typing.Tuple[typing.List[str], typing.List[str], typing.List[float]]",
    "docstring": "Return tuple of lists containing the filtered contents, ids, and scores\n\n:param query_embedding: Query embedding\n:param content_embeddings: Each content embedding\n:param content_list: Each content\n:param ids_list: Each id\n:param scores_list: Each score\n:param percentile: The percentile to cut off\n:return: Tuple of lists containing the filtered contents, ids, and scores"
  },
  {
    "function": "calculate_cosine_similarity",
    "module": "autorag.evaluation.metric.util",
    "params": [
      {
        "name": "a",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "b",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "embedding_query_content",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": ""
      },
      {
        "name": "contents_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": ""
      },
      {
        "name": "embedding_model",
        "param_type": "typing.Optional[str]",
        "description": ""
      },
      {
        "name": "batch",
        "param_type": "<class 'int'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "passage_filter_node",
    "module": "autorag.nodes.passagefilter.base",
    "params": [
      {
        "name": "func",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "similarity_threshold_cutoff",
    "module": "autorag.nodes.passagefilter.similarity_threshold_cutoff",
    "params": [
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": "The list of queries to use for filtering"
      },
      {
        "name": "contents_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of contents to filter"
      },
      {
        "name": "scores_list",
        "param_type": "typing.List[typing.List[float]]",
        "description": "The list of lists of scores retrieved"
      },
      {
        "name": "ids_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of ids retrieved"
      },
      {
        "name": "threshold",
        "param_type": "<class 'float'>",
        "description": "The threshold to cut off"
      },
      {
        "name": "embedding_model",
        "param_type": "typing.Optional[str]",
        "description": "The embedding model to use for calculating similarity\n    Default is OpenAIEmbedding."
      },
      {
        "name": "batch",
        "param_type": "<class 'int'>",
        "description": "The number of queries to be processed in a batch\n    Default is 128."
      }
    ],
    "return_type": "typing.Tuple[typing.List[typing.List[str]], typing.List[typing.List[str]], typing.List[typing.List[float]]]",
    "docstring": "Re-calculate each content's similarity with the query and filter out the contents that are below the threshold.\nIf all contents are filtered, keep the only one highest similarity content.\nThis is a filter and does not override scores.\nThe output of scores is not coming from query-content similarity.\n\n:param queries: The list of queries to use for filtering\n:param contents_list: The list of lists of contents to filter\n:param scores_list: The list of lists of scores retrieved\n:param ids_list: The list of lists of ids retrieved\n:param threshold: The threshold to cut off\n:param embedding_model: The embedding model to use for calculating similarity\n    Default is OpenAIEmbedding.\n:param batch: The number of queries to be processed in a batch\n    Default is 128.\n:return: Tuple of lists containing the filtered contents, ids, and scores"
  },
  {
    "function": "similarity_threshold_cutoff_pure",
    "module": "autorag.nodes.passagefilter.similarity_threshold_cutoff",
    "params": [
      {
        "name": "query_embedding",
        "param_type": "<class 'str'>",
        "description": "Query embedding"
      },
      {
        "name": "content_embeddings",
        "param_type": "typing.List[typing.List[float]]",
        "description": "Each content embedding"
      },
      {
        "name": "threshold",
        "param_type": "<class 'float'>",
        "description": "The threshold to cut off"
      }
    ],
    "return_type": "typing.List[int]",
    "docstring": "Return indices that have to remain.\nReturn at least one index if there is nothing to remain.\n\n:param query_embedding: Query embedding\n:param content_embeddings: Each content embedding\n:param threshold: The threshold to cut off\n:return: Indices to remain at the contents"
  },
  {
    "function": "passage_filter_node",
    "module": "autorag.nodes.passagefilter.base",
    "params": [
      {
        "name": "func",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "threshold_cutoff",
    "module": "autorag.nodes.passagefilter.threshold_cutoff",
    "params": [
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": "List of query strings (not used in the current implementation)."
      },
      {
        "name": "contents_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "List of content strings for each query."
      },
      {
        "name": "scores_list",
        "param_type": "typing.List[typing.List[float]]",
        "description": "List of scores for each content."
      },
      {
        "name": "ids_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "List of ids for each content."
      },
      {
        "name": "threshold",
        "param_type": "<class 'float'>",
        "description": "The minimum score to keep an item."
      },
      {
        "name": "reverse",
        "param_type": "<class 'bool'>",
        "description": "If True, the lower the score, the better.\n    Default is False."
      }
    ],
    "return_type": "typing.Tuple[typing.List[typing.List[str]], typing.List[typing.List[str]], typing.List[typing.List[float]]]",
    "docstring": "Filters the contents, scores, and ids based on a previous result's score.\nKeeps at least one item per query if all scores are below the threshold.\n\n:param queries: List of query strings (not used in the current implementation).\n:param contents_list: List of content strings for each query.\n:param scores_list: List of scores for each content.\n:param ids_list: List of ids for each content.\n:param threshold: The minimum score to keep an item.\n:param reverse: If True, the lower the score, the better.\n    Default is False.\n:return: Filtered lists of contents, ids, and scores."
  },
  {
    "function": "threshold_cutoff_pure",
    "module": "autorag.nodes.passagefilter.threshold_cutoff",
    "params": [
      {
        "name": "scores_list",
        "param_type": "typing.List[float]",
        "description": "Each score"
      },
      {
        "name": "threshold",
        "param_type": "<class 'float'>",
        "description": "The threshold to cut off"
      },
      {
        "name": "reverse",
        "param_type": "<class 'bool'>",
        "description": "If True, the lower the score, the better\n    Default is False."
      }
    ],
    "return_type": "typing.List[int]",
    "docstring": "Return indices that have to remain.\nReturn at least one index if there is nothing to remain.\n\n:param scores_list: Each score\n:param threshold: The threshold to cut off\n:param reverse: If True, the lower the score, the better\n    Default is False.\n:return: Indices to remain at the contents"
  },
  {
    "function": "cohere_reranker",
    "module": "autorag.nodes.passagereranker.cohere",
    "params": [
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": "The list of queries to use for reranking"
      },
      {
        "name": "contents_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of contents to rerank"
      },
      {
        "name": "scores_list",
        "param_type": "typing.List[typing.List[float]]",
        "description": "The list of lists of scores retrieved from the initial ranking"
      },
      {
        "name": "ids_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of ids retrieved from the initial ranking"
      },
      {
        "name": "top_k",
        "param_type": "<class 'int'>",
        "description": "The number of passages to be retrieved"
      },
      {
        "name": "batch",
        "param_type": "<class 'int'>",
        "description": "The number of queries to be processed in a batch"
      },
      {
        "name": "model",
        "param_type": "<class 'str'>",
        "description": "The model name for Cohere rerank.\n    You can choose between \"rerank-multilingual-v2.0\" and \"rerank-english-v2.0\".\n    Default is \"rerank-multilingual-v2.0\"."
      },
      {
        "name": "api_key",
        "param_type": "typing.Optional[str]",
        "description": "The API key for Cohere rerank.\n    You can set it in the environment variable COHERE_API_KEY.\n    Or, you can directly set it on the config YAML file using this parameter.\n    Default is env variable \"COHERE_API_KEY\"."
      }
    ],
    "return_type": "typing.Tuple[typing.List[typing.List[str]], typing.List[typing.List[str]], typing.List[typing.List[float]]]",
    "docstring": "Rerank a list of contents with Cohere rerank models.\nYou can get the API key from https://cohere.com/rerank and set it in the environment variable COHERE_API_KEY.\n\n:param queries: The list of queries to use for reranking\n:param contents_list: The list of lists of contents to rerank\n:param scores_list: The list of lists of scores retrieved from the initial ranking\n:param ids_list: The list of lists of ids retrieved from the initial ranking\n:param top_k: The number of passages to be retrieved\n:param batch: The number of queries to be processed in a batch\n:param model: The model name for Cohere rerank.\n    You can choose between \"rerank-multilingual-v2.0\" and \"rerank-english-v2.0\".\n    Default is \"rerank-multilingual-v2.0\".\n:param api_key: The API key for Cohere rerank.\n    You can set it in the environment variable COHERE_API_KEY.\n    Or, you can directly set it on the config YAML file using this parameter.\n    Default is env variable \"COHERE_API_KEY\".\n:return: Tuple of lists containing the reranked contents, ids, and scores"
  },
  {
    "function": "colbert_reranker",
    "module": "autorag.nodes.passagereranker.colbert",
    "params": [
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": "The list of queries to use for reranking"
      },
      {
        "name": "contents_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of contents to rerank"
      },
      {
        "name": "scores_list",
        "param_type": "typing.List[typing.List[float]]",
        "description": "The list of lists of scores retrieved from the initial ranking"
      },
      {
        "name": "ids_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of ids retrieved from the initial ranking"
      },
      {
        "name": "top_k",
        "param_type": "<class 'int'>",
        "description": "The number of passages to be retrieved"
      },
      {
        "name": "batch",
        "param_type": "<class 'int'>",
        "description": "The number of queries to be processed in a batch\n    Default is 64."
      },
      {
        "name": "model_name",
        "param_type": "<class 'str'>",
        "description": "The model name for Colbert rerank.\n    You can choose colbert model for reranking.\n    Default is \"colbert-ir/colbertv2.0\"."
      }
    ],
    "return_type": "typing.Tuple[typing.List[typing.List[str]], typing.List[typing.List[str]], typing.List[typing.List[float]]]",
    "docstring": "Rerank a list of contents with Colbert rerank models.\nYou can get more information about a Colbert model at https://huggingface.co/colbert-ir/colbertv2.0.\nIt uses BERT-based model, so recommend using CUDA gpu for faster reranking.\n\n:param queries: The list of queries to use for reranking\n:param contents_list: The list of lists of contents to rerank\n:param scores_list: The list of lists of scores retrieved from the initial ranking\n:param ids_list: The list of lists of ids retrieved from the initial ranking\n:param top_k: The number of passages to be retrieved\n:param batch: The number of queries to be processed in a batch\n    Default is 64.\n:param model_name: The model name for Colbert rerank.\n    You can choose colbert model for reranking.\n    Default is \"colbert-ir/colbertv2.0\".\n:return: Tuple of lists containing the reranked contents, ids, and scores"
  },
  {
    "function": "flag_embedding_llm_reranker",
    "module": "autorag.nodes.passagereranker.flag_embedding_llm",
    "params": [
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": "The list of queries to use for reranking"
      },
      {
        "name": "contents_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of contents to rerank"
      },
      {
        "name": "scores_list",
        "param_type": "typing.List[typing.List[float]]",
        "description": "The list of lists of scores retrieved from the initial ranking"
      },
      {
        "name": "ids_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of ids retrieved from the initial ranking"
      },
      {
        "name": "top_k",
        "param_type": "<class 'int'>",
        "description": "The number of passages to be retrieved"
      },
      {
        "name": "batch",
        "param_type": "<class 'int'>",
        "description": "The number of queries to be processed in a batch\n    Default is 64."
      },
      {
        "name": "use_fp16",
        "param_type": "<class 'bool'>",
        "description": "Whether to use fp16 for inference"
      },
      {
        "name": "model_name",
        "param_type": "<class 'str'>",
        "description": "The name of the BAAI Reranker LLM-based-model name.\n    Default is \"BAAI/bge-reranker-v2-gemma\""
      }
    ],
    "return_type": "typing.Tuple[typing.List[typing.List[str]], typing.List[typing.List[str]], typing.List[typing.List[float]]]",
    "docstring": "Rerank a list of contents based on their relevance to a query using BAAI LLM-based-Reranker model.\n\n:param queries: The list of queries to use for reranking\n:param contents_list: The list of lists of contents to rerank\n:param scores_list: The list of lists of scores retrieved from the initial ranking\n:param ids_list: The list of lists of ids retrieved from the initial ranking\n:param top_k: The number of passages to be retrieved\n:param batch: The number of queries to be processed in a batch\n    Default is 64.\n:param use_fp16: Whether to use fp16 for inference\n:param model_name: The name of the BAAI Reranker LLM-based-model name.\n    Default is \"BAAI/bge-reranker-v2-gemma\"\n:return: tuple of lists containing the reranked contents, ids, and scores"
  },
  {
    "function": "flag_embedding_reranker",
    "module": "autorag.nodes.passagereranker.flag_embedding",
    "params": [
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": "The list of queries to use for reranking"
      },
      {
        "name": "contents_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of contents to rerank"
      },
      {
        "name": "scores_list",
        "param_type": "typing.List[typing.List[float]]",
        "description": "The list of lists of scores retrieved from the initial ranking"
      },
      {
        "name": "ids_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of ids retrieved from the initial ranking"
      },
      {
        "name": "top_k",
        "param_type": "<class 'int'>",
        "description": "The number of passages to be retrieved"
      },
      {
        "name": "batch",
        "param_type": "<class 'int'>",
        "description": "The number of queries to be processed in a batch\n    Default is 64."
      },
      {
        "name": "use_fp16",
        "param_type": "<class 'bool'>",
        "description": "Whether to use fp16 for inference"
      },
      {
        "name": "model_name",
        "param_type": "<class 'str'>",
        "description": "The name of the BAAI Reranker normal-model name.\n    Default is \"BAAI/bge-reranker-large\""
      }
    ],
    "return_type": "typing.Tuple[typing.List[typing.List[str]], typing.List[typing.List[str]], typing.List[typing.List[float]]]",
    "docstring": "Rerank a list of contents based on their relevance to a query using BAAI normal-Reranker model.\n\n:param queries: The list of queries to use for reranking\n:param contents_list: The list of lists of contents to rerank\n:param scores_list: The list of lists of scores retrieved from the initial ranking\n:param ids_list: The list of lists of ids retrieved from the initial ranking\n:param top_k: The number of passages to be retrieved\n:param batch: The number of queries to be processed in a batch\n    Default is 64.\n:param use_fp16: Whether to use fp16 for inference\n:param model_name: The name of the BAAI Reranker normal-model name.\n    Default is \"BAAI/bge-reranker-large\"\n:return: tuple of lists containing the reranked contents, ids, and scores"
  },
  {
    "function": "jina_reranker",
    "module": "autorag.nodes.passagereranker.jina",
    "params": [
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": "The list of queries to use for reranking"
      },
      {
        "name": "contents_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of contents to rerank"
      },
      {
        "name": "scores_list",
        "param_type": "typing.List[typing.List[float]]",
        "description": "The list of lists of scores retrieved from the initial ranking"
      },
      {
        "name": "ids_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of ids retrieved from the initial ranking"
      },
      {
        "name": "top_k",
        "param_type": "<class 'int'>",
        "description": "The number of passages to be retrieved"
      },
      {
        "name": "api_key",
        "param_type": "typing.Optional[str]",
        "description": "The API key for Jina rerank.\n    You can set it in the environment variable JINAAI_API_KEY.\n    Or, you can directly set it on the config YAML file using this parameter.\n    Default is env variable \"JINAAI_API_KEY\"."
      },
      {
        "name": "model",
        "param_type": "<class 'str'>",
        "description": "The model name for Cohere rerank.\n    You can choose between \"jina-reranker-v1-base-en\" and \"jina-colbert-v1-en\".\n    Default is \"jina-reranker-v1-base-en\"."
      },
      {
        "name": "batch",
        "param_type": "<class 'int'>",
        "description": "The number of queries to be processed in a batch"
      }
    ],
    "return_type": "typing.Tuple[typing.List[typing.List[str]], typing.List[typing.List[str]], typing.List[typing.List[float]]]",
    "docstring": "Rerank a list of contents with Jina rerank models.\nYou can get the API key from https://jina.ai/reranker and set it in the environment variable JINAAI_API_KEY.\n\n:param queries: The list of queries to use for reranking\n:param contents_list: The list of lists of contents to rerank\n:param scores_list: The list of lists of scores retrieved from the initial ranking\n:param ids_list: The list of lists of ids retrieved from the initial ranking\n:param top_k: The number of passages to be retrieved\n:param api_key: The API key for Jina rerank.\n    You can set it in the environment variable JINAAI_API_KEY.\n    Or, you can directly set it on the config YAML file using this parameter.\n    Default is env variable \"JINAAI_API_KEY\".\n:param model: The model name for Cohere rerank.\n    You can choose between \"jina-reranker-v1-base-en\" and \"jina-colbert-v1-en\".\n    Default is \"jina-reranker-v1-base-en\".\n:param batch: The number of queries to be processed in a batch\n:return: Tuple of lists containing the reranked contents, ids, and scores"
  },
  {
    "function": "koreranker",
    "module": "autorag.nodes.passagereranker.koreranker",
    "params": [
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": "The list of queries to use for reranking"
      },
      {
        "name": "contents_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of contents to rerank"
      },
      {
        "name": "scores_list",
        "param_type": "typing.List[typing.List[float]]",
        "description": "The list of lists of scores retrieved from the initial ranking"
      },
      {
        "name": "ids_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of ids retrieved from the initial ranking"
      },
      {
        "name": "top_k",
        "param_type": "<class 'int'>",
        "description": "The number of passages to be retrieved"
      },
      {
        "name": "batch",
        "param_type": "<class 'int'>",
        "description": "The number of queries to be processed in a batch\n    Default is 64."
      }
    ],
    "return_type": "typing.Tuple[typing.List[typing.List[str]], typing.List[typing.List[str]], typing.List[typing.List[float]]]",
    "docstring": "Rerank a list of contents based on their relevance to a query using ko-reranker.\nko-reranker is a reranker based on korean (https://huggingface.co/Dongjin-kr/ko-reranker).\n\n:param queries: The list of queries to use for reranking\n:param contents_list: The list of lists of contents to rerank\n:param scores_list: The list of lists of scores retrieved from the initial ranking\n:param ids_list: The list of lists of ids retrieved from the initial ranking\n:param top_k: The number of passages to be retrieved\n:param batch: The number of queries to be processed in a batch\n    Default is 64.\n:return: tuple of lists containing the reranked contents, ids, and scores"
  },
  {
    "function": "monot5",
    "module": "autorag.nodes.passagereranker.monot5",
    "params": [
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": "The list of queries to use for reranking"
      },
      {
        "name": "contents_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of contents to rerank"
      },
      {
        "name": "scores_list",
        "param_type": "typing.List[typing.List[float]]",
        "description": "The list of lists of scores retrieved from the initial ranking"
      },
      {
        "name": "ids_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of ids retrieved from the initial ranking"
      },
      {
        "name": "top_k",
        "param_type": "<class 'int'>",
        "description": "The number of passages to be retrieved"
      },
      {
        "name": "model_name",
        "param_type": "<class 'str'>",
        "description": "The name of the MonoT5 model to use for reranking\n    Note: default model name is 'castorini/monot5-3b-msmarco-10k'\n        If there is a '/' in the model name parameter,\n        when we create the file to store the results, the path will be twisted because of the '/'.\n        Therefore, it will be received as '_' instead of '/'."
      },
      {
        "name": "batch",
        "param_type": "<class 'int'>",
        "description": "The number of queries to be processed in a batch"
      }
    ],
    "return_type": "typing.Tuple[typing.List[typing.List[str]], typing.List[typing.List[str]], typing.List[typing.List[float]]]",
    "docstring": "Rerank a list of contents based on their relevance to a query using MonoT5.\n\n:param queries: The list of queries to use for reranking\n:param contents_list: The list of lists of contents to rerank\n:param scores_list: The list of lists of scores retrieved from the initial ranking\n:param ids_list: The list of lists of ids retrieved from the initial ranking\n:param top_k: The number of passages to be retrieved\n:param model_name: The name of the MonoT5 model to use for reranking\n    Note: default model name is 'castorini/monot5-3b-msmarco-10k'\n        If there is a '/' in the model name parameter,\n        when we create the file to store the results, the path will be twisted because of the '/'.\n        Therefore, it will be received as '_' instead of '/'.\n:param batch: The number of queries to be processed in a batch\n:return: tuple of lists containing the reranked contents, ids, and scores"
  },
  {
    "function": "pass_reranker",
    "module": "autorag.nodes.passagereranker.pass_reranker",
    "params": [
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": ""
      },
      {
        "name": "contents_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": ""
      },
      {
        "name": "scores_list",
        "param_type": "typing.List[typing.List[float]]",
        "description": ""
      },
      {
        "name": "ids_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": ""
      },
      {
        "name": "top_k",
        "param_type": "<class 'int'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Do not perform reranking.\nReturn the given top-k passages as is."
  },
  {
    "function": "rankgpt",
    "module": "autorag.nodes.passagereranker.rankgpt",
    "params": [
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": "The list of queries to use for reranking"
      },
      {
        "name": "contents_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of contents to rerank"
      },
      {
        "name": "scores_list",
        "param_type": "typing.List[typing.List[float]]",
        "description": "The list of lists of scores retrieved from the initial ranking"
      },
      {
        "name": "ids_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of ids retrieved from the initial ranking"
      },
      {
        "name": "top_k",
        "param_type": "<class 'int'>",
        "description": "The number of passages to be retrieved"
      },
      {
        "name": "llm",
        "param_type": "typing.Optional[llama_index.core.llms.llm.LLM]",
        "description": "The LLM model to use for RankGPT rerank.\n    It is LlamaIndex model.\n    Default is OpenAI model with gpt-3.5-turbo-16k."
      },
      {
        "name": "verbose",
        "param_type": "<class 'bool'>",
        "description": "Whether to print intermediate steps."
      },
      {
        "name": "rankgpt_rerank_prompt",
        "param_type": "typing.Optional[str]",
        "description": "The prompt template for RankGPT rerank.\n    Default is RankGPT's default prompt."
      },
      {
        "name": "batch",
        "param_type": "<class 'int'>",
        "description": "The number of queries to be processed in a batch."
      },
      {
        "name": "kwargs",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "typing.Tuple[typing.List[typing.List[str]], typing.List[typing.List[str]], typing.List[typing.List[float]]]",
    "docstring": "Rerank given context paragraphs using RankGPT.\nReturn pseudo scores, since the actual scores are not available on RankGPT.\n\n:param queries: The list of queries to use for reranking\n:param contents_list: The list of lists of contents to rerank\n:param scores_list: The list of lists of scores retrieved from the initial ranking\n:param ids_list: The list of lists of ids retrieved from the initial ranking\n:param top_k: The number of passages to be retrieved\n:param llm: The LLM model to use for RankGPT rerank.\n    It is LlamaIndex model.\n    Default is OpenAI model with gpt-3.5-turbo-16k.\n:param verbose: Whether to print intermediate steps.\n:param rankgpt_rerank_prompt: The prompt template for RankGPT rerank.\n    Default is RankGPT's default prompt.\n:param batch: The number of queries to be processed in a batch.\n:return: Tuple of lists containing the reranked contents, ids, and scores"
  },
  {
    "function": "sentence_transformer_reranker",
    "module": "autorag.nodes.passagereranker.sentence_transformer",
    "params": [
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": "The list of queries to use for reranking"
      },
      {
        "name": "contents_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of contents to rerank"
      },
      {
        "name": "scores_list",
        "param_type": "typing.List[typing.List[float]]",
        "description": "The list of lists of scores retrieved from the initial ranking"
      },
      {
        "name": "ids_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of ids retrieved from the initial ranking"
      },
      {
        "name": "top_k",
        "param_type": "<class 'int'>",
        "description": "The number of passages to be retrieved"
      },
      {
        "name": "batch",
        "param_type": "<class 'int'>",
        "description": "The number of queries to be processed in a batch"
      },
      {
        "name": "sentence_transformer_max_length",
        "param_type": "<class 'int'>",
        "description": "The maximum length of the input text for the Sentence Transformer model"
      },
      {
        "name": "model_name",
        "param_type": "<class 'str'>",
        "description": "The name of the Sentence Transformer model to use for reranking\n    Default is \"cross-encoder/ms-marco-MiniLM-L-2-v2\""
      }
    ],
    "return_type": "typing.Tuple[typing.List[typing.List[str]], typing.List[typing.List[str]], typing.List[typing.List[float]]]",
    "docstring": "Rerank a list of contents based on their relevance to a query using Sentence Transformer model.\n\n:param queries: The list of queries to use for reranking\n:param contents_list: The list of lists of contents to rerank\n:param scores_list: The list of lists of scores retrieved from the initial ranking\n:param ids_list: The list of lists of ids retrieved from the initial ranking\n:param top_k: The number of passages to be retrieved\n:param batch: The number of queries to be processed in a batch\n:param sentence_transformer_max_length: The maximum length of the input text for the Sentence Transformer model\n:param model_name: The name of the Sentence Transformer model to use for reranking\n    Default is \"cross-encoder/ms-marco-MiniLM-L-2-v2\"\n:return: tuple of lists containing the reranked contents, ids, and scores"
  },
  {
    "function": "tart",
    "module": "autorag.nodes.passagereranker.tart.tart",
    "params": [
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": "The list of queries to use for reranking"
      },
      {
        "name": "contents_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of contents to rerank"
      },
      {
        "name": "scores_list",
        "param_type": "typing.List[typing.List[float]]",
        "description": "The list of lists of scores retrieved from the initial ranking"
      },
      {
        "name": "ids_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of ids retrieved from the initial ranking"
      },
      {
        "name": "top_k",
        "param_type": "<class 'int'>",
        "description": "The number of passages to be retrieved"
      },
      {
        "name": "instruction",
        "param_type": "<class 'str'>",
        "description": "The instruction for reranking.\n    Note: default instruction is \"Find passage to answer given question\"\n        The default instruction from the TART paper is being used.\n        If you want to use a different instruction, you can change the instruction through this parameter"
      },
      {
        "name": "batch",
        "param_type": "<class 'int'>",
        "description": "The number of queries to be processed in a batch"
      }
    ],
    "return_type": "typing.Tuple[typing.List[typing.List[str]], typing.List[typing.List[str]], typing.List[typing.List[float]]]",
    "docstring": "Rerank a list of contents based on their relevance to a query using Tart.\nTART is a reranker based on TART (https://github.com/facebookresearch/tart).\nYou can rerank the passages with the instruction using TARTReranker.\nThe default model is facebook/tart-full-flan-t5-xl.\n\n:param queries: The list of queries to use for reranking\n:param contents_list: The list of lists of contents to rerank\n:param scores_list: The list of lists of scores retrieved from the initial ranking\n:param ids_list: The list of lists of ids retrieved from the initial ranking\n:param top_k: The number of passages to be retrieved\n:param instruction: The instruction for reranking.\n    Note: default instruction is \"Find passage to answer given question\"\n        The default instruction from the TART paper is being used.\n        If you want to use a different instruction, you can change the instruction through this parameter\n:param batch: The number of queries to be processed in a batch\n:return: tuple of lists containing the reranked contents, ids, and scores"
  },
  {
    "function": "time_reranker",
    "module": "autorag.nodes.passagereranker.time_reranker",
    "params": [
      {
        "name": "contents_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of contents"
      },
      {
        "name": "scores_list",
        "param_type": "typing.List[typing.List[float]]",
        "description": "The list of lists of scores from the initial ranking"
      },
      {
        "name": "ids_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of ids"
      },
      {
        "name": "top_k",
        "param_type": "<class 'int'>",
        "description": "The number of passages to be retrieved after reranking"
      },
      {
        "name": "time_list",
        "param_type": "typing.List[typing.List[datetime.datetime]]",
        "description": "The metadata list of lists of datetime.datetime\n    It automatically extracts the 'last_modified_datetime' key from the metadata in the corpus data."
      }
    ],
    "return_type": "typing.Tuple[typing.List[typing.List[str]], typing.List[typing.List[str]], typing.List[typing.List[float]]]",
    "docstring": "Rerank the passages based on merely the datetime of the passage.\nIt uses 'last_modified_datetime' key in the corpus metadata,\nso the metadata should be in the format of {'last_modified_datetime': datetime.datetime} at the corpus data file.\n\n:param contents_list: The list of lists of contents\n:param scores_list: The list of lists of scores from the initial ranking\n:param ids_list: The list of lists of ids\n:param top_k: The number of passages to be retrieved after reranking\n:param time_list: The metadata list of lists of datetime.datetime\n    It automatically extracts the 'last_modified_datetime' key from the metadata in the corpus data.\n:return: The reranked contents, ids, and scores"
  },
  {
    "function": "upr",
    "module": "autorag.nodes.passagereranker.upr",
    "params": [
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": "The list of queries to use for reranking"
      },
      {
        "name": "contents_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of contents to rerank"
      },
      {
        "name": "scores_list",
        "param_type": "typing.List[typing.List[float]]",
        "description": "The list of lists of scores retrieved from the initial ranking"
      },
      {
        "name": "ids_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of ids retrieved from the initial ranking"
      },
      {
        "name": "top_k",
        "param_type": "<class 'int'>",
        "description": "The number of passages to be retrieved"
      },
      {
        "name": "use_bf16",
        "param_type": "<class 'bool'>",
        "description": "Whether to use bfloat16 for the model. Default is False."
      },
      {
        "name": "prefix_prompt",
        "param_type": "<class 'str'>",
        "description": "The prefix prompt for the language model that generates question for reranking.\n    Default is \"Passage: \".\n    The prefix prompt serves as the initial context or instruction for the language model.\n    It sets the stage for what is expected in the output"
      },
      {
        "name": "suffix_prompt",
        "param_type": "<class 'str'>",
        "description": "The suffix prompt for the language model that generates question for reranking.\n    Default is \"Please write a question based on this passage.\".\n    The suffix prompt provides a cue or a closing instruction to the language model,\n        signaling how to conclude the generated text or what format to follow at the end."
      }
    ],
    "return_type": "typing.Tuple[typing.List[typing.List[str]], typing.List[typing.List[str]], typing.List[typing.List[float]]]",
    "docstring": "Rerank a list of contents based on their relevance to a query using UPR.\nUPR is a reranker based on UPR (https://github.com/DevSinghSachan/unsupervised-passage-reranking).\nThe language model will make a question based on the passage and rerank the passages by the likelihood of the question.\nThe default model is t5-large.\n\n:param queries: The list of queries to use for reranking\n:param contents_list: The list of lists of contents to rerank\n:param scores_list: The list of lists of scores retrieved from the initial ranking\n:param ids_list: The list of lists of ids retrieved from the initial ranking\n:param top_k: The number of passages to be retrieved\n:param use_bf16: Whether to use bfloat16 for the model. Default is False.\n:param prefix_prompt: The prefix prompt for the language model that generates question for reranking.\n    Default is \"Passage: \".\n    The prefix prompt serves as the initial context or instruction for the language model.\n    It sets the stage for what is expected in the output\n:param suffix_prompt: The suffix prompt for the language model that generates question for reranking.\n    Default is \"Please write a question based on this passage.\".\n    The suffix prompt provides a cue or a closing instruction to the language model,\n        signaling how to conclude the generated text or what format to follow at the end.\n:return: tuple of lists containing the reranked contents, ids, and scores"
  },
  {
    "function": "fetch_contents",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "corpus_data",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": ""
      },
      {
        "name": "ids",
        "param_type": "typing.List[typing.List[str]]",
        "description": ""
      },
      {
        "name": "column_name",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "typing.List[typing.List[typing.Any]]",
    "docstring": null
  },
  {
    "function": "passage_reranker_node",
    "module": "autorag.nodes.passagereranker.base",
    "params": [
      {
        "name": "func",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "result_to_dataframe",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "column_names",
        "param_type": "typing.List[str]",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Decorator for converting results to pd.DataFrame."
  },
  {
    "function": "validate_qa_dataset",
    "module": "autorag.utils.preprocess",
    "params": [
      {
        "name": "df",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "cohere_rerank_pure",
    "module": "autorag.nodes.passagereranker.cohere",
    "params": [
      {
        "name": "cohere_client",
        "param_type": "<class 'cohere.client.AsyncClient'>",
        "description": "The Cohere AsyncClient to use for reranking"
      },
      {
        "name": "model",
        "param_type": "<class 'str'>",
        "description": "The model name for Cohere rerank"
      },
      {
        "name": "query",
        "param_type": "<class 'str'>",
        "description": "The query to use for reranking"
      },
      {
        "name": "documents",
        "param_type": "typing.List[str]",
        "description": "The list of contents to rerank"
      },
      {
        "name": "ids",
        "param_type": "typing.List[str]",
        "description": "The list of ids corresponding to the documents"
      },
      {
        "name": "top_k",
        "param_type": "<class 'int'>",
        "description": "The number of passages to be retrieved"
      }
    ],
    "return_type": "typing.Tuple[typing.List[str], typing.List[str], typing.List[float]]",
    "docstring": "Rerank a list of contents with Cohere rerank models.\n\n:param cohere_client: The Cohere AsyncClient to use for reranking\n:param model: The model name for Cohere rerank\n:param query: The query to use for reranking\n:param documents: The list of contents to rerank\n:param ids: The list of ids corresponding to the documents\n:param top_k: The number of passages to be retrieved\n:return: Tuple of lists containing the reranked contents, ids, and scores"
  },
  {
    "function": "cohere_reranker",
    "module": "autorag.nodes.passagereranker.cohere",
    "params": [
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": "The list of queries to use for reranking"
      },
      {
        "name": "contents_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of contents to rerank"
      },
      {
        "name": "scores_list",
        "param_type": "typing.List[typing.List[float]]",
        "description": "The list of lists of scores retrieved from the initial ranking"
      },
      {
        "name": "ids_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of ids retrieved from the initial ranking"
      },
      {
        "name": "top_k",
        "param_type": "<class 'int'>",
        "description": "The number of passages to be retrieved"
      },
      {
        "name": "batch",
        "param_type": "<class 'int'>",
        "description": "The number of queries to be processed in a batch"
      },
      {
        "name": "model",
        "param_type": "<class 'str'>",
        "description": "The model name for Cohere rerank.\n    You can choose between \"rerank-multilingual-v2.0\" and \"rerank-english-v2.0\".\n    Default is \"rerank-multilingual-v2.0\"."
      },
      {
        "name": "api_key",
        "param_type": "typing.Optional[str]",
        "description": "The API key for Cohere rerank.\n    You can set it in the environment variable COHERE_API_KEY.\n    Or, you can directly set it on the config YAML file using this parameter.\n    Default is env variable \"COHERE_API_KEY\"."
      }
    ],
    "return_type": "typing.Tuple[typing.List[typing.List[str]], typing.List[typing.List[str]], typing.List[typing.List[float]]]",
    "docstring": "Rerank a list of contents with Cohere rerank models.\nYou can get the API key from https://cohere.com/rerank and set it in the environment variable COHERE_API_KEY.\n\n:param queries: The list of queries to use for reranking\n:param contents_list: The list of lists of contents to rerank\n:param scores_list: The list of lists of scores retrieved from the initial ranking\n:param ids_list: The list of lists of ids retrieved from the initial ranking\n:param top_k: The number of passages to be retrieved\n:param batch: The number of queries to be processed in a batch\n:param model: The model name for Cohere rerank.\n    You can choose between \"rerank-multilingual-v2.0\" and \"rerank-english-v2.0\".\n    Default is \"rerank-multilingual-v2.0\".\n:param api_key: The API key for Cohere rerank.\n    You can set it in the environment variable COHERE_API_KEY.\n    Or, you can directly set it on the config YAML file using this parameter.\n    Default is env variable \"COHERE_API_KEY\".\n:return: Tuple of lists containing the reranked contents, ids, and scores"
  },
  {
    "function": "passage_reranker_node",
    "module": "autorag.nodes.passagereranker.base",
    "params": [
      {
        "name": "func",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "process_batch",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "tasks",
        "param_type": "Any",
        "description": "A list of no-argument functions or coroutines to be executed."
      },
      {
        "name": "batch_size",
        "param_type": "<class 'int'>",
        "description": "The number of tasks to process in a single batch.\n    Default is 64."
      }
    ],
    "return_type": "typing.List[typing.Any]",
    "docstring": "Processes tasks in batches asynchronously.\n\n:param tasks: A list of no-argument functions or coroutines to be executed.\n:param batch_size: The number of tasks to process in a single batch.\n    Default is 64.\n:return: A list of results from the processed tasks."
  },
  {
    "function": "colbert_reranker",
    "module": "autorag.nodes.passagereranker.colbert",
    "params": [
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": "The list of queries to use for reranking"
      },
      {
        "name": "contents_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of contents to rerank"
      },
      {
        "name": "scores_list",
        "param_type": "typing.List[typing.List[float]]",
        "description": "The list of lists of scores retrieved from the initial ranking"
      },
      {
        "name": "ids_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of ids retrieved from the initial ranking"
      },
      {
        "name": "top_k",
        "param_type": "<class 'int'>",
        "description": "The number of passages to be retrieved"
      },
      {
        "name": "batch",
        "param_type": "<class 'int'>",
        "description": "The number of queries to be processed in a batch\n    Default is 64."
      },
      {
        "name": "model_name",
        "param_type": "<class 'str'>",
        "description": "The model name for Colbert rerank.\n    You can choose colbert model for reranking.\n    Default is \"colbert-ir/colbertv2.0\"."
      }
    ],
    "return_type": "typing.Tuple[typing.List[typing.List[str]], typing.List[typing.List[str]], typing.List[typing.List[float]]]",
    "docstring": "Rerank a list of contents with Colbert rerank models.\nYou can get more information about a Colbert model at https://huggingface.co/colbert-ir/colbertv2.0.\nIt uses BERT-based model, so recommend using CUDA gpu for faster reranking.\n\n:param queries: The list of queries to use for reranking\n:param contents_list: The list of lists of contents to rerank\n:param scores_list: The list of lists of scores retrieved from the initial ranking\n:param ids_list: The list of lists of ids retrieved from the initial ranking\n:param top_k: The number of passages to be retrieved\n:param batch: The number of queries to be processed in a batch\n    Default is 64.\n:param model_name: The model name for Colbert rerank.\n    You can choose colbert model for reranking.\n    Default is \"colbert-ir/colbertv2.0\".\n:return: Tuple of lists containing the reranked contents, ids, and scores"
  },
  {
    "function": "flatten_apply",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "func",
        "param_type": "typing.Callable",
        "description": "The function that applies to the flattened list."
      },
      {
        "name": "nested_list",
        "param_type": "typing.List[typing.List[typing.Any]]",
        "description": "The nested list to be flattened."
      },
      {
        "name": "kwargs",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "typing.List[typing.List[typing.Any]]",
    "docstring": "This function flattens the input list and applies the function to the elements.\nAfter that, it reconstructs the list to the original shape.\nIts speciality is that the first dimension length of the list can be different from each other.\n\n:param func: The function that applies to the flattened list.\n:param nested_list: The nested list to be flattened.\n:return: The list that is reconstructed after applying the function."
  },
  {
    "function": "get_colbert_embedding_batch",
    "module": "autorag.nodes.passagereranker.colbert",
    "params": [
      {
        "name": "input_strings",
        "param_type": "typing.List[str]",
        "description": ""
      },
      {
        "name": "model",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "tokenizer",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "batch_size",
        "param_type": "<class 'int'>",
        "description": ""
      }
    ],
    "return_type": "typing.List[<built-in function array>]",
    "docstring": null
  },
  {
    "function": "get_colbert_score",
    "module": "autorag.nodes.passagereranker.colbert",
    "params": [
      {
        "name": "query_embedding",
        "param_type": "<built-in function array>",
        "description": ""
      },
      {
        "name": "content_embedding",
        "param_type": "<built-in function array>",
        "description": ""
      }
    ],
    "return_type": "<class 'float'>",
    "docstring": null
  },
  {
    "function": "passage_reranker_node",
    "module": "autorag.nodes.passagereranker.base",
    "params": [
      {
        "name": "func",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "select_top_k",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "df",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "column_names",
        "param_type": "typing.List[str]",
        "description": ""
      },
      {
        "name": "top_k",
        "param_type": "<class 'int'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "slice_tensor",
    "module": "autorag.nodes.passagereranker.colbert",
    "params": [
      {
        "name": "input_tensor",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "batch_size",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "slice_tokenizer_result",
    "module": "autorag.nodes.passagereranker.colbert",
    "params": [
      {
        "name": "tokenizer_output",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "batch_size",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "sort_by_scores",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "row",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "reverse",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Sorts each row by 'scores' column.\nThe input column names must be 'contents', 'ids', and 'scores'.\nAnd its elements must be list type."
  },
  {
    "function": "flag_embedding_reranker",
    "module": "autorag.nodes.passagereranker.flag_embedding",
    "params": [
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": "The list of queries to use for reranking"
      },
      {
        "name": "contents_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of contents to rerank"
      },
      {
        "name": "scores_list",
        "param_type": "typing.List[typing.List[float]]",
        "description": "The list of lists of scores retrieved from the initial ranking"
      },
      {
        "name": "ids_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of ids retrieved from the initial ranking"
      },
      {
        "name": "top_k",
        "param_type": "<class 'int'>",
        "description": "The number of passages to be retrieved"
      },
      {
        "name": "batch",
        "param_type": "<class 'int'>",
        "description": "The number of queries to be processed in a batch\n    Default is 64."
      },
      {
        "name": "use_fp16",
        "param_type": "<class 'bool'>",
        "description": "Whether to use fp16 for inference"
      },
      {
        "name": "model_name",
        "param_type": "<class 'str'>",
        "description": "The name of the BAAI Reranker normal-model name.\n    Default is \"BAAI/bge-reranker-large\""
      }
    ],
    "return_type": "typing.Tuple[typing.List[typing.List[str]], typing.List[typing.List[str]], typing.List[typing.List[float]]]",
    "docstring": "Rerank a list of contents based on their relevance to a query using BAAI normal-Reranker model.\n\n:param queries: The list of queries to use for reranking\n:param contents_list: The list of lists of contents to rerank\n:param scores_list: The list of lists of scores retrieved from the initial ranking\n:param ids_list: The list of lists of ids retrieved from the initial ranking\n:param top_k: The number of passages to be retrieved\n:param batch: The number of queries to be processed in a batch\n    Default is 64.\n:param use_fp16: Whether to use fp16 for inference\n:param model_name: The name of the BAAI Reranker normal-model name.\n    Default is \"BAAI/bge-reranker-large\"\n:return: tuple of lists containing the reranked contents, ids, and scores"
  },
  {
    "function": "flag_embedding_run_model",
    "module": "autorag.nodes.passagereranker.flag_embedding",
    "params": [
      {
        "name": "input_texts",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "model",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "batch_size",
        "param_type": "<class 'int'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "flatten_apply",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "func",
        "param_type": "typing.Callable",
        "description": "The function that applies to the flattened list."
      },
      {
        "name": "nested_list",
        "param_type": "typing.List[typing.List[typing.Any]]",
        "description": "The nested list to be flattened."
      },
      {
        "name": "kwargs",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "typing.List[typing.List[typing.Any]]",
    "docstring": "This function flattens the input list and applies the function to the elements.\nAfter that, it reconstructs the list to the original shape.\nIts speciality is that the first dimension length of the list can be different from each other.\n\n:param func: The function that applies to the flattened list.\n:param nested_list: The nested list to be flattened.\n:return: The list that is reconstructed after applying the function."
  },
  {
    "function": "make_batch",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "elems",
        "param_type": "typing.List[typing.Any]",
        "description": ""
      },
      {
        "name": "batch_size",
        "param_type": "<class 'int'>",
        "description": ""
      }
    ],
    "return_type": "typing.List[typing.List[typing.Any]]",
    "docstring": "Make a batch of elems with batch_size."
  },
  {
    "function": "passage_reranker_node",
    "module": "autorag.nodes.passagereranker.base",
    "params": [
      {
        "name": "func",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "select_top_k",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "df",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "column_names",
        "param_type": "typing.List[str]",
        "description": ""
      },
      {
        "name": "top_k",
        "param_type": "<class 'int'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "sort_by_scores",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "row",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "reverse",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Sorts each row by 'scores' column.\nThe input column names must be 'contents', 'ids', and 'scores'.\nAnd its elements must be list type."
  },
  {
    "function": "flag_embedding_llm_reranker",
    "module": "autorag.nodes.passagereranker.flag_embedding_llm",
    "params": [
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": "The list of queries to use for reranking"
      },
      {
        "name": "contents_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of contents to rerank"
      },
      {
        "name": "scores_list",
        "param_type": "typing.List[typing.List[float]]",
        "description": "The list of lists of scores retrieved from the initial ranking"
      },
      {
        "name": "ids_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of ids retrieved from the initial ranking"
      },
      {
        "name": "top_k",
        "param_type": "<class 'int'>",
        "description": "The number of passages to be retrieved"
      },
      {
        "name": "batch",
        "param_type": "<class 'int'>",
        "description": "The number of queries to be processed in a batch\n    Default is 64."
      },
      {
        "name": "use_fp16",
        "param_type": "<class 'bool'>",
        "description": "Whether to use fp16 for inference"
      },
      {
        "name": "model_name",
        "param_type": "<class 'str'>",
        "description": "The name of the BAAI Reranker LLM-based-model name.\n    Default is \"BAAI/bge-reranker-v2-gemma\""
      }
    ],
    "return_type": "typing.Tuple[typing.List[typing.List[str]], typing.List[typing.List[str]], typing.List[typing.List[float]]]",
    "docstring": "Rerank a list of contents based on their relevance to a query using BAAI LLM-based-Reranker model.\n\n:param queries: The list of queries to use for reranking\n:param contents_list: The list of lists of contents to rerank\n:param scores_list: The list of lists of scores retrieved from the initial ranking\n:param ids_list: The list of lists of ids retrieved from the initial ranking\n:param top_k: The number of passages to be retrieved\n:param batch: The number of queries to be processed in a batch\n    Default is 64.\n:param use_fp16: Whether to use fp16 for inference\n:param model_name: The name of the BAAI Reranker LLM-based-model name.\n    Default is \"BAAI/bge-reranker-v2-gemma\"\n:return: tuple of lists containing the reranked contents, ids, and scores"
  },
  {
    "function": "flag_embedding_run_model",
    "module": "autorag.nodes.passagereranker.flag_embedding",
    "params": [
      {
        "name": "input_texts",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "model",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "batch_size",
        "param_type": "<class 'int'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "flatten_apply",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "func",
        "param_type": "typing.Callable",
        "description": "The function that applies to the flattened list."
      },
      {
        "name": "nested_list",
        "param_type": "typing.List[typing.List[typing.Any]]",
        "description": "The nested list to be flattened."
      },
      {
        "name": "kwargs",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "typing.List[typing.List[typing.Any]]",
    "docstring": "This function flattens the input list and applies the function to the elements.\nAfter that, it reconstructs the list to the original shape.\nIts speciality is that the first dimension length of the list can be different from each other.\n\n:param func: The function that applies to the flattened list.\n:param nested_list: The nested list to be flattened.\n:return: The list that is reconstructed after applying the function."
  },
  {
    "function": "passage_reranker_node",
    "module": "autorag.nodes.passagereranker.base",
    "params": [
      {
        "name": "func",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "select_top_k",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "df",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "column_names",
        "param_type": "typing.List[str]",
        "description": ""
      },
      {
        "name": "top_k",
        "param_type": "<class 'int'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "sort_by_scores",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "row",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "reverse",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Sorts each row by 'scores' column.\nThe input column names must be 'contents', 'ids', and 'scores'.\nAnd its elements must be list type."
  },
  {
    "function": "jina_reranker",
    "module": "autorag.nodes.passagereranker.jina",
    "params": [
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": "The list of queries to use for reranking"
      },
      {
        "name": "contents_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of contents to rerank"
      },
      {
        "name": "scores_list",
        "param_type": "typing.List[typing.List[float]]",
        "description": "The list of lists of scores retrieved from the initial ranking"
      },
      {
        "name": "ids_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of ids retrieved from the initial ranking"
      },
      {
        "name": "top_k",
        "param_type": "<class 'int'>",
        "description": "The number of passages to be retrieved"
      },
      {
        "name": "api_key",
        "param_type": "typing.Optional[str]",
        "description": "The API key for Jina rerank.\n    You can set it in the environment variable JINAAI_API_KEY.\n    Or, you can directly set it on the config YAML file using this parameter.\n    Default is env variable \"JINAAI_API_KEY\"."
      },
      {
        "name": "model",
        "param_type": "<class 'str'>",
        "description": "The model name for Cohere rerank.\n    You can choose between \"jina-reranker-v1-base-en\" and \"jina-colbert-v1-en\".\n    Default is \"jina-reranker-v1-base-en\"."
      },
      {
        "name": "batch",
        "param_type": "<class 'int'>",
        "description": "The number of queries to be processed in a batch"
      }
    ],
    "return_type": "typing.Tuple[typing.List[typing.List[str]], typing.List[typing.List[str]], typing.List[typing.List[float]]]",
    "docstring": "Rerank a list of contents with Jina rerank models.\nYou can get the API key from https://jina.ai/reranker and set it in the environment variable JINAAI_API_KEY.\n\n:param queries: The list of queries to use for reranking\n:param contents_list: The list of lists of contents to rerank\n:param scores_list: The list of lists of scores retrieved from the initial ranking\n:param ids_list: The list of lists of ids retrieved from the initial ranking\n:param top_k: The number of passages to be retrieved\n:param api_key: The API key for Jina rerank.\n    You can set it in the environment variable JINAAI_API_KEY.\n    Or, you can directly set it on the config YAML file using this parameter.\n    Default is env variable \"JINAAI_API_KEY\".\n:param model: The model name for Cohere rerank.\n    You can choose between \"jina-reranker-v1-base-en\" and \"jina-colbert-v1-en\".\n    Default is \"jina-reranker-v1-base-en\".\n:param batch: The number of queries to be processed in a batch\n:return: Tuple of lists containing the reranked contents, ids, and scores"
  },
  {
    "function": "jina_reranker_pure",
    "module": "autorag.nodes.passagereranker.jina",
    "params": [
      {
        "name": "query",
        "param_type": "<class 'str'>",
        "description": ""
      },
      {
        "name": "contents",
        "param_type": "typing.List[str]",
        "description": ""
      },
      {
        "name": "scores",
        "param_type": "typing.List[float]",
        "description": ""
      },
      {
        "name": "ids",
        "param_type": "typing.List[str]",
        "description": ""
      },
      {
        "name": "top_k",
        "param_type": "<class 'int'>",
        "description": ""
      },
      {
        "name": "api_key",
        "param_type": "<class 'str'>",
        "description": ""
      },
      {
        "name": "model",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "typing.Tuple[typing.List[str], typing.List[str], typing.List[float]]",
    "docstring": null
  },
  {
    "function": "passage_reranker_node",
    "module": "autorag.nodes.passagereranker.base",
    "params": [
      {
        "name": "func",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "process_batch",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "tasks",
        "param_type": "Any",
        "description": "A list of no-argument functions or coroutines to be executed."
      },
      {
        "name": "batch_size",
        "param_type": "<class 'int'>",
        "description": "The number of tasks to process in a single batch.\n    Default is 64."
      }
    ],
    "return_type": "typing.List[typing.Any]",
    "docstring": "Processes tasks in batches asynchronously.\n\n:param tasks: A list of no-argument functions or coroutines to be executed.\n:param batch_size: The number of tasks to process in a single batch.\n    Default is 64.\n:return: A list of results from the processed tasks."
  },
  {
    "function": "exp_normalize",
    "module": "autorag.nodes.passagereranker.koreranker",
    "params": [
      {
        "name": "x",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "flatten_apply",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "func",
        "param_type": "typing.Callable",
        "description": "The function that applies to the flattened list."
      },
      {
        "name": "nested_list",
        "param_type": "typing.List[typing.List[typing.Any]]",
        "description": "The nested list to be flattened."
      },
      {
        "name": "kwargs",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "typing.List[typing.List[typing.Any]]",
    "docstring": "This function flattens the input list and applies the function to the elements.\nAfter that, it reconstructs the list to the original shape.\nIts speciality is that the first dimension length of the list can be different from each other.\n\n:param func: The function that applies to the flattened list.\n:param nested_list: The nested list to be flattened.\n:return: The list that is reconstructed after applying the function."
  },
  {
    "function": "koreranker",
    "module": "autorag.nodes.passagereranker.koreranker",
    "params": [
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": "The list of queries to use for reranking"
      },
      {
        "name": "contents_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of contents to rerank"
      },
      {
        "name": "scores_list",
        "param_type": "typing.List[typing.List[float]]",
        "description": "The list of lists of scores retrieved from the initial ranking"
      },
      {
        "name": "ids_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of ids retrieved from the initial ranking"
      },
      {
        "name": "top_k",
        "param_type": "<class 'int'>",
        "description": "The number of passages to be retrieved"
      },
      {
        "name": "batch",
        "param_type": "<class 'int'>",
        "description": "The number of queries to be processed in a batch\n    Default is 64."
      }
    ],
    "return_type": "typing.Tuple[typing.List[typing.List[str]], typing.List[typing.List[str]], typing.List[typing.List[float]]]",
    "docstring": "Rerank a list of contents based on their relevance to a query using ko-reranker.\nko-reranker is a reranker based on korean (https://huggingface.co/Dongjin-kr/ko-reranker).\n\n:param queries: The list of queries to use for reranking\n:param contents_list: The list of lists of contents to rerank\n:param scores_list: The list of lists of scores retrieved from the initial ranking\n:param ids_list: The list of lists of ids retrieved from the initial ranking\n:param top_k: The number of passages to be retrieved\n:param batch: The number of queries to be processed in a batch\n    Default is 64.\n:return: tuple of lists containing the reranked contents, ids, and scores"
  },
  {
    "function": "koreranker_run_model",
    "module": "autorag.nodes.passagereranker.koreranker",
    "params": [
      {
        "name": "input_texts",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "model",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "tokenizer",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "device",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "batch_size",
        "param_type": "<class 'int'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "make_batch",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "elems",
        "param_type": "typing.List[typing.Any]",
        "description": ""
      },
      {
        "name": "batch_size",
        "param_type": "<class 'int'>",
        "description": ""
      }
    ],
    "return_type": "typing.List[typing.List[typing.Any]]",
    "docstring": "Make a batch of elems with batch_size."
  },
  {
    "function": "passage_reranker_node",
    "module": "autorag.nodes.passagereranker.base",
    "params": [
      {
        "name": "func",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "select_top_k",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "df",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "column_names",
        "param_type": "typing.List[str]",
        "description": ""
      },
      {
        "name": "top_k",
        "param_type": "<class 'int'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "sort_by_scores",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "row",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "reverse",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Sorts each row by 'scores' column.\nThe input column names must be 'contents', 'ids', and 'scores'.\nAnd its elements must be list type."
  },
  {
    "function": "flatten_apply",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "func",
        "param_type": "typing.Callable",
        "description": "The function that applies to the flattened list."
      },
      {
        "name": "nested_list",
        "param_type": "typing.List[typing.List[typing.Any]]",
        "description": "The nested list to be flattened."
      },
      {
        "name": "kwargs",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "typing.List[typing.List[typing.Any]]",
    "docstring": "This function flattens the input list and applies the function to the elements.\nAfter that, it reconstructs the list to the original shape.\nIts speciality is that the first dimension length of the list can be different from each other.\n\n:param func: The function that applies to the flattened list.\n:param nested_list: The nested list to be flattened.\n:return: The list that is reconstructed after applying the function."
  },
  {
    "function": "make_batch",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "elems",
        "param_type": "typing.List[typing.Any]",
        "description": ""
      },
      {
        "name": "batch_size",
        "param_type": "<class 'int'>",
        "description": ""
      }
    ],
    "return_type": "typing.List[typing.List[typing.Any]]",
    "docstring": "Make a batch of elems with batch_size."
  },
  {
    "function": "monot5",
    "module": "autorag.nodes.passagereranker.monot5",
    "params": [
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": "The list of queries to use for reranking"
      },
      {
        "name": "contents_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of contents to rerank"
      },
      {
        "name": "scores_list",
        "param_type": "typing.List[typing.List[float]]",
        "description": "The list of lists of scores retrieved from the initial ranking"
      },
      {
        "name": "ids_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of ids retrieved from the initial ranking"
      },
      {
        "name": "top_k",
        "param_type": "<class 'int'>",
        "description": "The number of passages to be retrieved"
      },
      {
        "name": "model_name",
        "param_type": "<class 'str'>",
        "description": "The name of the MonoT5 model to use for reranking\n    Note: default model name is 'castorini/monot5-3b-msmarco-10k'\n        If there is a '/' in the model name parameter,\n        when we create the file to store the results, the path will be twisted because of the '/'.\n        Therefore, it will be received as '_' instead of '/'."
      },
      {
        "name": "batch",
        "param_type": "<class 'int'>",
        "description": "The number of queries to be processed in a batch"
      }
    ],
    "return_type": "typing.Tuple[typing.List[typing.List[str]], typing.List[typing.List[str]], typing.List[typing.List[float]]]",
    "docstring": "Rerank a list of contents based on their relevance to a query using MonoT5.\n\n:param queries: The list of queries to use for reranking\n:param contents_list: The list of lists of contents to rerank\n:param scores_list: The list of lists of scores retrieved from the initial ranking\n:param ids_list: The list of lists of ids retrieved from the initial ranking\n:param top_k: The number of passages to be retrieved\n:param model_name: The name of the MonoT5 model to use for reranking\n    Note: default model name is 'castorini/monot5-3b-msmarco-10k'\n        If there is a '/' in the model name parameter,\n        when we create the file to store the results, the path will be twisted because of the '/'.\n        Therefore, it will be received as '_' instead of '/'.\n:param batch: The number of queries to be processed in a batch\n:return: tuple of lists containing the reranked contents, ids, and scores"
  },
  {
    "function": "monot5_run_model",
    "module": "autorag.nodes.passagereranker.monot5",
    "params": [
      {
        "name": "input_texts",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "model",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "batch_size",
        "param_type": "<class 'int'>",
        "description": ""
      },
      {
        "name": "tokenizer",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "device",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "token_false_id",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "token_true_id",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "passage_reranker_node",
    "module": "autorag.nodes.passagereranker.base",
    "params": [
      {
        "name": "func",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "select_top_k",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "df",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "column_names",
        "param_type": "typing.List[str]",
        "description": ""
      },
      {
        "name": "top_k",
        "param_type": "<class 'int'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "sort_by_scores",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "row",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "reverse",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Sorts each row by 'scores' column.\nThe input column names must be 'contents', 'ids', and 'scores'.\nAnd its elements must be list type."
  },
  {
    "function": "pass_reranker",
    "module": "autorag.nodes.passagereranker.pass_reranker",
    "params": [
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": ""
      },
      {
        "name": "contents_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": ""
      },
      {
        "name": "scores_list",
        "param_type": "typing.List[typing.List[float]]",
        "description": ""
      },
      {
        "name": "ids_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": ""
      },
      {
        "name": "top_k",
        "param_type": "<class 'int'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Do not perform reranking.\nReturn the given top-k passages as is."
  },
  {
    "function": "passage_reranker_node",
    "module": "autorag.nodes.passagereranker.base",
    "params": [
      {
        "name": "func",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "AsyncRankGPTRerank.async_postprocess_nodes",
    "module": "autorag.nodes.passagereranker.rankgpt",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "nodes",
        "param_type": "typing.List[llama_index.core.schema.NodeWithScore]",
        "description": ""
      },
      {
        "name": "query_bundle",
        "param_type": "<class 'llama_index.core.schema.QueryBundle'>",
        "description": ""
      },
      {
        "name": "ids",
        "param_type": "typing.Optional[typing.List[str]]",
        "description": ""
      }
    ],
    "return_type": "typing.Tuple[typing.List[llama_index.core.schema.NodeWithScore], typing.List[str]]",
    "docstring": null
  },
  {
    "function": "AsyncRankGPTRerank.async_run_llm",
    "module": "autorag.nodes.passagereranker.rankgpt",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "messages",
        "param_type": "typing.Sequence[llama_index.core.base.llms.types.ChatMessage]",
        "description": ""
      }
    ],
    "return_type": "<class 'llama_index.core.base.llms.types.ChatResponse'>",
    "docstring": null
  },
  {
    "function": "passage_reranker_node",
    "module": "autorag.nodes.passagereranker.base",
    "params": [
      {
        "name": "func",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "process_batch",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "tasks",
        "param_type": "Any",
        "description": "A list of no-argument functions or coroutines to be executed."
      },
      {
        "name": "batch_size",
        "param_type": "<class 'int'>",
        "description": "The number of tasks to process in a single batch.\n    Default is 64."
      }
    ],
    "return_type": "typing.List[typing.Any]",
    "docstring": "Processes tasks in batches asynchronously.\n\n:param tasks: A list of no-argument functions or coroutines to be executed.\n:param batch_size: The number of tasks to process in a single batch.\n    Default is 64.\n:return: A list of results from the processed tasks."
  },
  {
    "function": "rankgpt",
    "module": "autorag.nodes.passagereranker.rankgpt",
    "params": [
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": "The list of queries to use for reranking"
      },
      {
        "name": "contents_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of contents to rerank"
      },
      {
        "name": "scores_list",
        "param_type": "typing.List[typing.List[float]]",
        "description": "The list of lists of scores retrieved from the initial ranking"
      },
      {
        "name": "ids_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of ids retrieved from the initial ranking"
      },
      {
        "name": "top_k",
        "param_type": "<class 'int'>",
        "description": "The number of passages to be retrieved"
      },
      {
        "name": "llm",
        "param_type": "typing.Optional[llama_index.core.llms.llm.LLM]",
        "description": "The LLM model to use for RankGPT rerank.\n    It is LlamaIndex model.\n    Default is OpenAI model with gpt-3.5-turbo-16k."
      },
      {
        "name": "verbose",
        "param_type": "<class 'bool'>",
        "description": "Whether to print intermediate steps."
      },
      {
        "name": "rankgpt_rerank_prompt",
        "param_type": "typing.Optional[str]",
        "description": "The prompt template for RankGPT rerank.\n    Default is RankGPT's default prompt."
      },
      {
        "name": "batch",
        "param_type": "<class 'int'>",
        "description": "The number of queries to be processed in a batch."
      },
      {
        "name": "kwargs",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "typing.Tuple[typing.List[typing.List[str]], typing.List[typing.List[str]], typing.List[typing.List[float]]]",
    "docstring": "Rerank given context paragraphs using RankGPT.\nReturn pseudo scores, since the actual scores are not available on RankGPT.\n\n:param queries: The list of queries to use for reranking\n:param contents_list: The list of lists of contents to rerank\n:param scores_list: The list of lists of scores retrieved from the initial ranking\n:param ids_list: The list of lists of ids retrieved from the initial ranking\n:param top_k: The number of passages to be retrieved\n:param llm: The LLM model to use for RankGPT rerank.\n    It is LlamaIndex model.\n    Default is OpenAI model with gpt-3.5-turbo-16k.\n:param verbose: Whether to print intermediate steps.\n:param rankgpt_rerank_prompt: The prompt template for RankGPT rerank.\n    Default is RankGPT's default prompt.\n:param batch: The number of queries to be processed in a batch.\n:return: Tuple of lists containing the reranked contents, ids, and scores"
  },
  {
    "function": "evaluate_retrieval_node",
    "module": "autorag.nodes.retrieval.run",
    "params": [
      {
        "name": "result_df",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": "The result dataframe from a retrieval node."
      },
      {
        "name": "retrieval_gt",
        "param_type": "Any",
        "description": "Ground truth for retrieval from qa dataset."
      },
      {
        "name": "metrics",
        "param_type": "Any",
        "description": "Metric list from input strategies."
      },
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": "Query list from input strategies."
      },
      {
        "name": "generation_gt",
        "param_type": "typing.List[typing.List[str]]",
        "description": "Ground truth for generation from qa dataset."
      }
    ],
    "return_type": "<class 'pandas.core.frame.DataFrame'>",
    "docstring": "Evaluate retrieval node from retrieval node result dataframe.\n\n:param result_df: The result dataframe from a retrieval node.\n:param retrieval_gt: Ground truth for retrieval from qa dataset.\n:param metrics: Metric list from input strategies.\n:param queries: Query list from input strategies.\n:param generation_gt: Ground truth for generation from qa dataset.\n:return: Return result_df with metrics columns.\n    The columns will be 'retrieved_contents', 'retrieved_ids', 'retrieve_scores', and metric names."
  },
  {
    "function": "filter_by_threshold",
    "module": "autorag.strategy",
    "params": [
      {
        "name": "results",
        "param_type": "Any",
        "description": "The result list to be filtered."
      },
      {
        "name": "value",
        "param_type": "Any",
        "description": "The value list to be filtered.\n    It must have the same length with results."
      },
      {
        "name": "threshold",
        "param_type": "Any",
        "description": "The threshold value."
      },
      {
        "name": "metadatas",
        "param_type": "Any",
        "description": "The metadata of each result."
      }
    ],
    "return_type": "typing.Tuple[typing.List, typing.List]",
    "docstring": "Filter results by value's threshold.\n\n:param results: The result list to be filtered.\n:param value: The value list to be filtered.\n    It must have the same length with results.\n:param threshold: The threshold value.\n:param metadatas: The metadata of each result.\n:return: Filtered list of results and filtered list of metadatas.\n    Metadatas will be returned even if you did not give input metadatas.\n:rtype: Tuple[List, List]"
  },
  {
    "function": "measure_speed",
    "module": "autorag.strategy",
    "params": [
      {
        "name": "func",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "args",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "kwargs",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Method for measuring execution speed of the function."
  },
  {
    "function": "run_passage_reranker_node",
    "module": "autorag.nodes.passagereranker.run",
    "params": [
      {
        "name": "modules",
        "param_type": "typing.List[typing.Callable]",
        "description": "Passage reranker modules to run."
      },
      {
        "name": "module_params",
        "param_type": "typing.List[typing.Dict]",
        "description": "Passage reranker module parameters."
      },
      {
        "name": "previous_result",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": "Previous result dataframe.\n    Could be retrieval, reranker modules result.\n    It means it must contain 'query', 'retrieved_contents', 'retrieved_ids', 'retrieve_scores' columns."
      },
      {
        "name": "node_line_dir",
        "param_type": "<class 'str'>",
        "description": "This node line's directory."
      },
      {
        "name": "strategies",
        "param_type": "typing.Dict",
        "description": "Strategies for passage reranker node.\n    In this node, we use 'retrieval_f1', 'retrieval_recall' and 'retrieval_precision'.\n    You can skip evaluation when you use only one module and a module parameter."
      }
    ],
    "return_type": "<class 'pandas.core.frame.DataFrame'>",
    "docstring": "Run evaluation and select the best module among passage reranker node results.\n\n:param modules: Passage reranker modules to run.\n:param module_params: Passage reranker module parameters.\n:param previous_result: Previous result dataframe.\n    Could be retrieval, reranker modules result.\n    It means it must contain 'query', 'retrieved_contents', 'retrieved_ids', 'retrieve_scores' columns.\n:param node_line_dir: This node line's directory.\n:param strategies: Strategies for passage reranker node.\n    In this node, we use 'retrieval_f1', 'retrieval_recall' and 'retrieval_precision'.\n    You can skip evaluation when you use only one module and a module parameter.\n:return: The best result dataframe with previous result columns."
  },
  {
    "function": "select_best",
    "module": "autorag.strategy",
    "params": [
      {
        "name": "results",
        "param_type": "typing.List[pandas.core.frame.DataFrame]",
        "description": ""
      },
      {
        "name": "columns",
        "param_type": "typing.Iterable[str]",
        "description": ""
      },
      {
        "name": "metadatas",
        "param_type": "typing.Optional[typing.List[typing.Any]]",
        "description": ""
      },
      {
        "name": "strategy_name",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "typing.Tuple[pandas.core.frame.DataFrame, typing.Any]",
    "docstring": null
  },
  {
    "function": "flatten_apply",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "func",
        "param_type": "typing.Callable",
        "description": "The function that applies to the flattened list."
      },
      {
        "name": "nested_list",
        "param_type": "typing.List[typing.List[typing.Any]]",
        "description": "The nested list to be flattened."
      },
      {
        "name": "kwargs",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "typing.List[typing.List[typing.Any]]",
    "docstring": "This function flattens the input list and applies the function to the elements.\nAfter that, it reconstructs the list to the original shape.\nIts speciality is that the first dimension length of the list can be different from each other.\n\n:param func: The function that applies to the flattened list.\n:param nested_list: The nested list to be flattened.\n:return: The list that is reconstructed after applying the function."
  },
  {
    "function": "make_batch",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "elems",
        "param_type": "typing.List[typing.Any]",
        "description": ""
      },
      {
        "name": "batch_size",
        "param_type": "<class 'int'>",
        "description": ""
      }
    ],
    "return_type": "typing.List[typing.List[typing.Any]]",
    "docstring": "Make a batch of elems with batch_size."
  },
  {
    "function": "passage_reranker_node",
    "module": "autorag.nodes.passagereranker.base",
    "params": [
      {
        "name": "func",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "select_top_k",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "df",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "column_names",
        "param_type": "typing.List[str]",
        "description": ""
      },
      {
        "name": "top_k",
        "param_type": "<class 'int'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "sentence_transformer_reranker",
    "module": "autorag.nodes.passagereranker.sentence_transformer",
    "params": [
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": "The list of queries to use for reranking"
      },
      {
        "name": "contents_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of contents to rerank"
      },
      {
        "name": "scores_list",
        "param_type": "typing.List[typing.List[float]]",
        "description": "The list of lists of scores retrieved from the initial ranking"
      },
      {
        "name": "ids_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of ids retrieved from the initial ranking"
      },
      {
        "name": "top_k",
        "param_type": "<class 'int'>",
        "description": "The number of passages to be retrieved"
      },
      {
        "name": "batch",
        "param_type": "<class 'int'>",
        "description": "The number of queries to be processed in a batch"
      },
      {
        "name": "sentence_transformer_max_length",
        "param_type": "<class 'int'>",
        "description": "The maximum length of the input text for the Sentence Transformer model"
      },
      {
        "name": "model_name",
        "param_type": "<class 'str'>",
        "description": "The name of the Sentence Transformer model to use for reranking\n    Default is \"cross-encoder/ms-marco-MiniLM-L-2-v2\""
      }
    ],
    "return_type": "typing.Tuple[typing.List[typing.List[str]], typing.List[typing.List[str]], typing.List[typing.List[float]]]",
    "docstring": "Rerank a list of contents based on their relevance to a query using Sentence Transformer model.\n\n:param queries: The list of queries to use for reranking\n:param contents_list: The list of lists of contents to rerank\n:param scores_list: The list of lists of scores retrieved from the initial ranking\n:param ids_list: The list of lists of ids retrieved from the initial ranking\n:param top_k: The number of passages to be retrieved\n:param batch: The number of queries to be processed in a batch\n:param sentence_transformer_max_length: The maximum length of the input text for the Sentence Transformer model\n:param model_name: The name of the Sentence Transformer model to use for reranking\n    Default is \"cross-encoder/ms-marco-MiniLM-L-2-v2\"\n:return: tuple of lists containing the reranked contents, ids, and scores"
  },
  {
    "function": "sentence_transformer_run_model",
    "module": "autorag.nodes.passagereranker.sentence_transformer",
    "params": [
      {
        "name": "input_texts",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "model",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "batch_size",
        "param_type": "<class 'int'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "sort_by_scores",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "row",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "reverse",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Sorts each row by 'scores' column.\nThe input column names must be 'contents', 'ids', and 'scores'.\nAnd its elements must be list type."
  },
  {
    "function": "EncT5ForSequenceClassification.__init__",
    "module": "autorag.nodes.passagereranker.tart.modeling_enc_t5",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "config",
        "param_type": "<class 'transformers.models.t5.configuration_t5.T5Config'>",
        "description": ""
      },
      {
        "name": "dropout",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Initialize internal Module state, shared by both nn.Module and ScriptModule."
  },
  {
    "function": "EncT5ForSequenceClassification._prune_heads",
    "module": "autorag.nodes.passagereranker.tart.modeling_enc_t5",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "heads_to_prune",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\nclass PreTrainedModel"
  },
  {
    "function": "EncT5ForSequenceClassification.deparallelize",
    "module": "autorag.nodes.passagereranker.tart.modeling_enc_t5",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "EncT5ForSequenceClassification.forward",
    "module": "autorag.nodes.passagereranker.tart.modeling_enc_t5",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "input_ids",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "attention_mask",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "head_mask",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "inputs_embeds",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "labels",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "output_attentions",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "output_hidden_states",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "return_dict",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Define the computation performed at every call.\n\nShould be overridden by all subclasses.\n\n.. note::\n    Although the recipe for forward pass needs to be defined within\n    this function, one should call the :class:`Module` instance afterwards\n    instead of this since the former takes care of running the\n    registered hooks while the latter silently ignores them."
  },
  {
    "function": "EncT5ForSequenceClassification.get_encoder",
    "module": "autorag.nodes.passagereranker.tart.modeling_enc_t5",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "EncT5ForSequenceClassification.get_input_embeddings",
    "module": "autorag.nodes.passagereranker.tart.modeling_enc_t5",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Returns the model's input embeddings.\n\nReturns:\n    `nn.Module`: A torch module mapping vocabulary to hidden states."
  },
  {
    "function": "EncT5ForSequenceClassification.parallelize",
    "module": "autorag.nodes.passagereranker.tart.modeling_enc_t5",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "device_map",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "EncT5ForSequenceClassification.set_input_embeddings",
    "module": "autorag.nodes.passagereranker.tart.modeling_enc_t5",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "new_embeddings",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Set model's input embeddings.\n\nArgs:\n    value (`nn.Module`): A module mapping vocabulary to hidden states."
  },
  {
    "function": "EncT5ForSequenceClassification.__init__",
    "module": "autorag.nodes.passagereranker.tart.modeling_enc_t5",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "config",
        "param_type": "<class 'transformers.models.t5.configuration_t5.T5Config'>",
        "description": ""
      },
      {
        "name": "dropout",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Initialize internal Module state, shared by both nn.Module and ScriptModule."
  },
  {
    "function": "EncT5ForSequenceClassification._prune_heads",
    "module": "autorag.nodes.passagereranker.tart.modeling_enc_t5",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "heads_to_prune",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\nclass PreTrainedModel"
  },
  {
    "function": "EncT5ForSequenceClassification.deparallelize",
    "module": "autorag.nodes.passagereranker.tart.modeling_enc_t5",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "EncT5ForSequenceClassification.forward",
    "module": "autorag.nodes.passagereranker.tart.modeling_enc_t5",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "input_ids",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "attention_mask",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "head_mask",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "inputs_embeds",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "labels",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "output_attentions",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "output_hidden_states",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "return_dict",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Define the computation performed at every call.\n\nShould be overridden by all subclasses.\n\n.. note::\n    Although the recipe for forward pass needs to be defined within\n    this function, one should call the :class:`Module` instance afterwards\n    instead of this since the former takes care of running the\n    registered hooks while the latter silently ignores them."
  },
  {
    "function": "EncT5ForSequenceClassification.get_encoder",
    "module": "autorag.nodes.passagereranker.tart.modeling_enc_t5",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "EncT5ForSequenceClassification.get_input_embeddings",
    "module": "autorag.nodes.passagereranker.tart.modeling_enc_t5",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Returns the model's input embeddings.\n\nReturns:\n    `nn.Module`: A torch module mapping vocabulary to hidden states."
  },
  {
    "function": "EncT5ForSequenceClassification.parallelize",
    "module": "autorag.nodes.passagereranker.tart.modeling_enc_t5",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "device_map",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "EncT5ForSequenceClassification.set_input_embeddings",
    "module": "autorag.nodes.passagereranker.tart.modeling_enc_t5",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "new_embeddings",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Set model's input embeddings.\n\nArgs:\n    value (`nn.Module`): A module mapping vocabulary to hidden states."
  },
  {
    "function": "EncT5Tokenizer.__init__",
    "module": "autorag.nodes.passagereranker.tart.tokenization_enc_t5",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "vocab_file",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "bos_token",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "eos_token",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "unk_token",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "pad_token",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "extra_ids",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "additional_special_tokens",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "sp_model_kwargs",
        "param_type": "typing.Optional[typing.Dict[str, typing.Any]]",
        "description": ""
      },
      {
        "name": "kwargs",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "None",
    "docstring": "Initialize self.  See help(type(self)) for accurate signature."
  },
  {
    "function": "EncT5Tokenizer.build_inputs_with_special_tokens",
    "module": "autorag.nodes.passagereranker.tart.tokenization_enc_t5",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "token_ids_0",
        "param_type": "typing.List[int]",
        "description": ""
      },
      {
        "name": "token_ids_1",
        "param_type": "typing.Optional[typing.List[int]]",
        "description": ""
      }
    ],
    "return_type": "typing.List[int]",
    "docstring": "Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\nadding special tokens. A sequence has the following format:\n- single sequence: `<s> X </s>`\n- pair of sequences: `<s> A </s> B </s>`\nArgs:\n    token_ids_0 (`List[int]`):\n        List of IDs to which the special tokens will be added.\n    token_ids_1 (`List[int]`, *optional*):\n        Optional second list of IDs for sequence pairs.\nReturns:\n    `List[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens."
  },
  {
    "function": "EncT5Tokenizer.create_token_type_ids_from_sequences",
    "module": "autorag.nodes.passagereranker.tart.tokenization_enc_t5",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "token_ids_0",
        "param_type": "typing.List[int]",
        "description": ""
      },
      {
        "name": "token_ids_1",
        "param_type": "typing.Optional[typing.List[int]]",
        "description": ""
      }
    ],
    "return_type": "typing.List[int]",
    "docstring": "Create a mask from the two sequences passed to be used in a sequence-pair classification task. T5 does not make\nuse of token type ids, therefore a list of zeros is returned.\nArgs:\n    token_ids_0 (`List[int]`):\n        List of IDs.\n    token_ids_1 (`List[int]`, *optional*):\n        Optional second list of IDs for sequence pairs.\nReturns:\n    `List[int]`: List of zeros."
  },
  {
    "function": "EncT5Tokenizer.get_special_tokens_mask",
    "module": "autorag.nodes.passagereranker.tart.tokenization_enc_t5",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "token_ids_0",
        "param_type": "typing.List[int]",
        "description": ""
      },
      {
        "name": "token_ids_1",
        "param_type": "typing.Optional[typing.List[int]]",
        "description": ""
      },
      {
        "name": "already_has_special_tokens",
        "param_type": "<class 'bool'>",
        "description": ""
      }
    ],
    "return_type": "typing.List[int]",
    "docstring": "Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding\nspecial tokens using the tokenizer `prepare_for_model` method.\nArgs:\n    token_ids_0 (`List[int]`):\n        List of IDs.\n    token_ids_1 (`List[int]`, *optional*):\n        Optional second list of IDs for sequence pairs.\n    already_has_special_tokens (`bool`, *optional*, defaults to `False`):\n        Whether or not the token list is already formatted with special tokens for the model.\nReturns:\n    `List[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token."
  },
  {
    "function": "flatten_apply",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "func",
        "param_type": "typing.Callable",
        "description": "The function that applies to the flattened list."
      },
      {
        "name": "nested_list",
        "param_type": "typing.List[typing.List[typing.Any]]",
        "description": "The nested list to be flattened."
      },
      {
        "name": "kwargs",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "typing.List[typing.List[typing.Any]]",
    "docstring": "This function flattens the input list and applies the function to the elements.\nAfter that, it reconstructs the list to the original shape.\nIts speciality is that the first dimension length of the list can be different from each other.\n\n:param func: The function that applies to the flattened list.\n:param nested_list: The nested list to be flattened.\n:return: The list that is reconstructed after applying the function."
  },
  {
    "function": "make_batch",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "elems",
        "param_type": "typing.List[typing.Any]",
        "description": ""
      },
      {
        "name": "batch_size",
        "param_type": "<class 'int'>",
        "description": ""
      }
    ],
    "return_type": "typing.List[typing.List[typing.Any]]",
    "docstring": "Make a batch of elems with batch_size."
  },
  {
    "function": "passage_reranker_node",
    "module": "autorag.nodes.passagereranker.base",
    "params": [
      {
        "name": "func",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "select_top_k",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "df",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "column_names",
        "param_type": "typing.List[str]",
        "description": ""
      },
      {
        "name": "top_k",
        "param_type": "<class 'int'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "sort_by_scores",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "row",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "reverse",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Sorts each row by 'scores' column.\nThe input column names must be 'contents', 'ids', and 'scores'.\nAnd its elements must be list type."
  },
  {
    "function": "tart",
    "module": "autorag.nodes.passagereranker.tart.tart",
    "params": [
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": "The list of queries to use for reranking"
      },
      {
        "name": "contents_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of contents to rerank"
      },
      {
        "name": "scores_list",
        "param_type": "typing.List[typing.List[float]]",
        "description": "The list of lists of scores retrieved from the initial ranking"
      },
      {
        "name": "ids_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of ids retrieved from the initial ranking"
      },
      {
        "name": "top_k",
        "param_type": "<class 'int'>",
        "description": "The number of passages to be retrieved"
      },
      {
        "name": "instruction",
        "param_type": "<class 'str'>",
        "description": "The instruction for reranking.\n    Note: default instruction is \"Find passage to answer given question\"\n        The default instruction from the TART paper is being used.\n        If you want to use a different instruction, you can change the instruction through this parameter"
      },
      {
        "name": "batch",
        "param_type": "<class 'int'>",
        "description": "The number of queries to be processed in a batch"
      }
    ],
    "return_type": "typing.Tuple[typing.List[typing.List[str]], typing.List[typing.List[str]], typing.List[typing.List[float]]]",
    "docstring": "Rerank a list of contents based on their relevance to a query using Tart.\nTART is a reranker based on TART (https://github.com/facebookresearch/tart).\nYou can rerank the passages with the instruction using TARTReranker.\nThe default model is facebook/tart-full-flan-t5-xl.\n\n:param queries: The list of queries to use for reranking\n:param contents_list: The list of lists of contents to rerank\n:param scores_list: The list of lists of scores retrieved from the initial ranking\n:param ids_list: The list of lists of ids retrieved from the initial ranking\n:param top_k: The number of passages to be retrieved\n:param instruction: The instruction for reranking.\n    Note: default instruction is \"Find passage to answer given question\"\n        The default instruction from the TART paper is being used.\n        If you want to use a different instruction, you can change the instruction through this parameter\n:param batch: The number of queries to be processed in a batch\n:return: tuple of lists containing the reranked contents, ids, and scores"
  },
  {
    "function": "tart_run_model",
    "module": "autorag.nodes.passagereranker.tart.tart",
    "params": [
      {
        "name": "input_texts",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "contents_list",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "model",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "batch_size",
        "param_type": "<class 'int'>",
        "description": ""
      },
      {
        "name": "tokenizer",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "device",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "EncT5Tokenizer.__init__",
    "module": "autorag.nodes.passagereranker.tart.tokenization_enc_t5",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "vocab_file",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "bos_token",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "eos_token",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "unk_token",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "pad_token",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "extra_ids",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "additional_special_tokens",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "sp_model_kwargs",
        "param_type": "typing.Optional[typing.Dict[str, typing.Any]]",
        "description": ""
      },
      {
        "name": "kwargs",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "None",
    "docstring": "Initialize self.  See help(type(self)) for accurate signature."
  },
  {
    "function": "EncT5Tokenizer.build_inputs_with_special_tokens",
    "module": "autorag.nodes.passagereranker.tart.tokenization_enc_t5",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "token_ids_0",
        "param_type": "typing.List[int]",
        "description": ""
      },
      {
        "name": "token_ids_1",
        "param_type": "typing.Optional[typing.List[int]]",
        "description": ""
      }
    ],
    "return_type": "typing.List[int]",
    "docstring": "Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\nadding special tokens. A sequence has the following format:\n- single sequence: `<s> X </s>`\n- pair of sequences: `<s> A </s> B </s>`\nArgs:\n    token_ids_0 (`List[int]`):\n        List of IDs to which the special tokens will be added.\n    token_ids_1 (`List[int]`, *optional*):\n        Optional second list of IDs for sequence pairs.\nReturns:\n    `List[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens."
  },
  {
    "function": "EncT5Tokenizer.create_token_type_ids_from_sequences",
    "module": "autorag.nodes.passagereranker.tart.tokenization_enc_t5",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "token_ids_0",
        "param_type": "typing.List[int]",
        "description": ""
      },
      {
        "name": "token_ids_1",
        "param_type": "typing.Optional[typing.List[int]]",
        "description": ""
      }
    ],
    "return_type": "typing.List[int]",
    "docstring": "Create a mask from the two sequences passed to be used in a sequence-pair classification task. T5 does not make\nuse of token type ids, therefore a list of zeros is returned.\nArgs:\n    token_ids_0 (`List[int]`):\n        List of IDs.\n    token_ids_1 (`List[int]`, *optional*):\n        Optional second list of IDs for sequence pairs.\nReturns:\n    `List[int]`: List of zeros."
  },
  {
    "function": "EncT5Tokenizer.get_special_tokens_mask",
    "module": "autorag.nodes.passagereranker.tart.tokenization_enc_t5",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "token_ids_0",
        "param_type": "typing.List[int]",
        "description": ""
      },
      {
        "name": "token_ids_1",
        "param_type": "typing.Optional[typing.List[int]]",
        "description": ""
      },
      {
        "name": "already_has_special_tokens",
        "param_type": "<class 'bool'>",
        "description": ""
      }
    ],
    "return_type": "typing.List[int]",
    "docstring": "Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding\nspecial tokens using the tokenizer `prepare_for_model` method.\nArgs:\n    token_ids_0 (`List[int]`):\n        List of IDs.\n    token_ids_1 (`List[int]`, *optional*):\n        Optional second list of IDs for sequence pairs.\n    already_has_special_tokens (`bool`, *optional*, defaults to `False`):\n        Whether or not the token list is already formatted with special tokens for the model.\nReturns:\n    `List[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token."
  },
  {
    "function": "passage_reranker_node",
    "module": "autorag.nodes.passagereranker.base",
    "params": [
      {
        "name": "func",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "time_reranker",
    "module": "autorag.nodes.passagereranker.time_reranker",
    "params": [
      {
        "name": "contents_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of contents"
      },
      {
        "name": "scores_list",
        "param_type": "typing.List[typing.List[float]]",
        "description": "The list of lists of scores from the initial ranking"
      },
      {
        "name": "ids_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of ids"
      },
      {
        "name": "top_k",
        "param_type": "<class 'int'>",
        "description": "The number of passages to be retrieved after reranking"
      },
      {
        "name": "time_list",
        "param_type": "typing.List[typing.List[datetime.datetime]]",
        "description": "The metadata list of lists of datetime.datetime\n    It automatically extracts the 'last_modified_datetime' key from the metadata in the corpus data."
      }
    ],
    "return_type": "typing.Tuple[typing.List[typing.List[str]], typing.List[typing.List[str]], typing.List[typing.List[float]]]",
    "docstring": "Rerank the passages based on merely the datetime of the passage.\nIt uses 'last_modified_datetime' key in the corpus metadata,\nso the metadata should be in the format of {'last_modified_datetime': datetime.datetime} at the corpus data file.\n\n:param contents_list: The list of lists of contents\n:param scores_list: The list of lists of scores from the initial ranking\n:param ids_list: The list of lists of ids\n:param top_k: The number of passages to be retrieved after reranking\n:param time_list: The metadata list of lists of datetime.datetime\n    It automatically extracts the 'last_modified_datetime' key from the metadata in the corpus data.\n:return: The reranked contents, ids, and scores"
  },
  {
    "function": "UPRScorer.__del__",
    "module": "autorag.nodes.passagereranker.upr",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "UPRScorer.__init__",
    "module": "autorag.nodes.passagereranker.upr",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "suffix_prompt",
        "param_type": "<class 'str'>",
        "description": ""
      },
      {
        "name": "prefix_prompt",
        "param_type": "<class 'str'>",
        "description": ""
      },
      {
        "name": "use_bf16",
        "param_type": "<class 'bool'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Initialize self.  See help(type(self)) for accurate signature."
  },
  {
    "function": "UPRScorer.compute",
    "module": "autorag.nodes.passagereranker.upr",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "query",
        "param_type": "<class 'str'>",
        "description": ""
      },
      {
        "name": "contents",
        "param_type": "typing.List[str]",
        "description": ""
      }
    ],
    "return_type": "typing.List[float]",
    "docstring": null
  },
  {
    "function": "passage_reranker_node",
    "module": "autorag.nodes.passagereranker.base",
    "params": [
      {
        "name": "func",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "select_top_k",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "df",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "column_names",
        "param_type": "typing.List[str]",
        "description": ""
      },
      {
        "name": "top_k",
        "param_type": "<class 'int'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "sort_by_scores",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "row",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "reverse",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Sorts each row by 'scores' column.\nThe input column names must be 'contents', 'ids', and 'scores'.\nAnd its elements must be list type."
  },
  {
    "function": "upr",
    "module": "autorag.nodes.passagereranker.upr",
    "params": [
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": "The list of queries to use for reranking"
      },
      {
        "name": "contents_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of contents to rerank"
      },
      {
        "name": "scores_list",
        "param_type": "typing.List[typing.List[float]]",
        "description": "The list of lists of scores retrieved from the initial ranking"
      },
      {
        "name": "ids_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": "The list of lists of ids retrieved from the initial ranking"
      },
      {
        "name": "top_k",
        "param_type": "<class 'int'>",
        "description": "The number of passages to be retrieved"
      },
      {
        "name": "use_bf16",
        "param_type": "<class 'bool'>",
        "description": "Whether to use bfloat16 for the model. Default is False."
      },
      {
        "name": "prefix_prompt",
        "param_type": "<class 'str'>",
        "description": "The prefix prompt for the language model that generates question for reranking.\n    Default is \"Passage: \".\n    The prefix prompt serves as the initial context or instruction for the language model.\n    It sets the stage for what is expected in the output"
      },
      {
        "name": "suffix_prompt",
        "param_type": "<class 'str'>",
        "description": "The suffix prompt for the language model that generates question for reranking.\n    Default is \"Please write a question based on this passage.\".\n    The suffix prompt provides a cue or a closing instruction to the language model,\n        signaling how to conclude the generated text or what format to follow at the end."
      }
    ],
    "return_type": "typing.Tuple[typing.List[typing.List[str]], typing.List[typing.List[str]], typing.List[typing.List[float]]]",
    "docstring": "Rerank a list of contents based on their relevance to a query using UPR.\nUPR is a reranker based on UPR (https://github.com/DevSinghSachan/unsupervised-passage-reranking).\nThe language model will make a question based on the passage and rerank the passages by the likelihood of the question.\nThe default model is t5-large.\n\n:param queries: The list of queries to use for reranking\n:param contents_list: The list of lists of contents to rerank\n:param scores_list: The list of lists of scores retrieved from the initial ranking\n:param ids_list: The list of lists of ids retrieved from the initial ranking\n:param top_k: The number of passages to be retrieved\n:param use_bf16: Whether to use bfloat16 for the model. Default is False.\n:param prefix_prompt: The prefix prompt for the language model that generates question for reranking.\n    Default is \"Passage: \".\n    The prefix prompt serves as the initial context or instruction for the language model.\n    It sets the stage for what is expected in the output\n:param suffix_prompt: The suffix prompt for the language model that generates question for reranking.\n    Default is \"Please write a question based on this passage.\".\n    The suffix prompt provides a cue or a closing instruction to the language model,\n        signaling how to conclude the generated text or what format to follow at the end.\n:return: tuple of lists containing the reranked contents, ids, and scores"
  },
  {
    "function": "fstring",
    "module": "autorag.nodes.promptmaker.fstring",
    "params": [
      {
        "name": "prompt",
        "param_type": "<class 'str'>",
        "description": "A prompt string."
      },
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": "List of query strings."
      },
      {
        "name": "retrieved_contents",
        "param_type": "typing.List[typing.List[str]]",
        "description": "List of retrieved contents."
      }
    ],
    "return_type": "typing.List[str]",
    "docstring": "   Make a prompt using f-string from a query and retrieved_contents.\n   You must type a prompt or prompt list at config yaml file like this:\n\n   .. Code:: yaml\n   nodes:\n   - node_type: prompt_maker\n     modules:\n     - module_type: fstring\n       prompt: [Answer this question: {query} \n\n{retrieved_contents},\n       Read the passages carefully and answer this question: {query} \n\nPassages: {retrieved_contents}]\n\n   :param prompt: A prompt string.\n   :param queries: List of query strings.\n   :param retrieved_contents: List of retrieved contents.\n   :return: Prompts that made by f-string.\n   "
  },
  {
    "function": "long_context_reorder",
    "module": "autorag.nodes.promptmaker.long_context_reorder",
    "params": [
      {
        "name": "prompt",
        "param_type": "<class 'str'>",
        "description": "A prompt string."
      },
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": "List of query strings."
      },
      {
        "name": "retrieved_contents",
        "param_type": "typing.List[typing.List[str]]",
        "description": "List of retrieved contents."
      },
      {
        "name": "retrieve_scores",
        "param_type": "typing.List[typing.List[float]]",
        "description": "List of retrieve scores."
      }
    ],
    "return_type": "typing.List[str]",
    "docstring": "   Models struggle to access significant details found\n   in the center of extended contexts. A study\n   (https://arxiv.org/abs/2307.03172) observed that the best\n   performance typically arises when crucial data is positioned\n   at the start or conclusion of the input context. Additionally,\n   as the input context lengthens, performance drops notably, even\n   in models designed for long contexts.\".\n\n   .. Code:: yaml\n   nodes:\n   - node_type: prompt_maker\n     modules:\n     - module_type: long_context_reorder\n       prompt: [Answer this question: {query} \n\n{retrieved_contents},\n       Read the passages carefully and answer this question: {query} \n\nPassages: {retrieved_contents}]\n\n   :param prompt: A prompt string.\n   :param queries: List of query strings.\n   :param retrieved_contents: List of retrieved contents.\n   :param retrieve_scores: List of retrieve scores.\n   :return: Prompts that made by long context reorder.\n   "
  },
  {
    "function": "window_replacement",
    "module": "autorag.nodes.promptmaker.window_replacement",
    "params": [
      {
        "name": "prompt",
        "param_type": "<class 'str'>",
        "description": "A prompt string."
      },
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": "List of query strings."
      },
      {
        "name": "retrieved_contents",
        "param_type": "typing.List[typing.List[str]]",
        "description": "List of retrieved contents."
      },
      {
        "name": "retrieved_metadata",
        "param_type": "typing.List[typing.List[typing.Dict]]",
        "description": "List of retrieved metadata."
      }
    ],
    "return_type": "typing.List[str]",
    "docstring": "   Replace retrieved_contents with window to create a Prompt\n   (only available for corpus chunked with Sentence window method)\n   You must type a prompt or prompt list at config yaml file like this:\n\n   .. Code:: yaml\n   nodes:\n   - node_type: prompt_maker\n     modules:\n     - module_type: window_replacement\n       prompt: [Answer this question: {query} \n\n{retrieved_contents},\n       Read the passages carefully and answer this question: {query} \n\nPassages: {retrieved_contents}]\n\n   :param prompt: A prompt string.\n   :param queries: List of query strings.\n   :param retrieved_contents: List of retrieved contents.\n   :param retrieved_metadata: List of retrieved metadata.\n   :return: Prompts that made by window_replacement.\n   "
  },
  {
    "function": "fetch_contents",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "corpus_data",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": ""
      },
      {
        "name": "ids",
        "param_type": "typing.List[typing.List[str]]",
        "description": ""
      },
      {
        "name": "column_name",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "typing.List[typing.List[typing.Any]]",
    "docstring": null
  },
  {
    "function": "prompt_maker_node",
    "module": "autorag.nodes.promptmaker.base",
    "params": [
      {
        "name": "func",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "result_to_dataframe",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "column_names",
        "param_type": "typing.List[str]",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Decorator for converting results to pd.DataFrame."
  },
  {
    "function": "fstring",
    "module": "autorag.nodes.promptmaker.fstring",
    "params": [
      {
        "name": "prompt",
        "param_type": "<class 'str'>",
        "description": "A prompt string."
      },
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": "List of query strings."
      },
      {
        "name": "retrieved_contents",
        "param_type": "typing.List[typing.List[str]]",
        "description": "List of retrieved contents."
      }
    ],
    "return_type": "typing.List[str]",
    "docstring": "   Make a prompt using f-string from a query and retrieved_contents.\n   You must type a prompt or prompt list at config yaml file like this:\n\n   .. Code:: yaml\n   nodes:\n   - node_type: prompt_maker\n     modules:\n     - module_type: fstring\n       prompt: [Answer this question: {query} \n\n{retrieved_contents},\n       Read the passages carefully and answer this question: {query} \n\nPassages: {retrieved_contents}]\n\n   :param prompt: A prompt string.\n   :param queries: List of query strings.\n   :param retrieved_contents: List of retrieved contents.\n   :return: Prompts that made by f-string.\n   "
  },
  {
    "function": "prompt_maker_node",
    "module": "autorag.nodes.promptmaker.base",
    "params": [
      {
        "name": "func",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "long_context_reorder",
    "module": "autorag.nodes.promptmaker.long_context_reorder",
    "params": [
      {
        "name": "prompt",
        "param_type": "<class 'str'>",
        "description": "A prompt string."
      },
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": "List of query strings."
      },
      {
        "name": "retrieved_contents",
        "param_type": "typing.List[typing.List[str]]",
        "description": "List of retrieved contents."
      },
      {
        "name": "retrieve_scores",
        "param_type": "typing.List[typing.List[float]]",
        "description": "List of retrieve scores."
      }
    ],
    "return_type": "typing.List[str]",
    "docstring": "   Models struggle to access significant details found\n   in the center of extended contexts. A study\n   (https://arxiv.org/abs/2307.03172) observed that the best\n   performance typically arises when crucial data is positioned\n   at the start or conclusion of the input context. Additionally,\n   as the input context lengthens, performance drops notably, even\n   in models designed for long contexts.\".\n\n   .. Code:: yaml\n   nodes:\n   - node_type: prompt_maker\n     modules:\n     - module_type: long_context_reorder\n       prompt: [Answer this question: {query} \n\n{retrieved_contents},\n       Read the passages carefully and answer this question: {query} \n\nPassages: {retrieved_contents}]\n\n   :param prompt: A prompt string.\n   :param queries: List of query strings.\n   :param retrieved_contents: List of retrieved contents.\n   :param retrieve_scores: List of retrieve scores.\n   :return: Prompts that made by long context reorder.\n   "
  },
  {
    "function": "prompt_maker_node",
    "module": "autorag.nodes.promptmaker.base",
    "params": [
      {
        "name": "func",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "cast_metrics",
    "module": "autorag.evaluation.util",
    "params": [
      {
        "name": "metrics",
        "param_type": "typing.Union[typing.List[str], typing.List[typing.Dict]]",
        "description": "List of string or dictionary."
      }
    ],
    "return_type": "typing.Tuple[typing.List[str], typing.List[typing.Dict[str, typing.Any]]]",
    "docstring": " Turn metrics to list of metric names and parameter list.\n\n:param metrics: List of string or dictionary.\n:return: The list of metric names and dictionary list of metric parameters."
  },
  {
    "function": "evaluate_generation",
    "module": "autorag.evaluation.generation",
    "params": [
      {
        "name": "generation_gt",
        "param_type": "typing.List[typing.List[str]]",
        "description": ""
      },
      {
        "name": "metrics",
        "param_type": "typing.Union[typing.List[str], typing.List[typing.Dict]]",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "evaluate_generator_result",
    "module": "autorag.nodes.promptmaker.run",
    "params": [
      {
        "name": "result_df",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": ""
      },
      {
        "name": "generation_gt",
        "param_type": "typing.List[typing.List[str]]",
        "description": ""
      },
      {
        "name": "metrics",
        "param_type": "typing.Union[typing.List[str], typing.List[typing.Dict]]",
        "description": ""
      }
    ],
    "return_type": "<class 'pandas.core.frame.DataFrame'>",
    "docstring": null
  },
  {
    "function": "evaluate_one_prompt_maker_node",
    "module": "autorag.nodes.promptmaker.run",
    "params": [
      {
        "name": "prompts",
        "param_type": "typing.List[str]",
        "description": ""
      },
      {
        "name": "generator_funcs",
        "param_type": "typing.List[typing.Callable]",
        "description": ""
      },
      {
        "name": "generator_params",
        "param_type": "typing.List[typing.Dict]",
        "description": ""
      },
      {
        "name": "generation_gt",
        "param_type": "typing.List[typing.List[str]]",
        "description": ""
      },
      {
        "name": "metrics",
        "param_type": "typing.Union[typing.List[str], typing.List[typing.Dict]]",
        "description": ""
      },
      {
        "name": "project_dir",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "strategy_name",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "<class 'pandas.core.frame.DataFrame'>",
    "docstring": null
  },
  {
    "function": "explode",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "index_values",
        "param_type": "typing.Collection[typing.Any]",
        "description": "The index values."
      },
      {
        "name": "explode_values",
        "param_type": "typing.Collection[typing.Collection[typing.Any]]",
        "description": "The exploded values."
      }
    ],
    "return_type": "Any",
    "docstring": "Explode index_values and explode_values.\nThe index_values and explode_values must have the same length.\nIt will flatten explode_values and keep index_values as a pair.\n\n:param index_values: The index values.\n:param explode_values: The exploded values.\n:return: Tuple of exploded index_values and exploded explode_values."
  },
  {
    "function": "filter_by_threshold",
    "module": "autorag.strategy",
    "params": [
      {
        "name": "results",
        "param_type": "Any",
        "description": "The result list to be filtered."
      },
      {
        "name": "value",
        "param_type": "Any",
        "description": "The value list to be filtered.\n    It must have the same length with results."
      },
      {
        "name": "threshold",
        "param_type": "Any",
        "description": "The threshold value."
      },
      {
        "name": "metadatas",
        "param_type": "Any",
        "description": "The metadata of each result."
      }
    ],
    "return_type": "typing.Tuple[typing.List, typing.List]",
    "docstring": "Filter results by value's threshold.\n\n:param results: The result list to be filtered.\n:param value: The value list to be filtered.\n    It must have the same length with results.\n:param threshold: The threshold value.\n:param metadatas: The metadata of each result.\n:return: Filtered list of results and filtered list of metadatas.\n    Metadatas will be returned even if you did not give input metadatas.\n:rtype: Tuple[List, List]"
  },
  {
    "function": "get_support_modules",
    "module": "autorag.support",
    "params": [
      {
        "name": "module_name",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "typing.Callable",
    "docstring": null
  },
  {
    "function": "make_combinations",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "target_dict",
        "param_type": "typing.Dict[str, typing.Any]",
        "description": "The target dictionary."
      }
    ],
    "return_type": "typing.List[typing.Dict[str, typing.Any]]",
    "docstring": "Make combinations from target_dict.\nThe target_dict key value must be a string,\nand the value can be list of values or single value.\nIf generates all combinations of values from target_dict,\nwhich means generated dictionaries that contain only one value for each key,\nand all dictionaries will be different from each other.\n\n:param target_dict: The target dictionary.\n:return: The list of generated dictionaries."
  },
  {
    "function": "make_generator_callable_params",
    "module": "autorag.nodes.promptmaker.run",
    "params": [
      {
        "name": "strategy_dict",
        "param_type": "typing.Dict",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "measure_speed",
    "module": "autorag.strategy",
    "params": [
      {
        "name": "func",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "args",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "kwargs",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Method for measuring execution speed of the function."
  },
  {
    "function": "run_prompt_maker_node",
    "module": "autorag.nodes.promptmaker.run",
    "params": [
      {
        "name": "modules",
        "param_type": "typing.List[typing.Callable]",
        "description": "Prompt maker modules to run."
      },
      {
        "name": "module_params",
        "param_type": "typing.List[typing.Dict]",
        "description": "Prompt maker module parameters."
      },
      {
        "name": "previous_result",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": "Previous result dataframe.\n    Could be query expansion's best result or qa data."
      },
      {
        "name": "node_line_dir",
        "param_type": "<class 'str'>",
        "description": "This node line's directory."
      },
      {
        "name": "strategies",
        "param_type": "typing.Dict",
        "description": "Strategies for prompt maker node."
      }
    ],
    "return_type": "<class 'pandas.core.frame.DataFrame'>",
    "docstring": "Run prompt maker node.\nWith this function, you can select the best prompt maker module.\nAs default, when you can use only one module, the evaluation will be skipped.\nIf you want to select the best prompt among modules, you can use strategies.\nWhen you use them, you must pass 'generator_modules' and its parameters at strategies.\nBecause it uses generator modules and generator metrics for evaluation this module.\nIt is recommended to use one params and modules for evaluation,\nbut you can use multiple params and modules for evaluation.\nWhen you don't set generator module at strategies, it will use the default generator module.\nThe default generator module is llama_index_llm with openai gpt-3.5-turbo model.\n\n:param modules: Prompt maker modules to run.\n:param module_params: Prompt maker module parameters.\n:param previous_result: Previous result dataframe.\n    Could be query expansion's best result or qa data.\n:param node_line_dir: This node line's directory.\n:param strategies: Strategies for prompt maker node.\n:return: The best result dataframe.\n    It contains previous result columns and prompt maker's result columns which is 'prompts'."
  },
  {
    "function": "select_best",
    "module": "autorag.strategy",
    "params": [
      {
        "name": "results",
        "param_type": "typing.List[pandas.core.frame.DataFrame]",
        "description": ""
      },
      {
        "name": "columns",
        "param_type": "typing.Iterable[str]",
        "description": ""
      },
      {
        "name": "metadatas",
        "param_type": "typing.Optional[typing.List[typing.Any]]",
        "description": ""
      },
      {
        "name": "strategy_name",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "typing.Tuple[pandas.core.frame.DataFrame, typing.Any]",
    "docstring": null
  },
  {
    "function": "split_dataframe",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "df",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "chunk_size",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "validate_qa_dataset",
    "module": "autorag.utils.preprocess",
    "params": [
      {
        "name": "df",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "prompt_maker_node",
    "module": "autorag.nodes.promptmaker.base",
    "params": [
      {
        "name": "func",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "window_replacement",
    "module": "autorag.nodes.promptmaker.window_replacement",
    "params": [
      {
        "name": "prompt",
        "param_type": "<class 'str'>",
        "description": "A prompt string."
      },
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": "List of query strings."
      },
      {
        "name": "retrieved_contents",
        "param_type": "typing.List[typing.List[str]]",
        "description": "List of retrieved contents."
      },
      {
        "name": "retrieved_metadata",
        "param_type": "typing.List[typing.List[typing.Dict]]",
        "description": "List of retrieved metadata."
      }
    ],
    "return_type": "typing.List[str]",
    "docstring": "   Replace retrieved_contents with window to create a Prompt\n   (only available for corpus chunked with Sentence window method)\n   You must type a prompt or prompt list at config yaml file like this:\n\n   .. Code:: yaml\n   nodes:\n   - node_type: prompt_maker\n     modules:\n     - module_type: window_replacement\n       prompt: [Answer this question: {query} \n\n{retrieved_contents},\n       Read the passages carefully and answer this question: {query} \n\nPassages: {retrieved_contents}]\n\n   :param prompt: A prompt string.\n   :param queries: List of query strings.\n   :param retrieved_contents: List of retrieved contents.\n   :param retrieved_metadata: List of retrieved metadata.\n   :return: Prompts that made by window_replacement.\n   "
  },
  {
    "function": "hyde",
    "module": "autorag.nodes.queryexpansion.hyde",
    "params": [
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": "List[str], queries to retrieve."
      },
      {
        "name": "generator_func",
        "param_type": "typing.Callable",
        "description": "Callable, generator functions."
      },
      {
        "name": "generator_params",
        "param_type": "typing.Dict",
        "description": "Dict, generator parameters."
      },
      {
        "name": "prompt",
        "param_type": "<class 'str'>",
        "description": "prompt to use when generating hypothetical passage"
      }
    ],
    "return_type": "typing.List[typing.List[str]]",
    "docstring": "HyDE, which inspired by \"Precise Zero-shot Dense Retrieval without Relevance Labels\" (https://arxiv.org/pdf/2212.10496.pdf)\nLLM model creates a hypothetical passage.\nAnd then, retrieve passages using hypothetical passage as a query.\n:param queries: List[str], queries to retrieve.\n:param generator_func: Callable, generator functions.\n:param generator_params: Dict, generator parameters.\n:param prompt: prompt to use when generating hypothetical passage\n:return: List[List[str]], List of hyde results."
  },
  {
    "function": "multi_query_expansion",
    "module": "autorag.nodes.queryexpansion.multi_query_expansion",
    "params": [
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": "List[str], queries to decompose."
      },
      {
        "name": "generator_func",
        "param_type": "typing.Callable",
        "description": "Callable, generator functions."
      },
      {
        "name": "generator_params",
        "param_type": "typing.Dict",
        "description": "Dict, generator parameters."
      },
      {
        "name": "prompt",
        "param_type": "<class 'str'>",
        "description": "str, prompt to use for multi-query expansion.\n    default prompt comes from langchain MultiQueryRetriever default query prompt."
      }
    ],
    "return_type": "typing.List[typing.List[str]]",
    "docstring": "Expand a list of queries using a multi-query expansion approach.\nLLM model generate 3 different versions queries for each input query.\n\n:param queries: List[str], queries to decompose.\n:param generator_func: Callable, generator functions.\n:param generator_params: Dict, generator parameters.\n:param prompt: str, prompt to use for multi-query expansion.\n    default prompt comes from langchain MultiQueryRetriever default query prompt.\n:return: List[List[str]], list of expansion query."
  },
  {
    "function": "pass_query_expansion",
    "module": "autorag.nodes.queryexpansion.pass_query_expansion",
    "params": [
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Do not perform query expansion.\nReturn with the same queries.\nThe dimension will be 2-d list, and the column name will be 'queries'."
  },
  {
    "function": "query_decompose",
    "module": "autorag.nodes.queryexpansion.query_decompose",
    "params": [
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": "List[str], queries to decompose."
      },
      {
        "name": "generator_func",
        "param_type": "typing.Callable",
        "description": "Callable, generator functions."
      },
      {
        "name": "generator_params",
        "param_type": "typing.Dict",
        "description": "Dict, generator parameters."
      },
      {
        "name": "prompt",
        "param_type": "<class 'str'>",
        "description": "str, prompt to use for query decomposition.\n    default prompt comes from Visconde's StrategyQA few-shot prompt."
      }
    ],
    "return_type": "typing.List[typing.List[str]]",
    "docstring": "decompose query to little piece of questions.\n:param queries: List[str], queries to decompose.\n:param generator_func: Callable, generator functions.\n:param generator_params: Dict, generator parameters.\n:param prompt: str, prompt to use for query decomposition.\n    default prompt comes from Visconde's StrategyQA few-shot prompt.\n:return: List[List[str]], list of decomposed query. Return input query if query is not decomposable."
  },
  {
    "function": "get_support_modules",
    "module": "autorag.support",
    "params": [
      {
        "name": "module_name",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "typing.Callable",
    "docstring": null
  },
  {
    "function": "make_generator_callable_param",
    "module": "autorag.nodes.queryexpansion.base",
    "params": [
      {
        "name": "generator_dict",
        "param_type": "typing.Optional[typing.Dict]",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "query_expansion_node",
    "module": "autorag.nodes.queryexpansion.base",
    "params": [
      {
        "name": "func",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "result_to_dataframe",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "column_names",
        "param_type": "typing.List[str]",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Decorator for converting results to pd.DataFrame."
  },
  {
    "function": "validate_qa_dataset",
    "module": "autorag.utils.preprocess",
    "params": [
      {
        "name": "df",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "hyde",
    "module": "autorag.nodes.queryexpansion.hyde",
    "params": [
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": "List[str], queries to retrieve."
      },
      {
        "name": "generator_func",
        "param_type": "typing.Callable",
        "description": "Callable, generator functions."
      },
      {
        "name": "generator_params",
        "param_type": "typing.Dict",
        "description": "Dict, generator parameters."
      },
      {
        "name": "prompt",
        "param_type": "<class 'str'>",
        "description": "prompt to use when generating hypothetical passage"
      }
    ],
    "return_type": "typing.List[typing.List[str]]",
    "docstring": "HyDE, which inspired by \"Precise Zero-shot Dense Retrieval without Relevance Labels\" (https://arxiv.org/pdf/2212.10496.pdf)\nLLM model creates a hypothetical passage.\nAnd then, retrieve passages using hypothetical passage as a query.\n:param queries: List[str], queries to retrieve.\n:param generator_func: Callable, generator functions.\n:param generator_params: Dict, generator parameters.\n:param prompt: prompt to use when generating hypothetical passage\n:return: List[List[str]], List of hyde results."
  },
  {
    "function": "query_expansion_node",
    "module": "autorag.nodes.queryexpansion.base",
    "params": [
      {
        "name": "func",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "get_multi_query_expansion",
    "module": "autorag.nodes.queryexpansion.multi_query_expansion",
    "params": [
      {
        "name": "query",
        "param_type": "<class 'str'>",
        "description": ""
      },
      {
        "name": "answer",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "typing.List[str]",
    "docstring": null
  },
  {
    "function": "multi_query_expansion",
    "module": "autorag.nodes.queryexpansion.multi_query_expansion",
    "params": [
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": "List[str], queries to decompose."
      },
      {
        "name": "generator_func",
        "param_type": "typing.Callable",
        "description": "Callable, generator functions."
      },
      {
        "name": "generator_params",
        "param_type": "typing.Dict",
        "description": "Dict, generator parameters."
      },
      {
        "name": "prompt",
        "param_type": "<class 'str'>",
        "description": "str, prompt to use for multi-query expansion.\n    default prompt comes from langchain MultiQueryRetriever default query prompt."
      }
    ],
    "return_type": "typing.List[typing.List[str]]",
    "docstring": "Expand a list of queries using a multi-query expansion approach.\nLLM model generate 3 different versions queries for each input query.\n\n:param queries: List[str], queries to decompose.\n:param generator_func: Callable, generator functions.\n:param generator_params: Dict, generator parameters.\n:param prompt: str, prompt to use for multi-query expansion.\n    default prompt comes from langchain MultiQueryRetriever default query prompt.\n:return: List[List[str]], list of expansion query."
  },
  {
    "function": "query_expansion_node",
    "module": "autorag.nodes.queryexpansion.base",
    "params": [
      {
        "name": "func",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "pass_query_expansion",
    "module": "autorag.nodes.queryexpansion.pass_query_expansion",
    "params": [
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Do not perform query expansion.\nReturn with the same queries.\nThe dimension will be 2-d list, and the column name will be 'queries'."
  },
  {
    "function": "query_expansion_node",
    "module": "autorag.nodes.queryexpansion.base",
    "params": [
      {
        "name": "func",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "get_query_decompose",
    "module": "autorag.nodes.queryexpansion.query_decompose",
    "params": [
      {
        "name": "query",
        "param_type": "<class 'str'>",
        "description": "str, query to decompose."
      },
      {
        "name": "answer",
        "param_type": "<class 'str'>",
        "description": "str, answer from query_decompose function."
      }
    ],
    "return_type": "typing.List[str]",
    "docstring": "decompose query to little piece of questions.\n:param query: str, query to decompose.\n:param answer: str, answer from query_decompose function.\n:return: List[str], list of a decomposed query. Return input query if query is not decomposable."
  },
  {
    "function": "query_decompose",
    "module": "autorag.nodes.queryexpansion.query_decompose",
    "params": [
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": "List[str], queries to decompose."
      },
      {
        "name": "generator_func",
        "param_type": "typing.Callable",
        "description": "Callable, generator functions."
      },
      {
        "name": "generator_params",
        "param_type": "typing.Dict",
        "description": "Dict, generator parameters."
      },
      {
        "name": "prompt",
        "param_type": "<class 'str'>",
        "description": "str, prompt to use for query decomposition.\n    default prompt comes from Visconde's StrategyQA few-shot prompt."
      }
    ],
    "return_type": "typing.List[typing.List[str]]",
    "docstring": "decompose query to little piece of questions.\n:param queries: List[str], queries to decompose.\n:param generator_func: Callable, generator functions.\n:param generator_params: Dict, generator parameters.\n:param prompt: str, prompt to use for query decomposition.\n    default prompt comes from Visconde's StrategyQA few-shot prompt.\n:return: List[List[str]], list of decomposed query. Return input query if query is not decomposable."
  },
  {
    "function": "query_expansion_node",
    "module": "autorag.nodes.queryexpansion.base",
    "params": [
      {
        "name": "func",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "evaluate_one_query_expansion_node",
    "module": "autorag.nodes.queryexpansion.run",
    "params": [
      {
        "name": "retrieval_funcs",
        "param_type": "typing.List[typing.Callable]",
        "description": ""
      },
      {
        "name": "retrieval_params",
        "param_type": "typing.List[typing.Dict]",
        "description": ""
      },
      {
        "name": "expanded_queries",
        "param_type": "typing.List[typing.List[str]]",
        "description": ""
      },
      {
        "name": "retrieval_gt",
        "param_type": "typing.List[typing.List[str]]",
        "description": ""
      },
      {
        "name": "metrics",
        "param_type": "typing.List[str]",
        "description": ""
      },
      {
        "name": "project_dir",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "previous_result",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": ""
      },
      {
        "name": "strategy_name",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "<class 'pandas.core.frame.DataFrame'>",
    "docstring": null
  },
  {
    "function": "evaluate_retrieval_node",
    "module": "autorag.nodes.retrieval.run",
    "params": [
      {
        "name": "result_df",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": "The result dataframe from a retrieval node."
      },
      {
        "name": "retrieval_gt",
        "param_type": "Any",
        "description": "Ground truth for retrieval from qa dataset."
      },
      {
        "name": "metrics",
        "param_type": "Any",
        "description": "Metric list from input strategies."
      },
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": "Query list from input strategies."
      },
      {
        "name": "generation_gt",
        "param_type": "typing.List[typing.List[str]]",
        "description": "Ground truth for generation from qa dataset."
      }
    ],
    "return_type": "<class 'pandas.core.frame.DataFrame'>",
    "docstring": "Evaluate retrieval node from retrieval node result dataframe.\n\n:param result_df: The result dataframe from a retrieval node.\n:param retrieval_gt: Ground truth for retrieval from qa dataset.\n:param metrics: Metric list from input strategies.\n:param queries: Query list from input strategies.\n:param generation_gt: Ground truth for generation from qa dataset.\n:return: Return result_df with metrics columns.\n    The columns will be 'retrieved_contents', 'retrieved_ids', 'retrieve_scores', and metric names."
  },
  {
    "function": "explode",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "index_values",
        "param_type": "typing.Collection[typing.Any]",
        "description": "The index values."
      },
      {
        "name": "explode_values",
        "param_type": "typing.Collection[typing.Collection[typing.Any]]",
        "description": "The exploded values."
      }
    ],
    "return_type": "Any",
    "docstring": "Explode index_values and explode_values.\nThe index_values and explode_values must have the same length.\nIt will flatten explode_values and keep index_values as a pair.\n\n:param index_values: The index values.\n:param explode_values: The exploded values.\n:return: Tuple of exploded index_values and exploded explode_values."
  },
  {
    "function": "filter_by_threshold",
    "module": "autorag.strategy",
    "params": [
      {
        "name": "results",
        "param_type": "Any",
        "description": "The result list to be filtered."
      },
      {
        "name": "value",
        "param_type": "Any",
        "description": "The value list to be filtered.\n    It must have the same length with results."
      },
      {
        "name": "threshold",
        "param_type": "Any",
        "description": "The threshold value."
      },
      {
        "name": "metadatas",
        "param_type": "Any",
        "description": "The metadata of each result."
      }
    ],
    "return_type": "typing.Tuple[typing.List, typing.List]",
    "docstring": "Filter results by value's threshold.\n\n:param results: The result list to be filtered.\n:param value: The value list to be filtered.\n    It must have the same length with results.\n:param threshold: The threshold value.\n:param metadatas: The metadata of each result.\n:return: Filtered list of results and filtered list of metadatas.\n    Metadatas will be returned even if you did not give input metadatas.\n:rtype: Tuple[List, List]"
  },
  {
    "function": "get_support_modules",
    "module": "autorag.support",
    "params": [
      {
        "name": "module_name",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "typing.Callable",
    "docstring": null
  },
  {
    "function": "make_combinations",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "target_dict",
        "param_type": "typing.Dict[str, typing.Any]",
        "description": "The target dictionary."
      }
    ],
    "return_type": "typing.List[typing.Dict[str, typing.Any]]",
    "docstring": "Make combinations from target_dict.\nThe target_dict key value must be a string,\nand the value can be list of values or single value.\nIf generates all combinations of values from target_dict,\nwhich means generated dictionaries that contain only one value for each key,\nand all dictionaries will be different from each other.\n\n:param target_dict: The target dictionary.\n:return: The list of generated dictionaries."
  },
  {
    "function": "make_retrieval_callable_params",
    "module": "autorag.nodes.queryexpansion.run",
    "params": [
      {
        "name": "strategy_dict",
        "param_type": "typing.Dict",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "strategy_dict looks like this:\n\n.. Code:: json\n\n    {\n        \"metrics\": [\"retrieval_f1\", \"retrieval_recall\"],\n        \"top_k\": 50,\n        \"retrieval_modules\": [\n          {\"module_type\": \"bm25\"},\n          {\"module_type\": \"vectordb\", \"embedding_model\": [\"openai\", \"huggingface\"]}\n        ]\n      }"
  },
  {
    "function": "measure_speed",
    "module": "autorag.strategy",
    "params": [
      {
        "name": "func",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "args",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "kwargs",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Method for measuring execution speed of the function."
  },
  {
    "function": "run_query_expansion_node",
    "module": "autorag.nodes.queryexpansion.run",
    "params": [
      {
        "name": "modules",
        "param_type": "typing.List[typing.Callable]",
        "description": "Query expansion modules to run."
      },
      {
        "name": "module_params",
        "param_type": "typing.List[typing.Dict]",
        "description": "Query expansion module parameters."
      },
      {
        "name": "previous_result",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": "Previous result dataframe.\n    In this case, it would be qa data."
      },
      {
        "name": "node_line_dir",
        "param_type": "<class 'str'>",
        "description": "This node line's directory."
      },
      {
        "name": "strategies",
        "param_type": "typing.Dict",
        "description": "Strategies for query expansion node."
      }
    ],
    "return_type": "<class 'pandas.core.frame.DataFrame'>",
    "docstring": "Run evaluation and select the best module among query expansion node results.\nInitially, retrieval is run using expanded_queries, the result of the query_expansion module.\nThe retrieval module is run as a combination of the retrieval_modules in strategies.\nIf there are multiple retrieval_modules, run them all and choose the best result.\nIf there are no retrieval_modules, run them with the default of bm25.\nIn this way, the best result is selected for each module, and then the best result is selected.\n\n:param modules: Query expansion modules to run.\n:param module_params: Query expansion module parameters.\n:param previous_result: Previous result dataframe.\n    In this case, it would be qa data.\n:param node_line_dir: This node line's directory.\n:param strategies: Strategies for query expansion node.\n:return: The best result dataframe."
  },
  {
    "function": "select_best",
    "module": "autorag.strategy",
    "params": [
      {
        "name": "results",
        "param_type": "typing.List[pandas.core.frame.DataFrame]",
        "description": ""
      },
      {
        "name": "columns",
        "param_type": "typing.Iterable[str]",
        "description": ""
      },
      {
        "name": "metadatas",
        "param_type": "typing.Optional[typing.List[typing.Any]]",
        "description": ""
      },
      {
        "name": "strategy_name",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "typing.Tuple[pandas.core.frame.DataFrame, typing.Any]",
    "docstring": null
  },
  {
    "function": "bm25",
    "module": "autorag.nodes.retrieval.bm25",
    "params": [
      {
        "name": "queries",
        "param_type": "typing.List[typing.List[str]]",
        "description": "2-d list of query strings.\n    Each element of the list is a query strings of each row."
      },
      {
        "name": "top_k",
        "param_type": "<class 'int'>",
        "description": "The number of passages to be retrieved."
      },
      {
        "name": "bm25_corpus",
        "param_type": "typing.Dict",
        "description": "A dictionary containing the bm25 corpus, which is doc_id from corpus and tokenized corpus.\n    Its data structure looks like this:\n\n    .. Code:: python\n\n        {\n            \"tokens\": [], # 2d list of tokens\n            \"passage_id\": [], # 2d list of passage_id.\n        }"
      },
      {
        "name": "bm25_tokenizer",
        "param_type": "<class 'str'>",
        "description": "The tokenizer name that uses to the BM25.\n    It supports 'porter_stemmer', 'ko_kiwi', and huggingface `AutoTokenizer`.\n    You can pass huggingface tokenizer name.\n    Default is porter_stemmer."
      }
    ],
    "return_type": "typing.Tuple[typing.List[typing.List[str]], typing.List[typing.List[float]]]",
    "docstring": "BM25 retrieval function.\nYou have to load a pickle file that is already ingested.\n\n:param queries: 2-d list of query strings.\n    Each element of the list is a query strings of each row.\n:param top_k: The number of passages to be retrieved.\n:param bm25_corpus: A dictionary containing the bm25 corpus, which is doc_id from corpus and tokenized corpus.\n    Its data structure looks like this:\n\n    .. Code:: python\n\n        {\n            \"tokens\": [], # 2d list of tokens\n            \"passage_id\": [], # 2d list of passage_id.\n        }\n\n:param bm25_tokenizer: The tokenizer name that uses to the BM25.\n    It supports 'porter_stemmer', 'ko_kiwi', and huggingface `AutoTokenizer`.\n    You can pass huggingface tokenizer name.\n    Default is porter_stemmer.\n:return: The 2-d list contains a list of passage ids that retrieved from bm25 and 2-d list of its scores.\n    It will be a length of queries. And each element has a length of top_k."
  },
  {
    "function": "hybrid_cc",
    "module": "autorag.nodes.retrieval.hybrid_cc",
    "params": [
      {
        "name": "ids",
        "param_type": "typing.Tuple",
        "description": "The tuple of ids that you want to fuse.\n    The length of this must be the same as the length of scores.\n    The semantic retrieval ids must be the first index."
      },
      {
        "name": "scores",
        "param_type": "typing.Tuple",
        "description": "The retrieve scores that you want to fuse.\n    The length of this must be the same as the length of ids.\n    The semantic retrieval scores must be the first index."
      },
      {
        "name": "top_k",
        "param_type": "<class 'int'>",
        "description": "The number of passages to be retrieved."
      },
      {
        "name": "weight",
        "param_type": "<class 'float'>",
        "description": "The weight value. If the weight is 1.0, it means the\n  weight to the semantic module will be 1.0 and weight to the lexical module will be 0.0."
      },
      {
        "name": "normalize_method",
        "param_type": "<class 'str'>",
        "description": "The normalization method to use.\n  There are some normalization method that you can use at the hybrid cc method.\n  AutoRAG support following.\n    - `mm`: Min-max scaling\n    - `tmm`: Theoretical min-max scaling\n    - `z`: z-score normalization\n    - `dbsf`: 3-sigma normalization"
      },
      {
        "name": "semantic_theoretical_min_value",
        "param_type": "<class 'float'>",
        "description": "This value used by `tmm` normalization method. You can set the\n    theoretical minimum value by yourself. Default is -1."
      },
      {
        "name": "lexical_theoretical_min_value",
        "param_type": "<class 'float'>",
        "description": "This value used by `tmm` normalization method. You can set the\n    theoretical minimum value by yourself. Default is 0."
      }
    ],
    "return_type": "typing.Tuple[typing.List[typing.List[str]], typing.List[typing.List[float]]]",
    "docstring": "Hybrid CC function.\nCC (convex combination) is a method to fuse lexical and semantic retrieval results.\nIt is a method that first normalizes the scores of each retrieval result,\nand then combines them with the given weights.\nIt is uniquer than other retrieval modules, because it does not really execute retrieval,\nbut just fuse the results of other retrieval functions.\nSo you have to run more than two retrieval modules before running this function.\nAnd collect ids and scores result from each retrieval module.\nMake it as tuple and input it to this function.\n\n:param ids: The tuple of ids that you want to fuse.\n    The length of this must be the same as the length of scores.\n    The semantic retrieval ids must be the first index.\n:param scores: The retrieve scores that you want to fuse.\n    The length of this must be the same as the length of ids.\n    The semantic retrieval scores must be the first index.\n:param top_k: The number of passages to be retrieved.\n:param normalize_method: The normalization method to use.\n  There are some normalization method that you can use at the hybrid cc method.\n  AutoRAG support following.\n    - `mm`: Min-max scaling\n    - `tmm`: Theoretical min-max scaling\n    - `z`: z-score normalization\n    - `dbsf`: 3-sigma normalization\n:param weight: The weight value. If the weight is 1.0, it means the\n  weight to the semantic module will be 1.0 and weight to the lexical module will be 0.0.\n:param semantic_theoretical_min_value: This value used by `tmm` normalization method. You can set the\n    theoretical minimum value by yourself. Default is -1.\n:param lexical_theoretical_min_value: This value used by `tmm` normalization method. You can set the\n    theoretical minimum value by yourself. Default is 0.\n:return: The tuple of ids and fused scores that fused by CC. Plus, the third element is selected weight value."
  },
  {
    "function": "hybrid_rrf",
    "module": "autorag.nodes.retrieval.hybrid_rrf",
    "params": [
      {
        "name": "ids",
        "param_type": "typing.Tuple",
        "description": "The tuple of ids that you want to fuse.\n    The length of this must be the same as the length of scores."
      },
      {
        "name": "scores",
        "param_type": "typing.Tuple",
        "description": "The retrieve scores that you want to fuse.\n    The length of this must be the same as the length of ids."
      },
      {
        "name": "top_k",
        "param_type": "<class 'int'>",
        "description": "The number of passages to be retrieved."
      },
      {
        "name": "weight",
        "param_type": "<class 'int'>",
        "description": "Hyperparameter for RRF.\n    It was originally rrf_k value.\n    Default is 60.\n    For more information, please visit our documentation."
      },
      {
        "name": "rrf_k",
        "param_type": "<class 'int'>",
        "description": "(Deprecated) Hyperparameter for RRF.\n    It was originally rrf_k value. Will remove at further version."
      }
    ],
    "return_type": "typing.Tuple[typing.List[typing.List[str]], typing.List[typing.List[float]]]",
    "docstring": "Hybrid RRF function.\nRRF (Rank Reciprocal Fusion) is a method to fuse multiple retrieval results.\nIt is common to fuse dense retrieval and sparse retrieval results using RRF.\nTo use this function, you must input ids and scores as tuple.\nIt is uniquer than other retrieval modules, because it does not really execute retrieval,\nbut just fuse the results of other retrieval functions.\nSo you have to run more than two retrieval modules before running this function.\nAnd collect ids and scores result from each retrieval module.\nMake it as tuple and input it to this function.\n\n:param ids: The tuple of ids that you want to fuse.\n    The length of this must be the same as the length of scores.\n:param scores: The retrieve scores that you want to fuse.\n    The length of this must be the same as the length of ids.\n:param top_k: The number of passages to be retrieved.\n:param weight: Hyperparameter for RRF.\n    It was originally rrf_k value.\n    Default is 60.\n    For more information, please visit our documentation.\n:param rrf_k: (Deprecated) Hyperparameter for RRF.\n    It was originally rrf_k value. Will remove at further version.\n:return: The tuple of ids and fused scores that fused by RRF."
  },
  {
    "function": "retrieval_node",
    "module": "autorag.nodes.retrieval.base",
    "params": [
      {
        "name": "func",
        "param_type": "Any",
        "description": "Retrieval function that returns a list of ids and a list of scores"
      }
    ],
    "return_type": "Any",
    "docstring": "Load resources for running retrieval_node.\nFor example, it loads bm25 corpus for bm25 retrieval.\n\n:param func: Retrieval function that returns a list of ids and a list of scores\n:return: A pandas Dataframe that contains retrieved contents, retrieved ids, and retrieve scores.\n    The column name will be \"retrieved_contents\", \"retrieved_ids\", and \"retrieve_scores\"."
  },
  {
    "function": "vectordb",
    "module": "autorag.nodes.retrieval.vectordb",
    "params": [
      {
        "name": "queries",
        "param_type": "typing.List[typing.List[str]]",
        "description": "2-d list of query strings.\n    Each element of the list is a query strings of each row."
      },
      {
        "name": "top_k",
        "param_type": "<class 'int'>",
        "description": "The number of passages to be retrieved."
      },
      {
        "name": "collection",
        "param_type": "<class 'chromadb.api.models.Collection.Collection'>",
        "description": "A chroma collection instance that will be used to retrieve passages."
      },
      {
        "name": "embedding_model",
        "param_type": "<class 'llama_index.core.base.embeddings.base.BaseEmbedding'>",
        "description": "An embedding model instance that will be used to embed queries."
      },
      {
        "name": "embedding_batch",
        "param_type": "<class 'int'>",
        "description": "The number of queries to be processed in parallel.\n    This is used to prevent API error at the query embedding.\n    Default is 128."
      }
    ],
    "return_type": "typing.Tuple[typing.List[typing.List[str]], typing.List[typing.List[float]]]",
    "docstring": "VectorDB retrieval function.\nYou have to get a chroma collection that is already ingested.\nYou have to get an embedding model that is already used in ingesting.\n\n:param queries: 2-d list of query strings.\n    Each element of the list is a query strings of each row.\n:param top_k: The number of passages to be retrieved.\n:param collection: A chroma collection instance that will be used to retrieve passages.\n:param embedding_model: An embedding model instance that will be used to embed queries.\n:param embedding_batch: The number of queries to be processed in parallel.\n    This is used to prevent API error at the query embedding.\n    Default is 128.\n\n:return: The 2-d list contains a list of passage ids that retrieved from vectordb and 2-d list of its scores.\n    It will be a length of queries. And each element has a length of top_k."
  },
  {
    "function": "cast_queries",
    "module": "autorag.nodes.retrieval.base",
    "params": [
      {
        "name": "queries",
        "param_type": "typing.Union[str, typing.List[str]]",
        "description": ""
      }
    ],
    "return_type": "typing.List[str]",
    "docstring": null
  },
  {
    "function": "evenly_distribute_passages",
    "module": "autorag.nodes.retrieval.base",
    "params": [
      {
        "name": "ids",
        "param_type": "typing.List[typing.List[str]]",
        "description": ""
      },
      {
        "name": "scores",
        "param_type": "typing.List[typing.List[float]]",
        "description": ""
      },
      {
        "name": "top_k",
        "param_type": "<class 'int'>",
        "description": ""
      }
    ],
    "return_type": "typing.Tuple[typing.List[str], typing.List[float]]",
    "docstring": null
  },
  {
    "function": "fetch_contents",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "corpus_data",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": ""
      },
      {
        "name": "ids",
        "param_type": "typing.List[typing.List[str]]",
        "description": ""
      },
      {
        "name": "column_name",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "typing.List[typing.List[typing.Any]]",
    "docstring": null
  },
  {
    "function": "get_bm25_pkl_name",
    "module": "autorag.nodes.retrieval.base",
    "params": [
      {
        "name": "bm25_tokenizer",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "get_support_modules",
    "module": "autorag.support",
    "params": [
      {
        "name": "module_name",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "typing.Callable",
    "docstring": null
  },
  {
    "function": "load_bm25_corpus",
    "module": "autorag.nodes.retrieval.base",
    "params": [
      {
        "name": "bm25_path",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "typing.Dict",
    "docstring": null
  },
  {
    "function": "load_chroma_collection",
    "module": "autorag.nodes.retrieval.base",
    "params": [
      {
        "name": "db_path",
        "param_type": "<class 'str'>",
        "description": ""
      },
      {
        "name": "collection_name",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "<class 'chromadb.api.models.Collection.Collection'>",
    "docstring": null
  },
  {
    "function": "result_to_dataframe",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "column_names",
        "param_type": "typing.List[str]",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Decorator for converting results to pd.DataFrame."
  },
  {
    "function": "retrieval_node",
    "module": "autorag.nodes.retrieval.base",
    "params": [
      {
        "name": "func",
        "param_type": "Any",
        "description": "Retrieval function that returns a list of ids and a list of scores"
      }
    ],
    "return_type": "Any",
    "docstring": "Load resources for running retrieval_node.\nFor example, it loads bm25 corpus for bm25 retrieval.\n\n:param func: Retrieval function that returns a list of ids and a list of scores\n:return: A pandas Dataframe that contains retrieved contents, retrieved ids, and retrieve scores.\n    The column name will be \"retrieved_contents\", \"retrieved_ids\", and \"retrieve_scores\"."
  },
  {
    "function": "validate_qa_dataset",
    "module": "autorag.utils.preprocess",
    "params": [
      {
        "name": "df",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "bm25",
    "module": "autorag.nodes.retrieval.bm25",
    "params": [
      {
        "name": "queries",
        "param_type": "typing.List[typing.List[str]]",
        "description": "2-d list of query strings.\n    Each element of the list is a query strings of each row."
      },
      {
        "name": "top_k",
        "param_type": "<class 'int'>",
        "description": "The number of passages to be retrieved."
      },
      {
        "name": "bm25_corpus",
        "param_type": "typing.Dict",
        "description": "A dictionary containing the bm25 corpus, which is doc_id from corpus and tokenized corpus.\n    Its data structure looks like this:\n\n    .. Code:: python\n\n        {\n            \"tokens\": [], # 2d list of tokens\n            \"passage_id\": [], # 2d list of passage_id.\n        }"
      },
      {
        "name": "bm25_tokenizer",
        "param_type": "<class 'str'>",
        "description": "The tokenizer name that uses to the BM25.\n    It supports 'porter_stemmer', 'ko_kiwi', and huggingface `AutoTokenizer`.\n    You can pass huggingface tokenizer name.\n    Default is porter_stemmer."
      }
    ],
    "return_type": "typing.Tuple[typing.List[typing.List[str]], typing.List[typing.List[float]]]",
    "docstring": "BM25 retrieval function.\nYou have to load a pickle file that is already ingested.\n\n:param queries: 2-d list of query strings.\n    Each element of the list is a query strings of each row.\n:param top_k: The number of passages to be retrieved.\n:param bm25_corpus: A dictionary containing the bm25 corpus, which is doc_id from corpus and tokenized corpus.\n    Its data structure looks like this:\n\n    .. Code:: python\n\n        {\n            \"tokens\": [], # 2d list of tokens\n            \"passage_id\": [], # 2d list of passage_id.\n        }\n\n:param bm25_tokenizer: The tokenizer name that uses to the BM25.\n    It supports 'porter_stemmer', 'ko_kiwi', and huggingface `AutoTokenizer`.\n    You can pass huggingface tokenizer name.\n    Default is porter_stemmer.\n:return: The 2-d list contains a list of passage ids that retrieved from bm25 and 2-d list of its scores.\n    It will be a length of queries. And each element has a length of top_k."
  },
  {
    "function": "bm25_ingest",
    "module": "autorag.nodes.retrieval.bm25",
    "params": [
      {
        "name": "corpus_path",
        "param_type": "<class 'str'>",
        "description": ""
      },
      {
        "name": "corpus_data",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": ""
      },
      {
        "name": "bm25_tokenizer",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "bm25_pure",
    "module": "autorag.nodes.retrieval.bm25",
    "params": [
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": "A list of query strings."
      },
      {
        "name": "top_k",
        "param_type": "<class 'int'>",
        "description": "The number of passages to be retrieved."
      },
      {
        "name": "tokenizer",
        "param_type": "Any",
        "description": "A tokenizer that will be used to tokenize queries."
      },
      {
        "name": "bm25_api",
        "param_type": "<class 'rank_bm25.BM25Okapi'>",
        "description": "A bm25 api instance that will be used to retrieve passages."
      },
      {
        "name": "bm25_corpus",
        "param_type": "typing.Dict",
        "description": "A dictionary containing the bm25 corpus, which is doc_id from corpus and tokenized corpus.\n    Its data structure looks like this:\n\n    .. Code:: python\n\n        {\n            \"tokens\": [], # 2d list of tokens\n            \"passage_id\": [], # 2d list of passage_id. Type must be str.\n        }"
      }
    ],
    "return_type": "typing.Tuple[typing.List[str], typing.List[float]]",
    "docstring": "Async BM25 retrieval function.\nIts usage is for async retrieval of bm25 row by row.\n\n:param queries: A list of query strings.\n:param top_k: The number of passages to be retrieved.\n:param tokenizer: A tokenizer that will be used to tokenize queries.\n:param bm25_api: A bm25 api instance that will be used to retrieve passages.\n:param bm25_corpus: A dictionary containing the bm25 corpus, which is doc_id from corpus and tokenized corpus.\n    Its data structure looks like this:\n\n    .. Code:: python\n\n        {\n            \"tokens\": [], # 2d list of tokens\n            \"passage_id\": [], # 2d list of passage_id. Type must be str.\n        }\n:return: The tuple contains a list of passage ids that retrieved from bm25 and its scores."
  },
  {
    "function": "evenly_distribute_passages",
    "module": "autorag.nodes.retrieval.base",
    "params": [
      {
        "name": "ids",
        "param_type": "typing.List[typing.List[str]]",
        "description": ""
      },
      {
        "name": "scores",
        "param_type": "typing.List[typing.List[float]]",
        "description": ""
      },
      {
        "name": "top_k",
        "param_type": "<class 'int'>",
        "description": ""
      }
    ],
    "return_type": "typing.Tuple[typing.List[str], typing.List[float]]",
    "docstring": null
  },
  {
    "function": "normalize_string",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "s",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "<class 'str'>",
    "docstring": "Taken from the official evaluation script for v1.1 of the SQuAD dataset.\nLower text and remove punctuation, articles and extra whitespace."
  },
  {
    "function": "retrieval_node",
    "module": "autorag.nodes.retrieval.base",
    "params": [
      {
        "name": "func",
        "param_type": "Any",
        "description": "Retrieval function that returns a list of ids and a list of scores"
      }
    ],
    "return_type": "Any",
    "docstring": "Load resources for running retrieval_node.\nFor example, it loads bm25 corpus for bm25 retrieval.\n\n:param func: Retrieval function that returns a list of ids and a list of scores\n:return: A pandas Dataframe that contains retrieved contents, retrieved ids, and retrieve scores.\n    The column name will be \"retrieved_contents\", \"retrieved_ids\", and \"retrieve_scores\"."
  },
  {
    "function": "select_bm25_tokenizer",
    "module": "autorag.nodes.retrieval.bm25",
    "params": [
      {
        "name": "bm25_tokenizer",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "typing.Callable[[str], typing.List[typing.Union[int, str]]]",
    "docstring": null
  },
  {
    "function": "tokenize_ko_kiwi",
    "module": "autorag.nodes.retrieval.bm25",
    "params": [
      {
        "name": "texts",
        "param_type": "typing.List[str]",
        "description": ""
      }
    ],
    "return_type": "typing.List[typing.List[str]]",
    "docstring": null
  },
  {
    "function": "tokenize_porter_stemmer",
    "module": "autorag.nodes.retrieval.bm25",
    "params": [
      {
        "name": "texts",
        "param_type": "typing.List[str]",
        "description": ""
      }
    ],
    "return_type": "typing.List[typing.List[str]]",
    "docstring": null
  },
  {
    "function": "tokenize_space",
    "module": "autorag.nodes.retrieval.bm25",
    "params": [
      {
        "name": "texts",
        "param_type": "typing.List[str]",
        "description": ""
      }
    ],
    "return_type": "typing.List[typing.List[str]]",
    "docstring": null
  },
  {
    "function": "validate_corpus_dataset",
    "module": "autorag.utils.preprocess",
    "params": [
      {
        "name": "df",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "fuse_per_query",
    "module": "autorag.nodes.retrieval.hybrid_cc",
    "params": [
      {
        "name": "semantic_ids",
        "param_type": "typing.List[str]",
        "description": ""
      },
      {
        "name": "lexical_ids",
        "param_type": "typing.List[str]",
        "description": ""
      },
      {
        "name": "semantic_scores",
        "param_type": "typing.List[float]",
        "description": ""
      },
      {
        "name": "lexical_scores",
        "param_type": "typing.List[float]",
        "description": ""
      },
      {
        "name": "normalize_method",
        "param_type": "<class 'str'>",
        "description": ""
      },
      {
        "name": "weight",
        "param_type": "<class 'float'>",
        "description": ""
      },
      {
        "name": "top_k",
        "param_type": "<class 'int'>",
        "description": ""
      },
      {
        "name": "semantic_theoretical_min_value",
        "param_type": "<class 'float'>",
        "description": ""
      },
      {
        "name": "lexical_theoretical_min_value",
        "param_type": "<class 'float'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "hybrid_cc",
    "module": "autorag.nodes.retrieval.hybrid_cc",
    "params": [
      {
        "name": "ids",
        "param_type": "typing.Tuple",
        "description": "The tuple of ids that you want to fuse.\n    The length of this must be the same as the length of scores.\n    The semantic retrieval ids must be the first index."
      },
      {
        "name": "scores",
        "param_type": "typing.Tuple",
        "description": "The retrieve scores that you want to fuse.\n    The length of this must be the same as the length of ids.\n    The semantic retrieval scores must be the first index."
      },
      {
        "name": "top_k",
        "param_type": "<class 'int'>",
        "description": "The number of passages to be retrieved."
      },
      {
        "name": "weight",
        "param_type": "<class 'float'>",
        "description": "The weight value. If the weight is 1.0, it means the\n  weight to the semantic module will be 1.0 and weight to the lexical module will be 0.0."
      },
      {
        "name": "normalize_method",
        "param_type": "<class 'str'>",
        "description": "The normalization method to use.\n  There are some normalization method that you can use at the hybrid cc method.\n  AutoRAG support following.\n    - `mm`: Min-max scaling\n    - `tmm`: Theoretical min-max scaling\n    - `z`: z-score normalization\n    - `dbsf`: 3-sigma normalization"
      },
      {
        "name": "semantic_theoretical_min_value",
        "param_type": "<class 'float'>",
        "description": "This value used by `tmm` normalization method. You can set the\n    theoretical minimum value by yourself. Default is -1."
      },
      {
        "name": "lexical_theoretical_min_value",
        "param_type": "<class 'float'>",
        "description": "This value used by `tmm` normalization method. You can set the\n    theoretical minimum value by yourself. Default is 0."
      }
    ],
    "return_type": "typing.Tuple[typing.List[typing.List[str]], typing.List[typing.List[float]]]",
    "docstring": "Hybrid CC function.\nCC (convex combination) is a method to fuse lexical and semantic retrieval results.\nIt is a method that first normalizes the scores of each retrieval result,\nand then combines them with the given weights.\nIt is uniquer than other retrieval modules, because it does not really execute retrieval,\nbut just fuse the results of other retrieval functions.\nSo you have to run more than two retrieval modules before running this function.\nAnd collect ids and scores result from each retrieval module.\nMake it as tuple and input it to this function.\n\n:param ids: The tuple of ids that you want to fuse.\n    The length of this must be the same as the length of scores.\n    The semantic retrieval ids must be the first index.\n:param scores: The retrieve scores that you want to fuse.\n    The length of this must be the same as the length of ids.\n    The semantic retrieval scores must be the first index.\n:param top_k: The number of passages to be retrieved.\n:param normalize_method: The normalization method to use.\n  There are some normalization method that you can use at the hybrid cc method.\n  AutoRAG support following.\n    - `mm`: Min-max scaling\n    - `tmm`: Theoretical min-max scaling\n    - `z`: z-score normalization\n    - `dbsf`: 3-sigma normalization\n:param weight: The weight value. If the weight is 1.0, it means the\n  weight to the semantic module will be 1.0 and weight to the lexical module will be 0.0.\n:param semantic_theoretical_min_value: This value used by `tmm` normalization method. You can set the\n    theoretical minimum value by yourself. Default is -1.\n:param lexical_theoretical_min_value: This value used by `tmm` normalization method. You can set the\n    theoretical minimum value by yourself. Default is 0.\n:return: The tuple of ids and fused scores that fused by CC. Plus, the third element is selected weight value."
  },
  {
    "function": "normalize_dbsf",
    "module": "autorag.nodes.retrieval.hybrid_cc",
    "params": [
      {
        "name": "scores",
        "param_type": "typing.List[str]",
        "description": ""
      },
      {
        "name": "fixed_min_value",
        "param_type": "<class 'float'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "normalize_mm",
    "module": "autorag.nodes.retrieval.hybrid_cc",
    "params": [
      {
        "name": "scores",
        "param_type": "typing.List[str]",
        "description": ""
      },
      {
        "name": "fixed_min_value",
        "param_type": "<class 'float'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "normalize_tmm",
    "module": "autorag.nodes.retrieval.hybrid_cc",
    "params": [
      {
        "name": "scores",
        "param_type": "typing.List[str]",
        "description": ""
      },
      {
        "name": "fixed_min_value",
        "param_type": "<class 'float'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "normalize_z",
    "module": "autorag.nodes.retrieval.hybrid_cc",
    "params": [
      {
        "name": "scores",
        "param_type": "typing.List[str]",
        "description": ""
      },
      {
        "name": "fixed_min_value",
        "param_type": "<class 'float'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "retrieval_node",
    "module": "autorag.nodes.retrieval.base",
    "params": [
      {
        "name": "func",
        "param_type": "Any",
        "description": "Retrieval function that returns a list of ids and a list of scores"
      }
    ],
    "return_type": "Any",
    "docstring": "Load resources for running retrieval_node.\nFor example, it loads bm25 corpus for bm25 retrieval.\n\n:param func: Retrieval function that returns a list of ids and a list of scores\n:return: A pandas Dataframe that contains retrieved contents, retrieved ids, and retrieve scores.\n    The column name will be \"retrieved_contents\", \"retrieved_ids\", and \"retrieve_scores\"."
  },
  {
    "function": "hybrid_rrf",
    "module": "autorag.nodes.retrieval.hybrid_rrf",
    "params": [
      {
        "name": "ids",
        "param_type": "typing.Tuple",
        "description": "The tuple of ids that you want to fuse.\n    The length of this must be the same as the length of scores."
      },
      {
        "name": "scores",
        "param_type": "typing.Tuple",
        "description": "The retrieve scores that you want to fuse.\n    The length of this must be the same as the length of ids."
      },
      {
        "name": "top_k",
        "param_type": "<class 'int'>",
        "description": "The number of passages to be retrieved."
      },
      {
        "name": "weight",
        "param_type": "<class 'int'>",
        "description": "Hyperparameter for RRF.\n    It was originally rrf_k value.\n    Default is 60.\n    For more information, please visit our documentation."
      },
      {
        "name": "rrf_k",
        "param_type": "<class 'int'>",
        "description": "(Deprecated) Hyperparameter for RRF.\n    It was originally rrf_k value. Will remove at further version."
      }
    ],
    "return_type": "typing.Tuple[typing.List[typing.List[str]], typing.List[typing.List[float]]]",
    "docstring": "Hybrid RRF function.\nRRF (Rank Reciprocal Fusion) is a method to fuse multiple retrieval results.\nIt is common to fuse dense retrieval and sparse retrieval results using RRF.\nTo use this function, you must input ids and scores as tuple.\nIt is uniquer than other retrieval modules, because it does not really execute retrieval,\nbut just fuse the results of other retrieval functions.\nSo you have to run more than two retrieval modules before running this function.\nAnd collect ids and scores result from each retrieval module.\nMake it as tuple and input it to this function.\n\n:param ids: The tuple of ids that you want to fuse.\n    The length of this must be the same as the length of scores.\n:param scores: The retrieve scores that you want to fuse.\n    The length of this must be the same as the length of ids.\n:param top_k: The number of passages to be retrieved.\n:param weight: Hyperparameter for RRF.\n    It was originally rrf_k value.\n    Default is 60.\n    For more information, please visit our documentation.\n:param rrf_k: (Deprecated) Hyperparameter for RRF.\n    It was originally rrf_k value. Will remove at further version.\n:return: The tuple of ids and fused scores that fused by RRF."
  },
  {
    "function": "retrieval_node",
    "module": "autorag.nodes.retrieval.base",
    "params": [
      {
        "name": "func",
        "param_type": "Any",
        "description": "Retrieval function that returns a list of ids and a list of scores"
      }
    ],
    "return_type": "Any",
    "docstring": "Load resources for running retrieval_node.\nFor example, it loads bm25 corpus for bm25 retrieval.\n\n:param func: Retrieval function that returns a list of ids and a list of scores\n:return: A pandas Dataframe that contains retrieved contents, retrieved ids, and retrieve scores.\n    The column name will be \"retrieved_contents\", \"retrieved_ids\", and \"retrieve_scores\"."
  },
  {
    "function": "rrf_calculate",
    "module": "autorag.nodes.retrieval.hybrid_rrf",
    "params": [
      {
        "name": "row",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "rrf_k",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "rrf_pure",
    "module": "autorag.nodes.retrieval.hybrid_rrf",
    "params": [
      {
        "name": "ids",
        "param_type": "typing.Tuple",
        "description": ""
      },
      {
        "name": "scores",
        "param_type": "typing.Tuple",
        "description": ""
      },
      {
        "name": "rrf_k",
        "param_type": "<class 'int'>",
        "description": ""
      },
      {
        "name": "top_k",
        "param_type": "<class 'int'>",
        "description": ""
      }
    ],
    "return_type": "typing.Tuple[typing.List[str], typing.List[float]]",
    "docstring": null
  },
  {
    "function": "edit_summary_df_params",
    "module": "autorag.nodes.retrieval.run",
    "params": [
      {
        "name": "summary_df",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": ""
      },
      {
        "name": "target_modules",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "target_module_params",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "<class 'pandas.core.frame.DataFrame'>",
    "docstring": null
  },
  {
    "function": "evaluate_retrieval",
    "module": "autorag.evaluation.retrieval",
    "params": [
      {
        "name": "retrieval_gt",
        "param_type": "typing.List[typing.List[typing.List[str]]]",
        "description": ""
      },
      {
        "name": "metrics",
        "param_type": "typing.Union[typing.List[str], typing.List[typing.Dict]]",
        "description": ""
      },
      {
        "name": "queries",
        "param_type": "typing.Optional[typing.List[str]]",
        "description": ""
      },
      {
        "name": "generation_gt",
        "param_type": "typing.Optional[typing.List[typing.List[str]]]",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "evaluate_retrieval_node",
    "module": "autorag.nodes.retrieval.run",
    "params": [
      {
        "name": "result_df",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": "The result dataframe from a retrieval node."
      },
      {
        "name": "retrieval_gt",
        "param_type": "Any",
        "description": "Ground truth for retrieval from qa dataset."
      },
      {
        "name": "metrics",
        "param_type": "Any",
        "description": "Metric list from input strategies."
      },
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": "Query list from input strategies."
      },
      {
        "name": "generation_gt",
        "param_type": "typing.List[typing.List[str]]",
        "description": "Ground truth for generation from qa dataset."
      }
    ],
    "return_type": "<class 'pandas.core.frame.DataFrame'>",
    "docstring": "Evaluate retrieval node from retrieval node result dataframe.\n\n:param result_df: The result dataframe from a retrieval node.\n:param retrieval_gt: Ground truth for retrieval from qa dataset.\n:param metrics: Metric list from input strategies.\n:param queries: Query list from input strategies.\n:param generation_gt: Ground truth for generation from qa dataset.\n:return: Return result_df with metrics columns.\n    The columns will be 'retrieved_contents', 'retrieved_ids', 'retrieve_scores', and metric names."
  },
  {
    "function": "filter_by_threshold",
    "module": "autorag.strategy",
    "params": [
      {
        "name": "results",
        "param_type": "Any",
        "description": "The result list to be filtered."
      },
      {
        "name": "value",
        "param_type": "Any",
        "description": "The value list to be filtered.\n    It must have the same length with results."
      },
      {
        "name": "threshold",
        "param_type": "Any",
        "description": "The threshold value."
      },
      {
        "name": "metadatas",
        "param_type": "Any",
        "description": "The metadata of each result."
      }
    ],
    "return_type": "typing.Tuple[typing.List, typing.List]",
    "docstring": "Filter results by value's threshold.\n\n:param results: The result list to be filtered.\n:param value: The value list to be filtered.\n    It must have the same length with results.\n:param threshold: The threshold value.\n:param metadatas: The metadata of each result.\n:return: Filtered list of results and filtered list of metadatas.\n    Metadatas will be returned even if you did not give input metadatas.\n:rtype: Tuple[List, List]"
  },
  {
    "function": "get_hybrid_execution_times",
    "module": "autorag.nodes.retrieval.run",
    "params": [
      {
        "name": "lexical_summary",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "semantic_summary",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "<class 'float'>",
    "docstring": null
  },
  {
    "function": "get_ids_and_scores",
    "module": "autorag.nodes.retrieval.run",
    "params": [
      {
        "name": "node_dir",
        "param_type": "<class 'str'>",
        "description": ""
      },
      {
        "name": "filenames",
        "param_type": "typing.List[str]",
        "description": ""
      }
    ],
    "return_type": "typing.Dict",
    "docstring": null
  },
  {
    "function": "measure_speed",
    "module": "autorag.strategy",
    "params": [
      {
        "name": "func",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "args",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "kwargs",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Method for measuring execution speed of the function."
  },
  {
    "function": "optimize_hybrid",
    "module": "autorag.nodes.retrieval.run",
    "params": [
      {
        "name": "hybrid_module_func",
        "param_type": "typing.Callable",
        "description": ""
      },
      {
        "name": "hybrid_module_param",
        "param_type": "typing.Dict",
        "description": ""
      },
      {
        "name": "strategy",
        "param_type": "typing.Dict",
        "description": ""
      },
      {
        "name": "retrieval_gt",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "qa_df",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": ""
      },
      {
        "name": "project_dir",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "previous_result",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "run_retrieval_node",
    "module": "autorag.nodes.retrieval.run",
    "params": [
      {
        "name": "modules",
        "param_type": "typing.List[typing.Callable]",
        "description": "Retrieval modules to run."
      },
      {
        "name": "module_params",
        "param_type": "typing.List[typing.Dict]",
        "description": "Retrieval module parameters."
      },
      {
        "name": "previous_result",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": "Previous result dataframe.\n    Could be query expansion's best result or qa data."
      },
      {
        "name": "node_line_dir",
        "param_type": "<class 'str'>",
        "description": "This node line's directory."
      },
      {
        "name": "strategies",
        "param_type": "typing.Dict",
        "description": "Strategies for retrieval node."
      }
    ],
    "return_type": "<class 'pandas.core.frame.DataFrame'>",
    "docstring": "Run evaluation and select the best module among retrieval node results.\n\n:param modules: Retrieval modules to run.\n:param module_params: Retrieval module parameters.\n:param previous_result: Previous result dataframe.\n    Could be query expansion's best result or qa data.\n:param node_line_dir: This node line's directory.\n:param strategies: Strategies for retrieval node.\n:return: The best result dataframe.\n    It contains previous result columns and retrieval node's result columns."
  },
  {
    "function": "select_best",
    "module": "autorag.strategy",
    "params": [
      {
        "name": "results",
        "param_type": "typing.List[pandas.core.frame.DataFrame]",
        "description": ""
      },
      {
        "name": "columns",
        "param_type": "typing.Iterable[str]",
        "description": ""
      },
      {
        "name": "metadatas",
        "param_type": "typing.Optional[typing.List[typing.Any]]",
        "description": ""
      },
      {
        "name": "strategy_name",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "typing.Tuple[pandas.core.frame.DataFrame, typing.Any]",
    "docstring": null
  },
  {
    "function": "evenly_distribute_passages",
    "module": "autorag.nodes.retrieval.base",
    "params": [
      {
        "name": "ids",
        "param_type": "typing.List[typing.List[str]]",
        "description": ""
      },
      {
        "name": "scores",
        "param_type": "typing.List[typing.List[float]]",
        "description": ""
      },
      {
        "name": "top_k",
        "param_type": "<class 'int'>",
        "description": ""
      }
    ],
    "return_type": "typing.Tuple[typing.List[str], typing.List[float]]",
    "docstring": null
  },
  {
    "function": "openai_truncate_by_token",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "texts",
        "param_type": "typing.List[str]",
        "description": ""
      },
      {
        "name": "token_limit",
        "param_type": "<class 'int'>",
        "description": ""
      },
      {
        "name": "model_name",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "typing.List[str]",
    "docstring": null
  },
  {
    "function": "process_batch",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "tasks",
        "param_type": "Any",
        "description": "A list of no-argument functions or coroutines to be executed."
      },
      {
        "name": "batch_size",
        "param_type": "<class 'int'>",
        "description": "The number of tasks to process in a single batch.\n    Default is 64."
      }
    ],
    "return_type": "typing.List[typing.Any]",
    "docstring": "Processes tasks in batches asynchronously.\n\n:param tasks: A list of no-argument functions or coroutines to be executed.\n:param batch_size: The number of tasks to process in a single batch.\n    Default is 64.\n:return: A list of results from the processed tasks."
  },
  {
    "function": "retrieval_node",
    "module": "autorag.nodes.retrieval.base",
    "params": [
      {
        "name": "func",
        "param_type": "Any",
        "description": "Retrieval function that returns a list of ids and a list of scores"
      }
    ],
    "return_type": "Any",
    "docstring": "Load resources for running retrieval_node.\nFor example, it loads bm25 corpus for bm25 retrieval.\n\n:param func: Retrieval function that returns a list of ids and a list of scores\n:return: A pandas Dataframe that contains retrieved contents, retrieved ids, and retrieve scores.\n    The column name will be \"retrieved_contents\", \"retrieved_ids\", and \"retrieve_scores\"."
  },
  {
    "function": "validate_corpus_dataset",
    "module": "autorag.utils.preprocess",
    "params": [
      {
        "name": "df",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "vectordb",
    "module": "autorag.nodes.retrieval.vectordb",
    "params": [
      {
        "name": "queries",
        "param_type": "typing.List[typing.List[str]]",
        "description": "2-d list of query strings.\n    Each element of the list is a query strings of each row."
      },
      {
        "name": "top_k",
        "param_type": "<class 'int'>",
        "description": "The number of passages to be retrieved."
      },
      {
        "name": "collection",
        "param_type": "<class 'chromadb.api.models.Collection.Collection'>",
        "description": "A chroma collection instance that will be used to retrieve passages."
      },
      {
        "name": "embedding_model",
        "param_type": "<class 'llama_index.core.base.embeddings.base.BaseEmbedding'>",
        "description": "An embedding model instance that will be used to embed queries."
      },
      {
        "name": "embedding_batch",
        "param_type": "<class 'int'>",
        "description": "The number of queries to be processed in parallel.\n    This is used to prevent API error at the query embedding.\n    Default is 128."
      }
    ],
    "return_type": "typing.Tuple[typing.List[typing.List[str]], typing.List[typing.List[float]]]",
    "docstring": "VectorDB retrieval function.\nYou have to get a chroma collection that is already ingested.\nYou have to get an embedding model that is already used in ingesting.\n\n:param queries: 2-d list of query strings.\n    Each element of the list is a query strings of each row.\n:param top_k: The number of passages to be retrieved.\n:param collection: A chroma collection instance that will be used to retrieve passages.\n:param embedding_model: An embedding model instance that will be used to embed queries.\n:param embedding_batch: The number of queries to be processed in parallel.\n    This is used to prevent API error at the query embedding.\n    Default is 128.\n\n:return: The 2-d list contains a list of passage ids that retrieved from vectordb and 2-d list of its scores.\n    It will be a length of queries. And each element has a length of top_k."
  },
  {
    "function": "vectordb_ingest",
    "module": "autorag.nodes.retrieval.vectordb",
    "params": [
      {
        "name": "collection",
        "param_type": "<class 'chromadb.api.models.Collection.Collection'>",
        "description": ""
      },
      {
        "name": "corpus_data",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": ""
      },
      {
        "name": "embedding_model",
        "param_type": "<class 'llama_index.core.base.embeddings.base.BaseEmbedding'>",
        "description": ""
      },
      {
        "name": "embedding_batch",
        "param_type": "<class 'int'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "vectordb_pure",
    "module": "autorag.nodes.retrieval.vectordb",
    "params": [
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": "A list of query strings."
      },
      {
        "name": "top_k",
        "param_type": "<class 'int'>",
        "description": "The number of passages to be retrieved."
      },
      {
        "name": "collection",
        "param_type": "<class 'chromadb.api.models.Collection.Collection'>",
        "description": "A chroma collection instance that will be used to retrieve passages."
      },
      {
        "name": "embedding_model",
        "param_type": "<class 'llama_index.core.base.embeddings.base.BaseEmbedding'>",
        "description": "An embedding model instance that will be used to embed queries."
      }
    ],
    "return_type": "typing.Tuple[typing.List[str], typing.List[float]]",
    "docstring": "Async VectorDB retrieval function.\nIts usage is for async retrieval of vector_db row by row.\n\n:param queries: A list of query strings.\n:param top_k: The number of passages to be retrieved.\n:param collection: A chroma collection instance that will be used to retrieve passages.\n:param embedding_model: An embedding model instance that will be used to embed queries.\n\n:return: The tuple contains a list of passage ids that retrieved from vectordb and a list of its scores."
  },
  {
    "function": "Module.__eq__",
    "module": "autorag.schema.module",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "other",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Return self==value."
  },
  {
    "function": "Module.__init__",
    "module": "autorag.schema.module",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "module_type",
        "param_type": "<class 'str'>",
        "description": ""
      },
      {
        "name": "module_param",
        "param_type": "typing.Dict",
        "description": ""
      }
    ],
    "return_type": "None",
    "docstring": "Initialize self.  See help(type(self)) for accurate signature."
  },
  {
    "function": "Module.__post_init__",
    "module": "autorag.schema.module",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "Module.__repr__",
    "module": "autorag.schema.module",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Return repr(self)."
  },
  {
    "function": "Module.from_dict",
    "module": "autorag.schema.module",
    "params": [
      {
        "name": "module_dict",
        "param_type": "typing.Dict",
        "description": ""
      }
    ],
    "return_type": "Module",
    "docstring": null
  },
  {
    "function": "Node.__eq__",
    "module": "autorag.schema.node",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "other",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Return self==value."
  },
  {
    "function": "Node.__init__",
    "module": "autorag.schema.node",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "node_type",
        "param_type": "<class 'str'>",
        "description": ""
      },
      {
        "name": "strategy",
        "param_type": "typing.Dict",
        "description": ""
      },
      {
        "name": "node_params",
        "param_type": "typing.Dict",
        "description": ""
      },
      {
        "name": "modules",
        "param_type": "typing.List[autorag.schema.module.Module]",
        "description": ""
      }
    ],
    "return_type": "None",
    "docstring": "Initialize self.  See help(type(self)) for accurate signature."
  },
  {
    "function": "Node.__post_init__",
    "module": "autorag.schema.node",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "Node.__repr__",
    "module": "autorag.schema.node",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Return repr(self)."
  },
  {
    "function": "Node.from_dict",
    "module": "autorag.schema.node",
    "params": [
      {
        "name": "node_dict",
        "param_type": "typing.Dict",
        "description": ""
      }
    ],
    "return_type": "Node",
    "docstring": null
  },
  {
    "function": "Node.get_param_combinations",
    "module": "autorag.schema.node",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "typing.Tuple[typing.List[typing.Callable], typing.List[typing.Dict]]",
    "docstring": "This method returns a combination of module and node parameters, also corresponding modules.\n\n:return: Each module and its module parameters.\n:rtype: Tuple[List[Callable], List[Dict]]"
  },
  {
    "function": "Node.run",
    "module": "autorag.schema.node",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "previous_result",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": ""
      },
      {
        "name": "node_line_dir",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "<class 'pandas.core.frame.DataFrame'>",
    "docstring": null
  },
  {
    "function": "Module.__eq__",
    "module": "autorag.schema.module",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "other",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Return self==value."
  },
  {
    "function": "Module.__init__",
    "module": "autorag.schema.module",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "module_type",
        "param_type": "<class 'str'>",
        "description": ""
      },
      {
        "name": "module_param",
        "param_type": "typing.Dict",
        "description": ""
      }
    ],
    "return_type": "None",
    "docstring": "Initialize self.  See help(type(self)) for accurate signature."
  },
  {
    "function": "Module.__post_init__",
    "module": "autorag.schema.module",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "Module.__repr__",
    "module": "autorag.schema.module",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Return repr(self)."
  },
  {
    "function": "Module.from_dict",
    "module": "autorag.schema.module",
    "params": [
      {
        "name": "module_dict",
        "param_type": "typing.Dict",
        "description": ""
      }
    ],
    "return_type": "Module",
    "docstring": null
  },
  {
    "function": "get_support_modules",
    "module": "autorag.support",
    "params": [
      {
        "name": "module_name",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "typing.Callable",
    "docstring": null
  },
  {
    "function": "Module.__eq__",
    "module": "autorag.schema.module",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "other",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Return self==value."
  },
  {
    "function": "Module.__init__",
    "module": "autorag.schema.module",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "module_type",
        "param_type": "<class 'str'>",
        "description": ""
      },
      {
        "name": "module_param",
        "param_type": "typing.Dict",
        "description": ""
      }
    ],
    "return_type": "None",
    "docstring": "Initialize self.  See help(type(self)) for accurate signature."
  },
  {
    "function": "Module.__post_init__",
    "module": "autorag.schema.module",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "Module.__repr__",
    "module": "autorag.schema.module",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Return repr(self)."
  },
  {
    "function": "Module.from_dict",
    "module": "autorag.schema.module",
    "params": [
      {
        "name": "module_dict",
        "param_type": "typing.Dict",
        "description": ""
      }
    ],
    "return_type": "Module",
    "docstring": null
  },
  {
    "function": "Node.__eq__",
    "module": "autorag.schema.node",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "other",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Return self==value."
  },
  {
    "function": "Node.__init__",
    "module": "autorag.schema.node",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "node_type",
        "param_type": "<class 'str'>",
        "description": ""
      },
      {
        "name": "strategy",
        "param_type": "typing.Dict",
        "description": ""
      },
      {
        "name": "node_params",
        "param_type": "typing.Dict",
        "description": ""
      },
      {
        "name": "modules",
        "param_type": "typing.List[autorag.schema.module.Module]",
        "description": ""
      }
    ],
    "return_type": "None",
    "docstring": "Initialize self.  See help(type(self)) for accurate signature."
  },
  {
    "function": "Node.__post_init__",
    "module": "autorag.schema.node",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "Node.__repr__",
    "module": "autorag.schema.node",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Return repr(self)."
  },
  {
    "function": "Node.from_dict",
    "module": "autorag.schema.node",
    "params": [
      {
        "name": "node_dict",
        "param_type": "typing.Dict",
        "description": ""
      }
    ],
    "return_type": "Node",
    "docstring": null
  },
  {
    "function": "Node.get_param_combinations",
    "module": "autorag.schema.node",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "typing.Tuple[typing.List[typing.Callable], typing.List[typing.Dict]]",
    "docstring": "This method returns a combination of module and node parameters, also corresponding modules.\n\n:return: Each module and its module parameters.\n:rtype: Tuple[List[Callable], List[Dict]]"
  },
  {
    "function": "Node.run",
    "module": "autorag.schema.node",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "previous_result",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": ""
      },
      {
        "name": "node_line_dir",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "<class 'pandas.core.frame.DataFrame'>",
    "docstring": null
  },
  {
    "function": "explode",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "index_values",
        "param_type": "typing.Collection[typing.Any]",
        "description": "The index values."
      },
      {
        "name": "explode_values",
        "param_type": "typing.Collection[typing.Collection[typing.Any]]",
        "description": "The exploded values."
      }
    ],
    "return_type": "Any",
    "docstring": "Explode index_values and explode_values.\nThe index_values and explode_values must have the same length.\nIt will flatten explode_values and keep index_values as a pair.\n\n:param index_values: The index values.\n:param explode_values: The exploded values.\n:return: Tuple of exploded index_values and exploded explode_values."
  },
  {
    "function": "extract_values",
    "module": "autorag.schema.node",
    "params": [
      {
        "name": "node",
        "param_type": "<class 'autorag.schema.node.Node'>",
        "description": "The node you want to extract values from."
      },
      {
        "name": "key",
        "param_type": "<class 'str'>",
        "description": "The key of module_param that you want to extract."
      }
    ],
    "return_type": "typing.List[str]",
    "docstring": "This function extract values from node's modules' module_param.\n\n:param node: The node you want to extract values from.\n:param key: The key of module_param that you want to extract.\n:return: The list of extracted values.\n    It removes duplicated elements automatically."
  },
  {
    "function": "extract_values_from_nodes",
    "module": "autorag.schema.node",
    "params": [
      {
        "name": "nodes",
        "param_type": "typing.List[autorag.schema.node.Node]",
        "description": "The nodes you want to extract values from."
      },
      {
        "name": "key",
        "param_type": "<class 'str'>",
        "description": "The key of module_param that you want to extract."
      }
    ],
    "return_type": "typing.List[str]",
    "docstring": "This function extract values from nodes' modules' module_param.\n\n:param nodes: The nodes you want to extract values from.\n:param key: The key of module_param that you want to extract.\n:return: The list of extracted values.\n    It removes duplicated elements automatically."
  },
  {
    "function": "get_support_nodes",
    "module": "autorag.support",
    "params": [
      {
        "name": "node_name",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "typing.Callable",
    "docstring": null
  },
  {
    "function": "make_combinations",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "target_dict",
        "param_type": "typing.Dict[str, typing.Any]",
        "description": "The target dictionary."
      }
    ],
    "return_type": "typing.List[typing.Dict[str, typing.Any]]",
    "docstring": "Make combinations from target_dict.\nThe target_dict key value must be a string,\nand the value can be list of values or single value.\nIf generates all combinations of values from target_dict,\nwhich means generated dictionaries that contain only one value for each key,\nand all dictionaries will be different from each other.\n\n:param target_dict: The target dictionary.\n:return: The list of generated dictionaries."
  },
  {
    "function": "module_type_exists",
    "module": "autorag.schema.node",
    "params": [
      {
        "name": "nodes",
        "param_type": "typing.List[autorag.schema.node.Node]",
        "description": "The nodes you want to check."
      },
      {
        "name": "module_type",
        "param_type": "<class 'str'>",
        "description": "The module type you want to check."
      }
    ],
    "return_type": "<class 'bool'>",
    "docstring": "This function check if the module type exists in the nodes.\n\n:param nodes: The nodes you want to check.\n:param module_type: The module type you want to check.\n:return: True if the module type exists in the nodes."
  },
  {
    "function": "avoid_empty_result",
    "module": "autorag.strategy",
    "params": [
      {
        "name": "return_index",
        "param_type": "typing.List[int]",
        "description": "The index of the result to be returned when there is no result."
      }
    ],
    "return_type": "Any",
    "docstring": "Decorator for avoiding empty results from the function.\nWhen the func returns an empty result, it will return the origin results.\nWhen the func returns a None, it will return the origin results.\nWhen the return value is a tuple, it will check all the value or list is empty.\nIf so, it will return the origin results.\nIt keeps parameters at return_index of the function as the origin results.\n\n:param return_index: The index of the result to be returned when there is no result.\n:return: The origin results or the results from the function."
  },
  {
    "function": "filter_by_threshold",
    "module": "autorag.strategy",
    "params": [
      {
        "name": "results",
        "param_type": "Any",
        "description": "The result list to be filtered."
      },
      {
        "name": "value",
        "param_type": "Any",
        "description": "The value list to be filtered.\n    It must have the same length with results."
      },
      {
        "name": "threshold",
        "param_type": "Any",
        "description": "The threshold value."
      },
      {
        "name": "metadatas",
        "param_type": "Any",
        "description": "The metadata of each result."
      }
    ],
    "return_type": "typing.Tuple[typing.List, typing.List]",
    "docstring": "Filter results by value's threshold.\n\n:param results: The result list to be filtered.\n:param value: The value list to be filtered.\n    It must have the same length with results.\n:param threshold: The threshold value.\n:param metadatas: The metadata of each result.\n:return: Filtered list of results and filtered list of metadatas.\n    Metadatas will be returned even if you did not give input metadatas.\n:rtype: Tuple[List, List]"
  },
  {
    "function": "measure_speed",
    "module": "autorag.strategy",
    "params": [
      {
        "name": "func",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "args",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "kwargs",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Method for measuring execution speed of the function."
  },
  {
    "function": "select_best",
    "module": "autorag.strategy",
    "params": [
      {
        "name": "results",
        "param_type": "typing.List[pandas.core.frame.DataFrame]",
        "description": ""
      },
      {
        "name": "columns",
        "param_type": "typing.Iterable[str]",
        "description": ""
      },
      {
        "name": "metadatas",
        "param_type": "typing.Optional[typing.List[typing.Any]]",
        "description": ""
      },
      {
        "name": "strategy_name",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "typing.Tuple[pandas.core.frame.DataFrame, typing.Any]",
    "docstring": null
  },
  {
    "function": "select_best_average",
    "module": "autorag.strategy",
    "params": [
      {
        "name": "results",
        "param_type": "typing.List[pandas.core.frame.DataFrame]",
        "description": "The list of results.\n    Each result must be pd.DataFrame."
      },
      {
        "name": "columns",
        "param_type": "typing.Iterable[str]",
        "description": "Column names to be averaged.\n    Standard to select the best result."
      },
      {
        "name": "metadatas",
        "param_type": "typing.Optional[typing.List[typing.Any]]",
        "description": "The metadata of each result. \n    It will select one metadata with the best result."
      }
    ],
    "return_type": "typing.Tuple[pandas.core.frame.DataFrame, typing.Any]",
    "docstring": "Select the best result by average value among given columns.\n\n:param results: The list of results.\n    Each result must be pd.DataFrame.\n:param columns: Column names to be averaged.\n    Standard to select the best result.\n:param metadatas: The metadata of each result. \n    It will select one metadata with the best result.\n:return: The best result and the best metadata.\n    The metadata will be returned even if you did not give input 'metadatas' parameter.\n:rtype: Tuple[pd.DataFrame, Any]"
  },
  {
    "function": "select_best_rr",
    "module": "autorag.strategy",
    "params": [
      {
        "name": "results",
        "param_type": "typing.List[pandas.core.frame.DataFrame]",
        "description": ""
      },
      {
        "name": "columns",
        "param_type": "typing.Iterable[str]",
        "description": ""
      },
      {
        "name": "metadatas",
        "param_type": "typing.Optional[typing.List[typing.Any]]",
        "description": ""
      }
    ],
    "return_type": "typing.Tuple[pandas.core.frame.DataFrame, typing.Any]",
    "docstring": null
  },
  {
    "function": "select_normalize_mean",
    "module": "autorag.strategy",
    "params": [
      {
        "name": "results",
        "param_type": "typing.List[pandas.core.frame.DataFrame]",
        "description": ""
      },
      {
        "name": "columns",
        "param_type": "typing.Iterable[str]",
        "description": ""
      },
      {
        "name": "metadatas",
        "param_type": "typing.Optional[typing.List[typing.Any]]",
        "description": ""
      }
    ],
    "return_type": "typing.Tuple[pandas.core.frame.DataFrame, typing.Any]",
    "docstring": null
  },
  {
    "function": "validate_strategy_inputs",
    "module": "autorag.strategy",
    "params": [
      {
        "name": "results",
        "param_type": "typing.List[pandas.core.frame.DataFrame]",
        "description": ""
      },
      {
        "name": "columns",
        "param_type": "typing.Iterable[str]",
        "description": ""
      },
      {
        "name": "metadatas",
        "param_type": "typing.Optional[typing.List[typing.Any]]",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "dynamically_find_function",
    "module": "autorag.support",
    "params": [
      {
        "name": "key",
        "param_type": "<class 'str'>",
        "description": ""
      },
      {
        "name": "target_dict",
        "param_type": "typing.Dict",
        "description": ""
      }
    ],
    "return_type": "typing.Callable",
    "docstring": null
  },
  {
    "function": "get_support_modules",
    "module": "autorag.support",
    "params": [
      {
        "name": "module_name",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "typing.Callable",
    "docstring": null
  },
  {
    "function": "get_support_nodes",
    "module": "autorag.support",
    "params": [
      {
        "name": "node_name",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "typing.Callable",
    "docstring": null
  },
  {
    "function": "cast_corpus_dataset",
    "module": "autorag.utils.preprocess",
    "params": [
      {
        "name": "df",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "cast_qa_dataset",
    "module": "autorag.utils.preprocess",
    "params": [
      {
        "name": "df",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "fetch_contents",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "corpus_data",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": ""
      },
      {
        "name": "ids",
        "param_type": "typing.List[typing.List[str]]",
        "description": ""
      },
      {
        "name": "column_name",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "typing.List[typing.List[typing.Any]]",
    "docstring": null
  },
  {
    "function": "result_to_dataframe",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "column_names",
        "param_type": "typing.List[str]",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Decorator for converting results to pd.DataFrame."
  },
  {
    "function": "sort_by_scores",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "row",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "reverse",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Sorts each row by 'scores' column.\nThe input column names must be 'contents', 'ids', and 'scores'.\nAnd its elements must be list type."
  },
  {
    "function": "validate_corpus_dataset",
    "module": "autorag.utils.preprocess",
    "params": [
      {
        "name": "df",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "validate_qa_dataset",
    "module": "autorag.utils.preprocess",
    "params": [
      {
        "name": "df",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "validate_qa_from_corpus_dataset",
    "module": "autorag.utils.preprocess",
    "params": [
      {
        "name": "qa_df",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": ""
      },
      {
        "name": "corpus_df",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "cast_corpus_dataset",
    "module": "autorag.utils.preprocess",
    "params": [
      {
        "name": "df",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "cast_qa_dataset",
    "module": "autorag.utils.preprocess",
    "params": [
      {
        "name": "df",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "normalize_unicode",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "text",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "<class 'str'>",
    "docstring": null
  },
  {
    "function": "validate_corpus_dataset",
    "module": "autorag.utils.preprocess",
    "params": [
      {
        "name": "df",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "validate_qa_dataset",
    "module": "autorag.utils.preprocess",
    "params": [
      {
        "name": "df",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "validate_qa_from_corpus_dataset",
    "module": "autorag.utils.preprocess",
    "params": [
      {
        "name": "qa_df",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": ""
      },
      {
        "name": "corpus_df",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "convert_datetime_string",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "s",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "convert_env_in_dict",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "d",
        "param_type": "typing.Dict",
        "description": "The dictionary to convert."
      }
    ],
    "return_type": "Any",
    "docstring": "Recursively converts environment variable string in a dictionary to actual environment variable.\n\n:param d: The dictionary to convert.\n:return: The converted dictionary."
  },
  {
    "function": "convert_inputs_to_list",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "func",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Decorator to convert all function inputs to Python lists."
  },
  {
    "function": "convert_string_to_tuple_in_dict",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "d",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Recursively converts strings that start with '(' and end with ')' to tuples in a dictionary."
  },
  {
    "function": "dict_to_markdown",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "d",
        "param_type": "Any",
        "description": "Dictionary to convert"
      },
      {
        "name": "level",
        "param_type": "Any",
        "description": "Current level of heading (used for nested dictionaries)"
      }
    ],
    "return_type": "Any",
    "docstring": "Convert a dictionary to a Markdown formatted string.\n\n:param d: Dictionary to convert\n:param level: Current level of heading (used for nested dictionaries)\n:return: Markdown formatted string"
  },
  {
    "function": "dict_to_markdown_table",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "data",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "key_column_name",
        "param_type": "<class 'str'>",
        "description": ""
      },
      {
        "name": "value_column_name",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "embedding_query_content",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "queries",
        "param_type": "typing.List[str]",
        "description": ""
      },
      {
        "name": "contents_list",
        "param_type": "typing.List[typing.List[str]]",
        "description": ""
      },
      {
        "name": "embedding_model",
        "param_type": "typing.Optional[str]",
        "description": ""
      },
      {
        "name": "batch",
        "param_type": "<class 'int'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "explode",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "index_values",
        "param_type": "typing.Collection[typing.Any]",
        "description": "The index values."
      },
      {
        "name": "explode_values",
        "param_type": "typing.Collection[typing.Collection[typing.Any]]",
        "description": "The exploded values."
      }
    ],
    "return_type": "Any",
    "docstring": "Explode index_values and explode_values.\nThe index_values and explode_values must have the same length.\nIt will flatten explode_values and keep index_values as a pair.\n\n:param index_values: The index values.\n:param explode_values: The exploded values.\n:return: Tuple of exploded index_values and exploded explode_values."
  },
  {
    "function": "fetch_contents",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "corpus_data",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": ""
      },
      {
        "name": "ids",
        "param_type": "typing.List[typing.List[str]]",
        "description": ""
      },
      {
        "name": "column_name",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "typing.List[typing.List[typing.Any]]",
    "docstring": null
  },
  {
    "function": "fetch_one_content",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "corpus_data",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": ""
      },
      {
        "name": "id_",
        "param_type": "<class 'str'>",
        "description": ""
      },
      {
        "name": "column_name",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "typing.Any",
    "docstring": null
  },
  {
    "function": "filter_dict_keys",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "dict_",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "keys",
        "param_type": "typing.List[str]",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "find_node_summary_files",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "trial_dir",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "typing.List[str]",
    "docstring": null
  },
  {
    "function": "find_trial_dir",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "project_dir",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "typing.List[str]",
    "docstring": null
  },
  {
    "function": "flatten_apply",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "func",
        "param_type": "typing.Callable",
        "description": "The function that applies to the flattened list."
      },
      {
        "name": "nested_list",
        "param_type": "typing.List[typing.List[typing.Any]]",
        "description": "The nested list to be flattened."
      },
      {
        "name": "kwargs",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "typing.List[typing.List[typing.Any]]",
    "docstring": "This function flattens the input list and applies the function to the elements.\nAfter that, it reconstructs the list to the original shape.\nIts speciality is that the first dimension length of the list can be different from each other.\n\n:param func: The function that applies to the flattened list.\n:param nested_list: The nested list to be flattened.\n:return: The list that is reconstructed after applying the function."
  },
  {
    "function": "load_summary_file",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "summary_path",
        "param_type": "<class 'str'>",
        "description": "The path of the summary file."
      },
      {
        "name": "dict_columns",
        "param_type": "typing.Optional[typing.List[str]]",
        "description": "The columns that are dictionary type.\n    You must fill this parameter if you want to load summary file properly.\n    Default is ['module_params']."
      }
    ],
    "return_type": "<class 'pandas.core.frame.DataFrame'>",
    "docstring": "Load summary file from summary_path.\n\n:param summary_path: The path of the summary file.\n:param dict_columns: The columns that are dictionary type.\n    You must fill this parameter if you want to load summary file properly.\n    Default is ['module_params'].\n:return: The summary dataframe."
  },
  {
    "function": "make_batch",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "elems",
        "param_type": "typing.List[typing.Any]",
        "description": ""
      },
      {
        "name": "batch_size",
        "param_type": "<class 'int'>",
        "description": ""
      }
    ],
    "return_type": "typing.List[typing.List[typing.Any]]",
    "docstring": "Make a batch of elems with batch_size."
  },
  {
    "function": "make_combinations",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "target_dict",
        "param_type": "typing.Dict[str, typing.Any]",
        "description": "The target dictionary."
      }
    ],
    "return_type": "typing.List[typing.Dict[str, typing.Any]]",
    "docstring": "Make combinations from target_dict.\nThe target_dict key value must be a string,\nand the value can be list of values or single value.\nIf generates all combinations of values from target_dict,\nwhich means generated dictionaries that contain only one value for each key,\nand all dictionaries will be different from each other.\n\n:param target_dict: The target dictionary.\n:return: The list of generated dictionaries."
  },
  {
    "function": "normalize_string",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "s",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "<class 'str'>",
    "docstring": "Taken from the official evaluation script for v1.1 of the SQuAD dataset.\nLower text and remove punctuation, articles and extra whitespace."
  },
  {
    "function": "normalize_unicode",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "text",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "<class 'str'>",
    "docstring": null
  },
  {
    "function": "openai_truncate_by_token",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "texts",
        "param_type": "typing.List[str]",
        "description": ""
      },
      {
        "name": "token_limit",
        "param_type": "<class 'int'>",
        "description": ""
      },
      {
        "name": "model_name",
        "param_type": "<class 'str'>",
        "description": ""
      }
    ],
    "return_type": "typing.List[str]",
    "docstring": null
  },
  {
    "function": "process_batch",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "tasks",
        "param_type": "Any",
        "description": "A list of no-argument functions or coroutines to be executed."
      },
      {
        "name": "batch_size",
        "param_type": "<class 'int'>",
        "description": "The number of tasks to process in a single batch.\n    Default is 64."
      }
    ],
    "return_type": "typing.List[typing.Any]",
    "docstring": "Processes tasks in batches asynchronously.\n\n:param tasks: A list of no-argument functions or coroutines to be executed.\n:param batch_size: The number of tasks to process in a single batch.\n    Default is 64.\n:return: A list of results from the processed tasks."
  },
  {
    "function": "reconstruct_list",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "flat_list",
        "param_type": "typing.List[typing.Any]",
        "description": ""
      },
      {
        "name": "lengths",
        "param_type": "typing.List[int]",
        "description": ""
      }
    ],
    "return_type": "typing.List[typing.List[typing.Any]]",
    "docstring": null
  },
  {
    "function": "replace_value_in_dict",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "target_dict",
        "param_type": "typing.Dict",
        "description": "The target dictionary."
      },
      {
        "name": "key",
        "param_type": "<class 'str'>",
        "description": "The key to replace."
      },
      {
        "name": "replace_value",
        "param_type": "typing.Any",
        "description": "The value to replace."
      }
    ],
    "return_type": "typing.Dict",
    "docstring": "Replace the value of the certain key in target_dict.\nIf there is not targeted key in target_dict, it will return target_dict.\n\n:param target_dict: The target dictionary.\n:param key: The key to replace.\n:param replace_value: The value to replace.\n:return: The replaced dictionary."
  },
  {
    "function": "result_to_dataframe",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "column_names",
        "param_type": "typing.List[str]",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Decorator for converting results to pd.DataFrame."
  },
  {
    "function": "save_parquet_safe",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "df",
        "param_type": "<class 'pandas.core.frame.DataFrame'>",
        "description": ""
      },
      {
        "name": "filepath",
        "param_type": "<class 'str'>",
        "description": ""
      },
      {
        "name": "upsert",
        "param_type": "<class 'bool'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "select_top_k",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "df",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "column_names",
        "param_type": "typing.List[str]",
        "description": ""
      },
      {
        "name": "top_k",
        "param_type": "<class 'int'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "sort_by_scores",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "row",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "reverse",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Sorts each row by 'scores' column.\nThe input column names must be 'contents', 'ids', and 'scores'.\nAnd its elements must be list type."
  },
  {
    "function": "split_dataframe",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "df",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "chunk_size",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "to_list",
    "module": "autorag.utils.util",
    "params": [
      {
        "name": "item",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Recursively convert collections to Python lists."
  },
  {
    "function": "Runner.__add_api_route",
    "module": "autorag.deploy",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "Runner.__init__",
    "module": "autorag.deploy",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "config",
        "param_type": "typing.Dict",
        "description": ""
      },
      {
        "name": "project_dir",
        "param_type": "typing.Optional[str]",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": "Initialize self.  See help(type(self)) for accurate signature."
  },
  {
    "function": "Runner.from_trial_folder",
    "module": "autorag.deploy",
    "params": [
      {
        "name": "trial_path",
        "param_type": "<class 'str'>",
        "description": "The path of the trial folder."
      }
    ],
    "return_type": "Any",
    "docstring": "Load Runner from evaluated trial folder.\nMust already be evaluated using Evaluator class.\nIt sets the project_dir as the parent directory of the trial folder.\n\n:param trial_path: The path of the trial folder.\n:return: Initialized Runner."
  },
  {
    "function": "Runner.from_yaml",
    "module": "autorag.deploy",
    "params": [
      {
        "name": "yaml_path",
        "param_type": "<class 'str'>",
        "description": "The path of the yaml file."
      },
      {
        "name": "project_dir",
        "param_type": "typing.Optional[str]",
        "description": "The path of the project directory.\n    Default is the current directory."
      }
    ],
    "return_type": "Any",
    "docstring": "Load Runner from yaml file.\nMust be extracted yaml file from evaluated trial using extract_best_config method.\n\n:param yaml_path: The path of the yaml file.\n:param project_dir: The path of the project directory.\n    Default is the current directory.\n:return: Initialized Runner."
  },
  {
    "function": "Runner.run",
    "module": "autorag.deploy",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "query",
        "param_type": "<class 'str'>",
        "description": "The query of the user."
      },
      {
        "name": "result_column",
        "param_type": "<class 'str'>",
        "description": "The result column name for the answer.\n    Default is `generated_texts`, which is the output of the `generation` module."
      }
    ],
    "return_type": "Any",
    "docstring": "Run the pipeline with query.\nThe loaded pipeline must start with a single query,\nso the first module of the pipeline must be `query_expansion` or `retrieval` module.\n\n:param query: The query of the user.\n:param result_column: The result column name for the answer.\n    Default is `generated_texts`, which is the output of the `generation` module.\n:return: The result of the pipeline."
  },
  {
    "function": "Runner.run_api_server",
    "module": "autorag.deploy",
    "params": [
      {
        "name": "self",
        "param_type": "Any",
        "description": ""
      },
      {
        "name": "host",
        "param_type": "<class 'str'>",
        "description": "The host of the api server."
      },
      {
        "name": "port",
        "param_type": "<class 'int'>",
        "description": "The port of the api server."
      },
      {
        "name": "kwargs",
        "param_type": "Any",
        "description": "Other arguments for uvicorn.run."
      }
    ],
    "return_type": "Any",
    "docstring": "Run the pipeline as api server.\nYou can send POST request to `http://host:port/run` with json body like below:\n\n.. Code:: json\n\n    {\n        \"Query\": \"your query\",\n        \"result_column\": \"answer\"\n    }\n\nAnd it returns json response like below:\n\n.. Code:: json\n\n    {\n        \"answer\": \"your answer\"\n    }\n\n:param host: The host of the api server.\n:param port: The port of the api server.\n:param kwargs: Other arguments for uvicorn.run."
  },
  {
    "function": "chat_box",
    "module": "autorag.web",
    "params": [
      {
        "name": "runner",
        "param_type": "<class 'autorag.deploy.Runner'>",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "get_runner",
    "module": "autorag.web",
    "params": [
      {
        "name": "yaml_path",
        "param_type": "typing.Optional[str]",
        "description": ""
      },
      {
        "name": "project_dir",
        "param_type": "typing.Optional[str]",
        "description": ""
      },
      {
        "name": "trial_path",
        "param_type": "typing.Optional[str]",
        "description": ""
      }
    ],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "set_initial_state",
    "module": "autorag.web",
    "params": [],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "set_page_config",
    "module": "autorag.web",
    "params": [],
    "return_type": "Any",
    "docstring": null
  },
  {
    "function": "set_page_header",
    "module": "autorag.web",
    "params": [],
    "return_type": "Any",
    "docstring": null
  }
]